{
  
    
        "post0": {
            "title": "Gold Recovery",
            "content": "1. Prepare the data . import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.ensemble import RandomForestRegressor from sklearn.multioutput import MultiOutputRegressor from sklearn.linear_model import Ridge from sklearn.dummy import DummyRegressor from sklearn.model_selection import train_test_split from sklearn.model_selection import cross_validate from sklearn.metrics import make_scorer from sklearn.metrics import mean_absolute_error as MAE . . 1.1. Open the files and look into the data. . train = pd.read_csv(&#39;/datasets/gold_recovery_train.csv&#39;) test = pd.read_csv(&#39;/datasets/gold_recovery_test.csv&#39;) full = pd.read_csv(&#39;/datasets/gold_recovery_full.csv&#39;) . display(train.head()) display(test.head()) full.head() . date final.output.concentrate_ag final.output.concentrate_pb final.output.concentrate_sol final.output.concentrate_au final.output.recovery final.output.tail_ag final.output.tail_pb final.output.tail_sol final.output.tail_au ... secondary_cleaner.state.floatbank4_a_air secondary_cleaner.state.floatbank4_a_level secondary_cleaner.state.floatbank4_b_air secondary_cleaner.state.floatbank4_b_level secondary_cleaner.state.floatbank5_a_air secondary_cleaner.state.floatbank5_a_level secondary_cleaner.state.floatbank5_b_air secondary_cleaner.state.floatbank5_b_level secondary_cleaner.state.floatbank6_a_air secondary_cleaner.state.floatbank6_a_level . 0 | 2016-01-15 00:00:00 | 6.055403 | 9.889648 | 5.507324 | 42.192020 | 70.541216 | 10.411962 | 0.895447 | 16.904297 | 2.143149 | ... | 14.016835 | -502.488007 | 12.099931 | -504.715942 | 9.925633 | -498.310211 | 8.079666 | -500.470978 | 14.151341 | -605.841980 | . 1 | 2016-01-15 01:00:00 | 6.029369 | 9.968944 | 5.257781 | 42.701629 | 69.266198 | 10.462676 | 0.927452 | 16.634514 | 2.224930 | ... | 13.992281 | -505.503262 | 11.950531 | -501.331529 | 10.039245 | -500.169983 | 7.984757 | -500.582168 | 13.998353 | -599.787184 | . 2 | 2016-01-15 02:00:00 | 6.055926 | 10.213995 | 5.383759 | 42.657501 | 68.116445 | 10.507046 | 0.953716 | 16.208849 | 2.257889 | ... | 14.015015 | -502.520901 | 11.912783 | -501.133383 | 10.070913 | -500.129135 | 8.013877 | -500.517572 | 14.028663 | -601.427363 | . 3 | 2016-01-15 03:00:00 | 6.047977 | 9.977019 | 4.858634 | 42.689819 | 68.347543 | 10.422762 | 0.883763 | 16.532835 | 2.146849 | ... | 14.036510 | -500.857308 | 11.999550 | -501.193686 | 9.970366 | -499.201640 | 7.977324 | -500.255908 | 14.005551 | -599.996129 | . 4 | 2016-01-15 04:00:00 | 6.148599 | 10.142511 | 4.939416 | 42.774141 | 66.927016 | 10.360302 | 0.792826 | 16.525686 | 2.055292 | ... | 14.027298 | -499.838632 | 11.953070 | -501.053894 | 9.925709 | -501.686727 | 7.894242 | -500.356035 | 13.996647 | -601.496691 | . 5 rows × 87 columns . date primary_cleaner.input.sulfate primary_cleaner.input.depressant primary_cleaner.input.feed_size primary_cleaner.input.xanthate primary_cleaner.state.floatbank8_a_air primary_cleaner.state.floatbank8_a_level primary_cleaner.state.floatbank8_b_air primary_cleaner.state.floatbank8_b_level primary_cleaner.state.floatbank8_c_air ... secondary_cleaner.state.floatbank4_a_air secondary_cleaner.state.floatbank4_a_level secondary_cleaner.state.floatbank4_b_air secondary_cleaner.state.floatbank4_b_level secondary_cleaner.state.floatbank5_a_air secondary_cleaner.state.floatbank5_a_level secondary_cleaner.state.floatbank5_b_air secondary_cleaner.state.floatbank5_b_level secondary_cleaner.state.floatbank6_a_air secondary_cleaner.state.floatbank6_a_level . 0 | 2016-09-01 00:59:59 | 210.800909 | 14.993118 | 8.080000 | 1.005021 | 1398.981301 | -500.225577 | 1399.144926 | -499.919735 | 1400.102998 | ... | 12.023554 | -497.795834 | 8.016656 | -501.289139 | 7.946562 | -432.317850 | 4.872511 | -500.037437 | 26.705889 | -499.709414 | . 1 | 2016-09-01 01:59:59 | 215.392455 | 14.987471 | 8.080000 | 0.990469 | 1398.777912 | -500.057435 | 1398.055362 | -499.778182 | 1396.151033 | ... | 12.058140 | -498.695773 | 8.130979 | -499.634209 | 7.958270 | -525.839648 | 4.878850 | -500.162375 | 25.019940 | -499.819438 | . 2 | 2016-09-01 02:59:59 | 215.259946 | 12.884934 | 7.786667 | 0.996043 | 1398.493666 | -500.868360 | 1398.860436 | -499.764529 | 1398.075709 | ... | 11.962366 | -498.767484 | 8.096893 | -500.827423 | 8.071056 | -500.801673 | 4.905125 | -499.828510 | 24.994862 | -500.622559 | . 3 | 2016-09-01 03:59:59 | 215.336236 | 12.006805 | 7.640000 | 0.863514 | 1399.618111 | -498.863574 | 1397.440120 | -499.211024 | 1400.129303 | ... | 12.033091 | -498.350935 | 8.074946 | -499.474407 | 7.897085 | -500.868509 | 4.931400 | -499.963623 | 24.948919 | -498.709987 | . 4 | 2016-09-01 04:59:59 | 199.099327 | 10.682530 | 7.530000 | 0.805575 | 1401.268123 | -500.808305 | 1398.128818 | -499.504543 | 1402.172226 | ... | 12.025367 | -500.786497 | 8.054678 | -500.397500 | 8.107890 | -509.526725 | 4.957674 | -500.360026 | 25.003331 | -500.856333 | . 5 rows × 53 columns . date final.output.concentrate_ag final.output.concentrate_pb final.output.concentrate_sol final.output.concentrate_au final.output.recovery final.output.tail_ag final.output.tail_pb final.output.tail_sol final.output.tail_au ... secondary_cleaner.state.floatbank4_a_air secondary_cleaner.state.floatbank4_a_level secondary_cleaner.state.floatbank4_b_air secondary_cleaner.state.floatbank4_b_level secondary_cleaner.state.floatbank5_a_air secondary_cleaner.state.floatbank5_a_level secondary_cleaner.state.floatbank5_b_air secondary_cleaner.state.floatbank5_b_level secondary_cleaner.state.floatbank6_a_air secondary_cleaner.state.floatbank6_a_level . 0 | 2016-01-15 00:00:00 | 6.055403 | 9.889648 | 5.507324 | 42.192020 | 70.541216 | 10.411962 | 0.895447 | 16.904297 | 2.143149 | ... | 14.016835 | -502.488007 | 12.099931 | -504.715942 | 9.925633 | -498.310211 | 8.079666 | -500.470978 | 14.151341 | -605.841980 | . 1 | 2016-01-15 01:00:00 | 6.029369 | 9.968944 | 5.257781 | 42.701629 | 69.266198 | 10.462676 | 0.927452 | 16.634514 | 2.224930 | ... | 13.992281 | -505.503262 | 11.950531 | -501.331529 | 10.039245 | -500.169983 | 7.984757 | -500.582168 | 13.998353 | -599.787184 | . 2 | 2016-01-15 02:00:00 | 6.055926 | 10.213995 | 5.383759 | 42.657501 | 68.116445 | 10.507046 | 0.953716 | 16.208849 | 2.257889 | ... | 14.015015 | -502.520901 | 11.912783 | -501.133383 | 10.070913 | -500.129135 | 8.013877 | -500.517572 | 14.028663 | -601.427363 | . 3 | 2016-01-15 03:00:00 | 6.047977 | 9.977019 | 4.858634 | 42.689819 | 68.347543 | 10.422762 | 0.883763 | 16.532835 | 2.146849 | ... | 14.036510 | -500.857308 | 11.999550 | -501.193686 | 9.970366 | -499.201640 | 7.977324 | -500.255908 | 14.005551 | -599.996129 | . 4 | 2016-01-15 04:00:00 | 6.148599 | 10.142511 | 4.939416 | 42.774141 | 66.927016 | 10.360302 | 0.792826 | 16.525686 | 2.055292 | ... | 14.027298 | -499.838632 | 11.953070 | -501.053894 | 9.925709 | -501.686727 | 7.894242 | -500.356035 | 13.996647 | -601.496691 | . 5 rows × 87 columns . . train.info() print() test.info() print() full.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16860 entries, 0 to 16859 Data columns (total 87 columns): date 16860 non-null object final.output.concentrate_ag 16788 non-null float64 final.output.concentrate_pb 16788 non-null float64 final.output.concentrate_sol 16490 non-null float64 final.output.concentrate_au 16789 non-null float64 final.output.recovery 15339 non-null float64 final.output.tail_ag 16794 non-null float64 final.output.tail_pb 16677 non-null float64 final.output.tail_sol 16715 non-null float64 final.output.tail_au 16794 non-null float64 primary_cleaner.input.sulfate 15553 non-null float64 primary_cleaner.input.depressant 15598 non-null float64 primary_cleaner.input.feed_size 16860 non-null float64 primary_cleaner.input.xanthate 15875 non-null float64 primary_cleaner.output.concentrate_ag 16778 non-null float64 primary_cleaner.output.concentrate_pb 16502 non-null float64 primary_cleaner.output.concentrate_sol 16224 non-null float64 primary_cleaner.output.concentrate_au 16778 non-null float64 primary_cleaner.output.tail_ag 16777 non-null float64 primary_cleaner.output.tail_pb 16761 non-null float64 primary_cleaner.output.tail_sol 16579 non-null float64 primary_cleaner.output.tail_au 16777 non-null float64 primary_cleaner.state.floatbank8_a_air 16820 non-null float64 primary_cleaner.state.floatbank8_a_level 16827 non-null float64 primary_cleaner.state.floatbank8_b_air 16820 non-null float64 primary_cleaner.state.floatbank8_b_level 16833 non-null float64 primary_cleaner.state.floatbank8_c_air 16822 non-null float64 primary_cleaner.state.floatbank8_c_level 16833 non-null float64 primary_cleaner.state.floatbank8_d_air 16821 non-null float64 primary_cleaner.state.floatbank8_d_level 16833 non-null float64 rougher.calculation.sulfate_to_au_concentrate 16833 non-null float64 rougher.calculation.floatbank10_sulfate_to_au_feed 16833 non-null float64 rougher.calculation.floatbank11_sulfate_to_au_feed 16833 non-null float64 rougher.calculation.au_pb_ratio 15618 non-null float64 rougher.input.feed_ag 16778 non-null float64 rougher.input.feed_pb 16632 non-null float64 rougher.input.feed_rate 16347 non-null float64 rougher.input.feed_size 16443 non-null float64 rougher.input.feed_sol 16568 non-null float64 rougher.input.feed_au 16777 non-null float64 rougher.input.floatbank10_sulfate 15816 non-null float64 rougher.input.floatbank10_xanthate 16514 non-null float64 rougher.input.floatbank11_sulfate 16237 non-null float64 rougher.input.floatbank11_xanthate 14956 non-null float64 rougher.output.concentrate_ag 16778 non-null float64 rougher.output.concentrate_pb 16778 non-null float64 rougher.output.concentrate_sol 16698 non-null float64 rougher.output.concentrate_au 16778 non-null float64 rougher.output.recovery 14287 non-null float64 rougher.output.tail_ag 14610 non-null float64 rougher.output.tail_pb 16778 non-null float64 rougher.output.tail_sol 14611 non-null float64 rougher.output.tail_au 14611 non-null float64 rougher.state.floatbank10_a_air 16807 non-null float64 rougher.state.floatbank10_a_level 16807 non-null float64 rougher.state.floatbank10_b_air 16807 non-null float64 rougher.state.floatbank10_b_level 16807 non-null float64 rougher.state.floatbank10_c_air 16807 non-null float64 rougher.state.floatbank10_c_level 16814 non-null float64 rougher.state.floatbank10_d_air 16802 non-null float64 rougher.state.floatbank10_d_level 16809 non-null float64 rougher.state.floatbank10_e_air 16257 non-null float64 rougher.state.floatbank10_e_level 16809 non-null float64 rougher.state.floatbank10_f_air 16802 non-null float64 rougher.state.floatbank10_f_level 16802 non-null float64 secondary_cleaner.output.tail_ag 16776 non-null float64 secondary_cleaner.output.tail_pb 16764 non-null float64 secondary_cleaner.output.tail_sol 14874 non-null float64 secondary_cleaner.output.tail_au 16778 non-null float64 secondary_cleaner.state.floatbank2_a_air 16497 non-null float64 secondary_cleaner.state.floatbank2_a_level 16751 non-null float64 secondary_cleaner.state.floatbank2_b_air 16705 non-null float64 secondary_cleaner.state.floatbank2_b_level 16748 non-null float64 secondary_cleaner.state.floatbank3_a_air 16763 non-null float64 secondary_cleaner.state.floatbank3_a_level 16747 non-null float64 secondary_cleaner.state.floatbank3_b_air 16752 non-null float64 secondary_cleaner.state.floatbank3_b_level 16750 non-null float64 secondary_cleaner.state.floatbank4_a_air 16731 non-null float64 secondary_cleaner.state.floatbank4_a_level 16747 non-null float64 secondary_cleaner.state.floatbank4_b_air 16768 non-null float64 secondary_cleaner.state.floatbank4_b_level 16767 non-null float64 secondary_cleaner.state.floatbank5_a_air 16775 non-null float64 secondary_cleaner.state.floatbank5_a_level 16775 non-null float64 secondary_cleaner.state.floatbank5_b_air 16775 non-null float64 secondary_cleaner.state.floatbank5_b_level 16776 non-null float64 secondary_cleaner.state.floatbank6_a_air 16757 non-null float64 secondary_cleaner.state.floatbank6_a_level 16775 non-null float64 dtypes: float64(86), object(1) memory usage: 11.2+ MB &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 5856 entries, 0 to 5855 Data columns (total 53 columns): date 5856 non-null object primary_cleaner.input.sulfate 5554 non-null float64 primary_cleaner.input.depressant 5572 non-null float64 primary_cleaner.input.feed_size 5856 non-null float64 primary_cleaner.input.xanthate 5690 non-null float64 primary_cleaner.state.floatbank8_a_air 5840 non-null float64 primary_cleaner.state.floatbank8_a_level 5840 non-null float64 primary_cleaner.state.floatbank8_b_air 5840 non-null float64 primary_cleaner.state.floatbank8_b_level 5840 non-null float64 primary_cleaner.state.floatbank8_c_air 5840 non-null float64 primary_cleaner.state.floatbank8_c_level 5840 non-null float64 primary_cleaner.state.floatbank8_d_air 5840 non-null float64 primary_cleaner.state.floatbank8_d_level 5840 non-null float64 rougher.input.feed_ag 5840 non-null float64 rougher.input.feed_pb 5840 non-null float64 rougher.input.feed_rate 5816 non-null float64 rougher.input.feed_size 5834 non-null float64 rougher.input.feed_sol 5789 non-null float64 rougher.input.feed_au 5840 non-null float64 rougher.input.floatbank10_sulfate 5599 non-null float64 rougher.input.floatbank10_xanthate 5733 non-null float64 rougher.input.floatbank11_sulfate 5801 non-null float64 rougher.input.floatbank11_xanthate 5503 non-null float64 rougher.state.floatbank10_a_air 5839 non-null float64 rougher.state.floatbank10_a_level 5840 non-null float64 rougher.state.floatbank10_b_air 5839 non-null float64 rougher.state.floatbank10_b_level 5840 non-null float64 rougher.state.floatbank10_c_air 5839 non-null float64 rougher.state.floatbank10_c_level 5840 non-null float64 rougher.state.floatbank10_d_air 5839 non-null float64 rougher.state.floatbank10_d_level 5840 non-null float64 rougher.state.floatbank10_e_air 5839 non-null float64 rougher.state.floatbank10_e_level 5840 non-null float64 rougher.state.floatbank10_f_air 5839 non-null float64 rougher.state.floatbank10_f_level 5840 non-null float64 secondary_cleaner.state.floatbank2_a_air 5836 non-null float64 secondary_cleaner.state.floatbank2_a_level 5840 non-null float64 secondary_cleaner.state.floatbank2_b_air 5833 non-null float64 secondary_cleaner.state.floatbank2_b_level 5840 non-null float64 secondary_cleaner.state.floatbank3_a_air 5822 non-null float64 secondary_cleaner.state.floatbank3_a_level 5840 non-null float64 secondary_cleaner.state.floatbank3_b_air 5840 non-null float64 secondary_cleaner.state.floatbank3_b_level 5840 non-null float64 secondary_cleaner.state.floatbank4_a_air 5840 non-null float64 secondary_cleaner.state.floatbank4_a_level 5840 non-null float64 secondary_cleaner.state.floatbank4_b_air 5840 non-null float64 secondary_cleaner.state.floatbank4_b_level 5840 non-null float64 secondary_cleaner.state.floatbank5_a_air 5840 non-null float64 secondary_cleaner.state.floatbank5_a_level 5840 non-null float64 secondary_cleaner.state.floatbank5_b_air 5840 non-null float64 secondary_cleaner.state.floatbank5_b_level 5840 non-null float64 secondary_cleaner.state.floatbank6_a_air 5840 non-null float64 secondary_cleaner.state.floatbank6_a_level 5840 non-null float64 dtypes: float64(52), object(1) memory usage: 2.4+ MB &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 22716 entries, 0 to 22715 Data columns (total 87 columns): date 22716 non-null object final.output.concentrate_ag 22627 non-null float64 final.output.concentrate_pb 22629 non-null float64 final.output.concentrate_sol 22331 non-null float64 final.output.concentrate_au 22630 non-null float64 final.output.recovery 20753 non-null float64 final.output.tail_ag 22633 non-null float64 final.output.tail_pb 22516 non-null float64 final.output.tail_sol 22445 non-null float64 final.output.tail_au 22635 non-null float64 primary_cleaner.input.sulfate 21107 non-null float64 primary_cleaner.input.depressant 21170 non-null float64 primary_cleaner.input.feed_size 22716 non-null float64 primary_cleaner.input.xanthate 21565 non-null float64 primary_cleaner.output.concentrate_ag 22618 non-null float64 primary_cleaner.output.concentrate_pb 22268 non-null float64 primary_cleaner.output.concentrate_sol 21918 non-null float64 primary_cleaner.output.concentrate_au 22618 non-null float64 primary_cleaner.output.tail_ag 22614 non-null float64 primary_cleaner.output.tail_pb 22594 non-null float64 primary_cleaner.output.tail_sol 22365 non-null float64 primary_cleaner.output.tail_au 22617 non-null float64 primary_cleaner.state.floatbank8_a_air 22660 non-null float64 primary_cleaner.state.floatbank8_a_level 22667 non-null float64 primary_cleaner.state.floatbank8_b_air 22660 non-null float64 primary_cleaner.state.floatbank8_b_level 22673 non-null float64 primary_cleaner.state.floatbank8_c_air 22662 non-null float64 primary_cleaner.state.floatbank8_c_level 22673 non-null float64 primary_cleaner.state.floatbank8_d_air 22661 non-null float64 primary_cleaner.state.floatbank8_d_level 22673 non-null float64 rougher.calculation.sulfate_to_au_concentrate 22672 non-null float64 rougher.calculation.floatbank10_sulfate_to_au_feed 22672 non-null float64 rougher.calculation.floatbank11_sulfate_to_au_feed 22672 non-null float64 rougher.calculation.au_pb_ratio 21089 non-null float64 rougher.input.feed_ag 22618 non-null float64 rougher.input.feed_pb 22472 non-null float64 rougher.input.feed_rate 22163 non-null float64 rougher.input.feed_size 22277 non-null float64 rougher.input.feed_sol 22357 non-null float64 rougher.input.feed_au 22617 non-null float64 rougher.input.floatbank10_sulfate 21415 non-null float64 rougher.input.floatbank10_xanthate 22247 non-null float64 rougher.input.floatbank11_sulfate 22038 non-null float64 rougher.input.floatbank11_xanthate 20459 non-null float64 rougher.output.concentrate_ag 22618 non-null float64 rougher.output.concentrate_pb 22618 non-null float64 rougher.output.concentrate_sol 22526 non-null float64 rougher.output.concentrate_au 22618 non-null float64 rougher.output.recovery 19597 non-null float64 rougher.output.tail_ag 19979 non-null float64 rougher.output.tail_pb 22618 non-null float64 rougher.output.tail_sol 19980 non-null float64 rougher.output.tail_au 19980 non-null float64 rougher.state.floatbank10_a_air 22646 non-null float64 rougher.state.floatbank10_a_level 22647 non-null float64 rougher.state.floatbank10_b_air 22646 non-null float64 rougher.state.floatbank10_b_level 22647 non-null float64 rougher.state.floatbank10_c_air 22646 non-null float64 rougher.state.floatbank10_c_level 22654 non-null float64 rougher.state.floatbank10_d_air 22641 non-null float64 rougher.state.floatbank10_d_level 22649 non-null float64 rougher.state.floatbank10_e_air 22096 non-null float64 rougher.state.floatbank10_e_level 22649 non-null float64 rougher.state.floatbank10_f_air 22641 non-null float64 rougher.state.floatbank10_f_level 22642 non-null float64 secondary_cleaner.output.tail_ag 22616 non-null float64 secondary_cleaner.output.tail_pb 22600 non-null float64 secondary_cleaner.output.tail_sol 20501 non-null float64 secondary_cleaner.output.tail_au 22618 non-null float64 secondary_cleaner.state.floatbank2_a_air 22333 non-null float64 secondary_cleaner.state.floatbank2_a_level 22591 non-null float64 secondary_cleaner.state.floatbank2_b_air 22538 non-null float64 secondary_cleaner.state.floatbank2_b_level 22588 non-null float64 secondary_cleaner.state.floatbank3_a_air 22585 non-null float64 secondary_cleaner.state.floatbank3_a_level 22587 non-null float64 secondary_cleaner.state.floatbank3_b_air 22592 non-null float64 secondary_cleaner.state.floatbank3_b_level 22590 non-null float64 secondary_cleaner.state.floatbank4_a_air 22571 non-null float64 secondary_cleaner.state.floatbank4_a_level 22587 non-null float64 secondary_cleaner.state.floatbank4_b_air 22608 non-null float64 secondary_cleaner.state.floatbank4_b_level 22607 non-null float64 secondary_cleaner.state.floatbank5_a_air 22615 non-null float64 secondary_cleaner.state.floatbank5_a_level 22615 non-null float64 secondary_cleaner.state.floatbank5_b_air 22615 non-null float64 secondary_cleaner.state.floatbank5_b_level 22616 non-null float64 secondary_cleaner.state.floatbank6_a_air 22597 non-null float64 secondary_cleaner.state.floatbank6_a_level 22615 non-null float64 dtypes: float64(86), object(1) memory usage: 15.1+ MB . . train.dropna(inplace=True) train.reset_index(drop=True, inplace=True) train.tail() . date final.output.concentrate_ag final.output.concentrate_pb final.output.concentrate_sol final.output.concentrate_au final.output.recovery final.output.tail_ag final.output.tail_pb final.output.tail_sol final.output.tail_au ... secondary_cleaner.state.floatbank4_a_air secondary_cleaner.state.floatbank4_a_level secondary_cleaner.state.floatbank4_b_air secondary_cleaner.state.floatbank4_b_level secondary_cleaner.state.floatbank5_a_air secondary_cleaner.state.floatbank5_a_level secondary_cleaner.state.floatbank5_b_air secondary_cleaner.state.floatbank5_b_level secondary_cleaner.state.floatbank6_a_air secondary_cleaner.state.floatbank6_a_level . 11012 | 2018-08-18 06:59:59 | 3.224920 | 11.356233 | 6.803482 | 46.713954 | 73.755150 | 8.769645 | 3.141541 | 10.403181 | 1.529220 | ... | 23.031497 | -501.167942 | 20.007571 | -499.740028 | 18.006038 | -499.834374 | 13.001114 | -500.155694 | 20.007840 | -501.296428 | . 11013 | 2018-08-18 07:59:59 | 3.195978 | 11.349355 | 6.862249 | 46.866780 | 69.049291 | 8.897321 | 3.130493 | 10.549470 | 1.612542 | ... | 22.960095 | -501.612783 | 20.035660 | -500.251357 | 17.998535 | -500.395178 | 12.954048 | -499.895163 | 19.968498 | -501.041608 | . 11014 | 2018-08-18 08:59:59 | 3.109998 | 11.434366 | 6.886013 | 46.795691 | 67.002189 | 8.529606 | 2.911418 | 11.115147 | 1.596616 | ... | 23.015718 | -501.711599 | 19.951231 | -499.857027 | 18.019543 | -500.451156 | 13.023431 | -499.914391 | 19.990885 | -501.518452 | . 11015 | 2018-08-18 09:59:59 | 3.367241 | 11.625587 | 6.799433 | 46.408188 | 65.523246 | 8.777171 | 2.819214 | 10.463847 | 1.602879 | ... | 23.024963 | -501.153409 | 20.054122 | -500.314711 | 17.979515 | -499.272871 | 12.992404 | -499.976268 | 20.013986 | -500.625471 | . 11016 | 2018-08-18 10:59:59 | 3.598375 | 11.737832 | 6.717509 | 46.299438 | 70.281454 | 8.406690 | 2.517518 | 10.652193 | 1.389434 | ... | 23.018622 | -500.492702 | 20.020205 | -500.220296 | 17.963512 | -499.939490 | 12.990306 | -500.080993 | 19.990336 | -499.191575 | . 5 rows × 87 columns . The data has tons of Null values, to avoid trying to sort through all the holes we dropped them | After trimming down the data set there is still 11017 entries | If we struggle to find a good model we will revisit this method of approach | . 1.2. Check that recovery is calculated correctly. . Using the training set, calculate recovery for the rougher.output.recovery feature. | Find the MAE between your calculations and the feature values. | Provide findings. | . C = train[&#39;rougher.output.concentrate_au&#39;] F = train[&#39;rougher.input.feed_au&#39;] T = train[&#39;rougher.output.tail_au&#39;] train[&#39;rougher.calculation.recovery&#39;] = C * (F - T) / (F * (C - T)) * 100 MAE(train[&#39;rougher.output.recovery&#39;], train[&#39;rougher.calculation.recovery&#39;]) . 9.555596961987514e-15 . This is an incredibly small number, we&#39;re safe to proceed with testing | Tried following suit with labeling standards | . 1.3. Analyze the features not available in the test set. . What are these parameters? | What is their type? | . columns_full = set(full.columns) columns_test = set(test.columns) removable_features = list(columns_full.difference(columns_test)) . removable_features.remove(&#39;final.output.recovery&#39;) removable_features.remove(&#39;rougher.output.recovery&#39;) removable_features . [&#39;primary_cleaner.output.tail_ag&#39;, &#39;final.output.concentrate_ag&#39;, &#39;secondary_cleaner.output.tail_ag&#39;, &#39;final.output.tail_au&#39;, &#39;secondary_cleaner.output.tail_sol&#39;, &#39;final.output.tail_sol&#39;, &#39;secondary_cleaner.output.tail_au&#39;, &#39;primary_cleaner.output.concentrate_sol&#39;, &#39;final.output.concentrate_sol&#39;, &#39;primary_cleaner.output.concentrate_au&#39;, &#39;rougher.calculation.sulfate_to_au_concentrate&#39;, &#39;rougher.output.tail_sol&#39;, &#39;rougher.output.concentrate_au&#39;, &#39;rougher.calculation.floatbank11_sulfate_to_au_feed&#39;, &#39;rougher.output.tail_au&#39;, &#39;primary_cleaner.output.tail_sol&#39;, &#39;final.output.tail_ag&#39;, &#39;rougher.calculation.au_pb_ratio&#39;, &#39;primary_cleaner.output.concentrate_ag&#39;, &#39;secondary_cleaner.output.tail_pb&#39;, &#39;final.output.concentrate_pb&#39;, &#39;primary_cleaner.output.tail_pb&#39;, &#39;primary_cleaner.output.concentrate_pb&#39;, &#39;rougher.calculation.floatbank10_sulfate_to_au_feed&#39;, &#39;final.output.concentrate_au&#39;, &#39;rougher.output.concentrate_sol&#39;, &#39;final.output.tail_pb&#39;, &#39;rougher.output.concentrate_ag&#39;, &#39;rougher.output.concentrate_pb&#39;, &#39;rougher.output.tail_pb&#39;, &#39;rougher.output.tail_ag&#39;, &#39;primary_cleaner.output.tail_au&#39;] . All the missing columns from the test set appear to be output values or calculations | We have no Target columns in the test dataset making it useless for testing. | . 1.4. Perform data preprocessing. . STATE = 12345 . full.dropna(inplace=True) full.reset_index(drop=True, inplace=True) full.shape . (16094, 87) . train, test = train_test_split(full, test_size=0.25, random_state=STATE) . With missing Target columns in the test set I decided to trash the train and test data and resplit the full data set | If this method wasn&#39;t acceptable to the owner or project supervisor I would manually calculate the target values as I did earlier in this project. | Roughly 6000 data entries were trashed due to Null values, if this causes issues down the line we will revisit scrapping these points | . 2. Analyze the data . 2.1. Take note of how the concentrations of metals (Au, Ag, Pb) change depending on the purification stage. . ORES = [&#39;au&#39;, &#39;ag&#39;, &#39;pb&#39;] STAGES = [&#39;rougher.output.concentrate_&#39;, &#39;primary_cleaner.output.concentrate_&#39;, &#39;final.output.concentrate_&#39;] . for ore in ORES: plt.figure(figsize=(12,6)) for stage in STAGES: plt.hist(full[stage + ore], label=(stage + ore), bins = 50, alpha=0.5) plt.xlabel(ore.capitalize() + &#39; Concentration&#39;) plt.title(ore.capitalize() + &#39; Concentrations&#39;) plt.legend() plt.grid() plt.show() . Gold(Au) concentrations increase with each stage | Silver(Ag) gets filtered out and goes down with each stage | Lead(Pb) stays roughly the same throughout the process | There appears to be some outliers where ore == 0 | . 2.2. Compare the feed particle size distributions in the training set and in the test set. If the distributions vary significantly, the model evaluation will be incorrect. . plt.figure(figsize=(12,6)) plt.hist(train[&#39;rougher.input.feed_size&#39;], label=&#39;Training Set&#39;, bins=75, alpha=0.5, density=True) plt.hist(test[&#39;rougher.input.feed_size&#39;], label=&#39;Test Set&#39;, bins=75, alpha=0.5, density=True) plt.xlabel(&#39;Feed Size&#39;) plt.xlim((0, 150)) plt.title(&#39;Feed Size Distributions&#39;) plt.legend() plt.grid() plt.show() . The feed size distributions appear to be very similar making things safe to proceed | There are some major outliers in the dataset extending into the 300+ range, an xlim was placed to get a better view of the bulk of the data | . for ore in ORES: plt.figure(figsize=(12,6)) plt.hist(train[&#39;rougher.input.feed_&#39; + ore], label=&#39;Training Set&#39;, bins = 50, alpha=0.25, density=True) plt.hist(test[&#39;rougher.input.feed_&#39; + ore], label= &#39;Test Set&#39;, bins = 50, alpha=0.50, density=True) plt.xlabel(ore.capitalize() + &#39; Feed Size&#39;) plt.title(ore.capitalize() + &#39; Feed Size Distributions&#39;) plt.legend() plt.grid() plt.show() . Individual distributions for Gold(Au), Silver(Ag), and Lead(Pb) appear to be similar as well | . 2.3. Consider the total concentrations of all substances at different stages: raw feed, rougher concentrate, and final concentrate. . Do you notice any abnormal values in the total distribution? | If you do, is it worth removing such values from both samples? | Describe the findings and eliminate anomalies. | . print(&#39;Train Size Before:&#39;, train.shape[0]) print(&#39;Test Size Before:&#39;, test.shape[0]) train = train[train[&#39;rougher.input.feed_size&#39;] &lt;= 120] test = test[test[&#39;rougher.input.feed_size&#39;] &lt;= 120] for ore in ORES: for stage in STAGES: [stage + ore] train = train[train[stage + ore] &gt; 0] test = test[test[stage + ore] &gt; 0] print(&#39;Train Size After:&#39;, train.shape[0]) print(&#39;Test Size After:&#39;, test.shape[0]) . Train Size Before: 12070 Test Size Before: 4024 Train Size After: 11707 Test Size After: 3884 . 3. Build the model . train = train.drop(removable_features + [&#39;date&#39;], axis=1) test = test.drop(removable_features + [&#39;date&#39;], axis=1) print(train.shape) test.shape . (11707, 54) . (3884, 54) . Removed columns that were not initially available in the test set (excluding target columns) | . features_train = train.drop([&#39;rougher.output.recovery&#39;, &#39;final.output.recovery&#39;], axis=1) targets_train = train[[&#39;rougher.output.recovery&#39;, &#39;final.output.recovery&#39;]] features_test = test.drop([&#39;rougher.output.recovery&#39;, &#39;final.output.recovery&#39;], axis=1) targets_test = test[[&#39;rougher.output.recovery&#39;, &#39;final.output.recovery&#39;]] . Datasets prepared for model training | . 3.1. Write a function to calculate the final sMAPE value. . def smape(target, predictions): return np.sum(np.abs(target - predictions) / ((np.abs(target) + np.abs(predictions)) / 2)) / len(target) * 100 def final_smape(targets, predictions): return (.25 * smape(targets[0], predictions[0])) + (.75 * smape(targets[1], predictions[1])) . Broke the smape function into 2 parts, so that smape can still be called on a single target | . 3.2. Train different models. . Evaluate them using cross-validation. | Pick the best model and test it using the test sample. Provide findings. | . MultiOutputRegressor - Ridge . model = MultiOutputRegressor(Ridge(random_state=STATE)) scores = cross_validate( model, features_train, targets_train.values, cv=10, scoring=make_scorer(final_smape, greater_is_better=False), n_jobs=-1 ) . scores[&#39;test_score&#39;] = np.abs(scores[&#39;test_score&#39;]) print(&#39;All Scores:&#39;, scores[&#39;test_score&#39;]) print(&#39;Min Score:&#39;, scores[&#39;test_score&#39;].min()) print(&#39;Max Score:&#39;, scores[&#39;test_score&#39;].max()) print(&#39;Mean Score&#39;, scores[&#39;test_score&#39;].mean()) . All Scores: [3.28774945 5.78103106 3.30564647 1.78031086 8.5693093 4.72647352 3.0575838 2.57316625 2.14893175 4.33952347] Min Score: 1.7803108614465526 Max Score: 8.56930929520799 Mean Score 3.9569725918259193 . We have a set of decent scores, time to train model 2 and compare | . MultiOutput - RandomForestRegressor . model = MultiOutputRegressor(RandomForestRegressor(n_estimators=100, max_depth=20, random_state=STATE)) . scores = cross_validate( model, features_train, targets_train.values, cv=10, scoring=make_scorer(final_smape, greater_is_better=False), n_jobs=-1 ) . scores[&#39;test_score&#39;] = np.abs(scores[&#39;test_score&#39;]) print(&#39;All Scores:&#39;, scores[&#39;test_score&#39;]) print(&#39;Min Score:&#39;, scores[&#39;test_score&#39;].min()) print(&#39;Max Score:&#39;, scores[&#39;test_score&#39;].max()) print(&#39;Mean Score&#39;, scores[&#39;test_score&#39;].mean()) . All Scores: [3.4162687 6.70766298 1.83670086 2.70828718 6.03572728 4.60284131 2.83019246 0.74377943 1.02469512 0.81966925] Min Score: 0.743779429586609 Max Score: 6.707662976360825 Mean Score 3.0725824553865158 . This is computationally expensive and so cross_validation was done less times | Overall though these scores appear to be much better than our previous model | . Sanity Test . dummy_model = DummyRegressor(strategy=&#39;mean&#39;) dummy_model.fit(features_train, targets_train.values) dummy_pred = dummy_model.predict(features_test) . print(f&quot;Baseline using the mean: {final_smape(targets_test.values, dummy_pred):.2f}% sMAPE&quot;) . Baseline using the mean: 7.31% sMAPE . model = MultiOutputRegressor(RandomForestRegressor(n_estimators=100, max_depth=20, random_state=STATE)) model.fit(features_train, targets_train.values) predictions = model.predict(features_test) . print(f&quot;Our model yields: {final_smape(targets_test.values, predictions):.2f}% sMAPE&quot;) . Our model yields: 3.77% sMAPE . Final Conclusion: . We were able to achieve a 3.54% improvement over the sanity test model. | With more time we may be able to bring this number down even more by pruning out less data points | The model chosen may not appeal to the company due to its overall computational cost | Looking into more advanced models and estimators may also be of benefit to the company | .",
            "url": "https://nicholas-j-snyder.github.io/portfolio/fastpages/jupyter/2020/12/20/Gold-Recovery.html",
            "relUrl": "/fastpages/jupyter/2020/12/20/Gold-Recovery.html",
            "date": " • Dec 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Steel Processing",
            "content": "Init . import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.metrics import mean_absolute_error as MAE from sklearn.model_selection import train_test_split from sklearn.dummy import DummyRegressor from sklearn.linear_model import LinearRegression from sklearn.linear_model import Ridge from catboost import CatBoostRegressor from sklearn.model_selection import GridSearchCV . . data_elec = pd.read_csv(&#39;/datasets/data_arc_en.csv&#39;) data_bulk = pd.read_csv(&#39;/datasets/data_bulk_en.csv&#39;) data_temp = pd.read_csv(&#39;/datasets/data_temp_en.csv&#39;) data_wire = pd.read_csv(&#39;/datasets/data_wire_en.csv&#39;) data_gas = pd.read_csv(&#39;/datasets/data_gas_en.csv&#39;) . . Cleaning &amp; Merging . data_temp[&#39;Sampling time&#39;] = pd.to_datetime(data_temp[&#39;Sampling time&#39;]) final_temp = (data_temp .drop_duplicates([&#39;key&#39;], keep=&#39;last&#39;) .reset_index(drop=True) .rename(columns={&#39;Temperature&#39;: &#39;Final Temp&#39;, &#39;Sampling time&#39;: &#39;End Time&#39;})) initial_temp = (data_temp .drop_duplicates([&#39;key&#39;]) .reset_index(drop=True) .rename(columns={&#39;Temperature&#39;: &#39;Initial Temp&#39;, &#39;Sampling time&#39;: &#39;Start Time&#39;})) final_temp[&#39;Duration&#39;] = (final_temp[&#39;End Time&#39;] - initial_temp[&#39;Start Time&#39;]).dt.total_seconds() initial_temp = initial_temp.drop([&#39;Start Time&#39;], axis=1) final_temp = final_temp.drop([&#39;End Time&#39;], axis=1) display(initial_temp.head()) final_temp.head() . key Initial Temp . 0 | 1 | 1571.0 | . 1 | 2 | 1581.0 | . 2 | 3 | 1596.0 | . 3 | 4 | 1601.0 | . 4 | 5 | 1576.0 | . key Final Temp Duration . 0 | 1 | 1613.0 | 861.0 | . 1 | 2 | 1602.0 | 1305.0 | . 2 | 3 | 1599.0 | 1300.0 | . 3 | 4 | 1625.0 | 388.0 | . 4 | 5 | 1602.0 | 762.0 | . data_elec = (data_elec .pivot_table(index=&#39;key&#39;, values=[&#39;Active power&#39;, &#39;Reactive power&#39;], aggfunc=&#39;sum&#39;) .reset_index()) data_elec.head() . key Active power Reactive power . 0 | 1 | 4.878147 | 3.183241 | . 1 | 2 | 3.052598 | 1.998112 | . 2 | 3 | 2.525882 | 1.599076 | . 3 | 4 | 3.209250 | 2.060298 | . 4 | 5 | 3.347173 | 2.252643 | . data_bulk = data_bulk.fillna(0) data_wire = data_wire.fillna(0) . data_all = (initial_temp .merge(final_temp, on=&#39;key&#39;, how=&#39;outer&#39;) .merge(data_bulk, on=&#39;key&#39;, how=&#39;outer&#39;) .merge(data_wire, on=&#39;key&#39;, how=&#39;outer&#39;) .merge(data_elec, on=&#39;key&#39;, how=&#39;outer&#39;) .merge(data_gas, on=&#39;key&#39;, how=&#39;outer&#39;) .drop(&#39;key&#39;, axis = 1)) . data_all.info() data_all.head() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 3241 entries, 0 to 3240 Data columns (total 30 columns): Initial Temp 3216 non-null float64 Final Temp 2477 non-null float64 Duration 3216 non-null float64 Bulk 1 3129 non-null float64 Bulk 2 3129 non-null float64 Bulk 3 3129 non-null float64 Bulk 4 3129 non-null float64 Bulk 5 3129 non-null float64 Bulk 6 3129 non-null float64 Bulk 7 3129 non-null float64 Bulk 8 3129 non-null float64 Bulk 9 3129 non-null float64 Bulk 10 3129 non-null float64 Bulk 11 3129 non-null float64 Bulk 12 3129 non-null float64 Bulk 13 3129 non-null float64 Bulk 14 3129 non-null float64 Bulk 15 3129 non-null float64 Wire 1 3081 non-null float64 Wire 2 3081 non-null float64 Wire 3 3081 non-null float64 Wire 4 3081 non-null float64 Wire 5 3081 non-null float64 Wire 6 3081 non-null float64 Wire 7 3081 non-null float64 Wire 8 3081 non-null float64 Wire 9 3081 non-null float64 Active power 3214 non-null float64 Reactive power 3214 non-null float64 Gas 1 3239 non-null float64 dtypes: float64(30) memory usage: 784.9 KB . Initial Temp Final Temp Duration Bulk 1 Bulk 2 Bulk 3 Bulk 4 Bulk 5 Bulk 6 Bulk 7 ... Wire 3 Wire 4 Wire 5 Wire 6 Wire 7 Wire 8 Wire 9 Active power Reactive power Gas 1 . 0 | 1571.0 | 1613.0 | 861.0 | 0.0 | 0.0 | 0.0 | 43.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 4.878147 | 3.183241 | 29.749986 | . 1 | 1581.0 | 1602.0 | 1305.0 | 0.0 | 0.0 | 0.0 | 73.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 3.052598 | 1.998112 | 12.555561 | . 2 | 1596.0 | 1599.0 | 1300.0 | 0.0 | 0.0 | 0.0 | 34.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2.525882 | 1.599076 | 28.554793 | . 3 | 1601.0 | 1625.0 | 388.0 | 0.0 | 0.0 | 0.0 | 81.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 3.209250 | 2.060298 | 18.841219 | . 4 | 1576.0 | 1602.0 | 762.0 | 0.0 | 0.0 | 0.0 | 78.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 3.347173 | 2.252643 | 5.413692 | . 5 rows × 30 columns . data_all = data_all.dropna() data_all.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 2329 entries, 0 to 2476 Data columns (total 30 columns): Initial Temp 2329 non-null float64 Final Temp 2329 non-null float64 Duration 2329 non-null float64 Bulk 1 2329 non-null float64 Bulk 2 2329 non-null float64 Bulk 3 2329 non-null float64 Bulk 4 2329 non-null float64 Bulk 5 2329 non-null float64 Bulk 6 2329 non-null float64 Bulk 7 2329 non-null float64 Bulk 8 2329 non-null float64 Bulk 9 2329 non-null float64 Bulk 10 2329 non-null float64 Bulk 11 2329 non-null float64 Bulk 12 2329 non-null float64 Bulk 13 2329 non-null float64 Bulk 14 2329 non-null float64 Bulk 15 2329 non-null float64 Wire 1 2329 non-null float64 Wire 2 2329 non-null float64 Wire 3 2329 non-null float64 Wire 4 2329 non-null float64 Wire 5 2329 non-null float64 Wire 6 2329 non-null float64 Wire 7 2329 non-null float64 Wire 8 2329 non-null float64 Wire 9 2329 non-null float64 Active power 2329 non-null float64 Reactive power 2329 non-null float64 Gas 1 2329 non-null float64 dtypes: float64(30) memory usage: 564.1 KB . Train/Test Split . features = data_all.drop(&#39;Final Temp&#39;, axis=1) target = data_all[&#39;Final Temp&#39;] . features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.2, random_state=54321) . display(features_train.shape) display(target_train.shape) display(features_test.shape) target_test.shape . (1863, 29) . (1863,) . (466, 29) . (466,) . Model 0 - Sanity Test . model_0 = DummyRegressor(strategy=&#39;mean&#39;) . model_0.fit(features_train, target_train) pred_0 = model_0.predict(features_test) . print(&#39;Sanity Test MAE:&#39;, MAE(target_test, pred_0)) . Sanity Test MAE: 8.025588660128683 . Model 1 - LinearRegression . model_1 = LinearRegression() . model_1.fit(features_train, target_train) pred_1 = model_1.predict(features_test) . print(&#39;Linear Regression MAE:&#39;, MAE(target_test, pred_1)) . Linear Regression MAE: 5.9500027174198 . Model 2 - CatBoost . model_2 = CatBoostRegressor(random_state=54321, loss_function=&#39;MAE&#39;) . #params = {&#39;learning_rate&#39;: [0.03, 0.05, 0.1], # &#39;depth&#39;: [4, 6, 10], # &#39;l2_leaf_reg&#39;: [1, 3, 5, 7, 9]} # #model_2.grid_search(params, # X=features_train, # y=target_train, # refit=True, # partition_random_seed= 12345) model_2.fit(features_train, target_train) pred_2 = model_2.predict(features_test) . 0: learn: 7.9989684 total: 50.1ms remaining: 50.1s 1: learn: 7.9565330 total: 52.7ms remaining: 26.3s 2: learn: 7.8866288 total: 55ms remaining: 18.3s 3: learn: 7.8106662 total: 57.7ms remaining: 14.4s 4: learn: 7.7578909 total: 79.7ms remaining: 15.9s 5: learn: 7.6960067 total: 82.1ms remaining: 13.6s 6: learn: 7.6343002 total: 84.3ms remaining: 12s 7: learn: 7.5814495 total: 86.7ms remaining: 10.8s 8: learn: 7.5321962 total: 89.2ms remaining: 9.82s 9: learn: 7.4817674 total: 96.7ms remaining: 9.57s 10: learn: 7.4338809 total: 178ms remaining: 16s 11: learn: 7.3868121 total: 181ms remaining: 14.9s 12: learn: 7.3400538 total: 184ms remaining: 13.9s 13: learn: 7.2879249 total: 186ms remaining: 13.1s 14: learn: 7.2449540 total: 193ms remaining: 12.7s 15: learn: 7.1894576 total: 277ms remaining: 17.1s 16: learn: 7.1415323 total: 280ms remaining: 16.2s 17: learn: 7.1078082 total: 282ms remaining: 15.4s 18: learn: 7.0719712 total: 285ms remaining: 14.7s 19: learn: 7.0252944 total: 298ms remaining: 14.6s 20: learn: 6.9934865 total: 374ms remaining: 17.4s 21: learn: 6.9572568 total: 377ms remaining: 16.7s 22: learn: 6.9196134 total: 379ms remaining: 16.1s 23: learn: 6.8806717 total: 381ms remaining: 15.5s 24: learn: 6.8404567 total: 389ms remaining: 15.2s 25: learn: 6.8082277 total: 471ms remaining: 17.7s 26: learn: 6.7740479 total: 476ms remaining: 17.1s 27: learn: 6.7423628 total: 479ms remaining: 16.6s 28: learn: 6.7187170 total: 481ms remaining: 16.1s 29: learn: 6.6867719 total: 568ms remaining: 18.4s 30: learn: 6.6627984 total: 571ms remaining: 17.9s 31: learn: 6.6434860 total: 574ms remaining: 17.4s 32: learn: 6.6079620 total: 576ms remaining: 16.9s 33: learn: 6.5747595 total: 584ms remaining: 16.6s 34: learn: 6.5535987 total: 668ms remaining: 18.4s 35: learn: 6.5362917 total: 670ms remaining: 17.9s 36: learn: 6.5166561 total: 673ms remaining: 17.5s 37: learn: 6.4875137 total: 676ms remaining: 17.1s 38: learn: 6.4600186 total: 683ms remaining: 16.8s 39: learn: 6.4376929 total: 765ms remaining: 18.3s 40: learn: 6.4169295 total: 767ms remaining: 17.9s 41: learn: 6.3894418 total: 769ms remaining: 17.5s 42: learn: 6.3653935 total: 772ms remaining: 17.2s 43: learn: 6.3428477 total: 779ms remaining: 16.9s 44: learn: 6.3238255 total: 863ms remaining: 18.3s 45: learn: 6.3081973 total: 865ms remaining: 17.9s 46: learn: 6.2870123 total: 867ms remaining: 17.6s 47: learn: 6.2712817 total: 872ms remaining: 17.3s 48: learn: 6.2558996 total: 959ms remaining: 18.6s 49: learn: 6.2380085 total: 962ms remaining: 18.3s 50: learn: 6.2203729 total: 964ms remaining: 17.9s 51: learn: 6.1988614 total: 967ms remaining: 17.6s 52: learn: 6.1780180 total: 975ms remaining: 17.4s 53: learn: 6.1645581 total: 1.06s remaining: 18.5s 54: learn: 6.1484354 total: 1.06s remaining: 18.2s 55: learn: 6.1281721 total: 1.06s remaining: 17.9s 56: learn: 6.1146503 total: 1.06s remaining: 17.6s 57: learn: 6.0998612 total: 1.15s remaining: 18.8s 58: learn: 6.0824644 total: 1.16s remaining: 18.5s 59: learn: 6.0669359 total: 1.16s remaining: 18.2s 60: learn: 6.0547116 total: 1.16s remaining: 17.9s 61: learn: 6.0394568 total: 1.16s remaining: 17.6s 62: learn: 6.0223344 total: 1.17s remaining: 17.4s 63: learn: 6.0011750 total: 1.25s remaining: 18.3s 64: learn: 5.9849108 total: 1.25s remaining: 18.1s 65: learn: 5.9701874 total: 1.26s remaining: 17.8s 66: learn: 5.9551243 total: 1.26s remaining: 17.5s 67: learn: 5.9306202 total: 1.27s remaining: 17.4s 68: learn: 5.9135860 total: 1.35s remaining: 18.2s 69: learn: 5.9006360 total: 1.35s remaining: 18s 70: learn: 5.8903074 total: 1.36s remaining: 17.7s 71: learn: 5.8852777 total: 1.36s remaining: 17.5s 72: learn: 5.8705188 total: 1.37s remaining: 17.4s 73: learn: 5.8594710 total: 1.46s remaining: 18.3s 74: learn: 5.8445154 total: 1.47s remaining: 18.1s 75: learn: 5.8315849 total: 1.47s remaining: 17.9s 76: learn: 5.8115439 total: 1.47s remaining: 17.7s 77: learn: 5.8001994 total: 1.55s remaining: 18.3s 78: learn: 5.7874286 total: 1.55s remaining: 18.1s 79: learn: 5.7757598 total: 1.55s remaining: 17.9s 80: learn: 5.7640241 total: 1.56s remaining: 17.7s 81: learn: 5.7537453 total: 1.57s remaining: 17.6s 82: learn: 5.7436947 total: 1.64s remaining: 18.1s 83: learn: 5.7250444 total: 1.64s remaining: 17.9s 84: learn: 5.7146712 total: 1.65s remaining: 17.7s 85: learn: 5.7001721 total: 1.65s remaining: 17.5s 86: learn: 5.6855273 total: 1.66s remaining: 17.4s 87: learn: 5.6786800 total: 1.74s remaining: 18s 88: learn: 5.6681231 total: 1.74s remaining: 17.8s 89: learn: 5.6554205 total: 1.75s remaining: 17.6s 90: learn: 5.6454602 total: 1.75s remaining: 17.5s 91: learn: 5.6352421 total: 1.76s remaining: 17.4s 92: learn: 5.6234342 total: 1.84s remaining: 17.9s 93: learn: 5.6153725 total: 1.84s remaining: 17.8s 94: learn: 5.6056223 total: 1.84s remaining: 17.6s 95: learn: 5.5963865 total: 1.85s remaining: 17.4s 96: learn: 5.5874907 total: 1.85s remaining: 17.3s 97: learn: 5.5794233 total: 1.94s remaining: 17.8s 98: learn: 5.5681365 total: 1.94s remaining: 17.6s 99: learn: 5.5565902 total: 1.94s remaining: 17.5s 100: learn: 5.5476485 total: 1.94s remaining: 17.3s 101: learn: 5.5381966 total: 2.03s remaining: 17.9s 102: learn: 5.5326418 total: 2.04s remaining: 17.7s 103: learn: 5.5238155 total: 2.04s remaining: 17.6s 104: learn: 5.5180262 total: 2.04s remaining: 17.4s 105: learn: 5.5094624 total: 2.04s remaining: 17.2s 106: learn: 5.4990685 total: 2.05s remaining: 17.1s 107: learn: 5.4895483 total: 2.13s remaining: 17.6s 108: learn: 5.4809529 total: 2.13s remaining: 17.5s 109: learn: 5.4767501 total: 2.14s remaining: 17.3s 110: learn: 5.4663553 total: 2.14s remaining: 17.1s 111: learn: 5.4566162 total: 2.14s remaining: 17s 112: learn: 5.4485638 total: 2.23s remaining: 17.5s 113: learn: 5.4416017 total: 2.23s remaining: 17.4s 114: learn: 5.4327902 total: 2.23s remaining: 17.2s 115: learn: 5.4230518 total: 2.24s remaining: 17.1s 116: learn: 5.4116698 total: 2.25s remaining: 16.9s 117: learn: 5.4035858 total: 2.33s remaining: 17.4s 118: learn: 5.3960060 total: 2.33s remaining: 17.3s 119: learn: 5.3872724 total: 2.33s remaining: 17.1s 120: learn: 5.3780943 total: 2.33s remaining: 17s 121: learn: 5.3753177 total: 2.34s remaining: 16.9s 122: learn: 5.3682899 total: 2.42s remaining: 17.3s 123: learn: 5.3613156 total: 2.43s remaining: 17.1s 124: learn: 5.3513967 total: 2.43s remaining: 17s 125: learn: 5.3430297 total: 2.43s remaining: 16.9s 126: learn: 5.3391559 total: 2.44s remaining: 16.8s 127: learn: 5.3298846 total: 2.52s remaining: 17.2s 128: learn: 5.3219533 total: 2.53s remaining: 17.1s 129: learn: 5.3130333 total: 2.53s remaining: 16.9s 130: learn: 5.3040000 total: 2.53s remaining: 16.8s 131: learn: 5.2953496 total: 2.62s remaining: 17.2s 132: learn: 5.2870989 total: 2.62s remaining: 17.1s 133: learn: 5.2821485 total: 2.62s remaining: 17s 134: learn: 5.2767381 total: 2.63s remaining: 16.8s 135: learn: 5.2704406 total: 2.64s remaining: 16.7s 136: learn: 5.2636590 total: 2.72s remaining: 17.1s 137: learn: 5.2580650 total: 2.72s remaining: 17s 138: learn: 5.2531861 total: 2.72s remaining: 16.9s 139: learn: 5.2440501 total: 2.73s remaining: 16.7s 140: learn: 5.2368854 total: 2.73s remaining: 16.7s 141: learn: 5.2303881 total: 2.82s remaining: 17s 142: learn: 5.2236966 total: 2.82s remaining: 16.9s 143: learn: 5.2186475 total: 2.82s remaining: 16.8s 144: learn: 5.2084361 total: 2.82s remaining: 16.6s 145: learn: 5.2009415 total: 2.83s remaining: 16.6s 146: learn: 5.1971680 total: 2.91s remaining: 16.9s 147: learn: 5.1905406 total: 2.92s remaining: 16.8s 148: learn: 5.1867698 total: 2.92s remaining: 16.7s 149: learn: 5.1796599 total: 2.92s remaining: 16.6s 150: learn: 5.1732887 total: 2.93s remaining: 16.5s 151: learn: 5.1686048 total: 3.01s remaining: 16.8s 152: learn: 5.1638773 total: 3.01s remaining: 16.7s 153: learn: 5.1522659 total: 3.02s remaining: 16.6s 154: learn: 5.1464580 total: 3.02s remaining: 16.5s 155: learn: 5.1432412 total: 3.03s remaining: 16.4s 156: learn: 5.1370482 total: 3.11s remaining: 16.7s 157: learn: 5.1330465 total: 3.11s remaining: 16.6s 158: learn: 5.1269448 total: 3.12s remaining: 16.5s 159: learn: 5.1230983 total: 3.12s remaining: 16.4s 160: learn: 5.1167087 total: 3.13s remaining: 16.3s 161: learn: 5.1106685 total: 3.21s remaining: 16.6s 162: learn: 5.1019416 total: 3.21s remaining: 16.5s 163: learn: 5.0979975 total: 3.21s remaining: 16.4s 164: learn: 5.0910596 total: 3.21s remaining: 16.3s 165: learn: 5.0837986 total: 3.22s remaining: 16.2s 166: learn: 5.0786427 total: 3.31s remaining: 16.5s 167: learn: 5.0749346 total: 3.31s remaining: 16.4s 168: learn: 5.0713508 total: 3.31s remaining: 16.3s 169: learn: 5.0626808 total: 3.31s remaining: 16.2s 170: learn: 5.0590199 total: 3.32s remaining: 16.1s 171: learn: 5.0511884 total: 3.4s remaining: 16.4s 172: learn: 5.0436095 total: 3.41s remaining: 16.3s 173: learn: 5.0385636 total: 3.41s remaining: 16.2s 174: learn: 5.0335001 total: 3.41s remaining: 16.1s 175: learn: 5.0285511 total: 3.5s remaining: 16.4s 176: learn: 5.0215091 total: 3.5s remaining: 16.3s 177: learn: 5.0149400 total: 3.5s remaining: 16.2s 178: learn: 5.0094276 total: 3.51s remaining: 16.1s 179: learn: 5.0040345 total: 3.52s remaining: 16s 180: learn: 4.9997440 total: 3.6s remaining: 16.3s 181: learn: 4.9952747 total: 3.6s remaining: 16.2s 182: learn: 4.9913883 total: 3.6s remaining: 16.1s 183: learn: 4.9856592 total: 3.61s remaining: 16s 184: learn: 4.9790543 total: 3.62s remaining: 15.9s 185: learn: 4.9709138 total: 3.7s remaining: 16.2s 186: learn: 4.9665312 total: 3.7s remaining: 16.1s 187: learn: 4.9575560 total: 3.7s remaining: 16s 188: learn: 4.9506248 total: 3.7s remaining: 15.9s 189: learn: 4.9444609 total: 3.79s remaining: 16.2s 190: learn: 4.9418450 total: 3.8s remaining: 16.1s 191: learn: 4.9357947 total: 3.8s remaining: 16s 192: learn: 4.9278965 total: 3.8s remaining: 15.9s 193: learn: 4.9246260 total: 3.8s remaining: 15.8s 194: learn: 4.9220296 total: 3.81s remaining: 15.7s 195: learn: 4.9184070 total: 3.89s remaining: 16s 196: learn: 4.9100968 total: 3.89s remaining: 15.9s 197: learn: 4.9061396 total: 3.9s remaining: 15.8s 198: learn: 4.8992746 total: 3.9s remaining: 15.7s 199: learn: 4.8916824 total: 3.91s remaining: 15.6s 200: learn: 4.8864691 total: 3.99s remaining: 15.9s 201: learn: 4.8801288 total: 3.99s remaining: 15.8s 202: learn: 4.8760833 total: 4s remaining: 15.7s 203: learn: 4.8724722 total: 4s remaining: 15.6s 204: learn: 4.8670718 total: 4.09s remaining: 15.9s 205: learn: 4.8598153 total: 4.09s remaining: 15.8s 206: learn: 4.8515935 total: 4.09s remaining: 15.7s 207: learn: 4.8440654 total: 4.09s remaining: 15.6s 208: learn: 4.8405031 total: 4.1s remaining: 15.5s 209: learn: 4.8374213 total: 4.18s remaining: 15.7s 210: learn: 4.8354563 total: 4.19s remaining: 15.7s 211: learn: 4.8296779 total: 4.19s remaining: 15.6s 212: learn: 4.8236735 total: 4.19s remaining: 15.5s 213: learn: 4.8186799 total: 4.2s remaining: 15.4s 214: learn: 4.8123070 total: 4.28s remaining: 15.6s 215: learn: 4.8107865 total: 4.29s remaining: 15.6s 216: learn: 4.8045297 total: 4.29s remaining: 15.5s 217: learn: 4.7979472 total: 4.29s remaining: 15.4s 218: learn: 4.7867515 total: 4.3s remaining: 15.3s 219: learn: 4.7822940 total: 4.38s remaining: 15.5s 220: learn: 4.7759049 total: 4.38s remaining: 15.5s 221: learn: 4.7717890 total: 4.39s remaining: 15.4s 222: learn: 4.7696340 total: 4.39s remaining: 15.3s 223: learn: 4.7662194 total: 4.4s remaining: 15.2s 224: learn: 4.7623178 total: 4.48s remaining: 15.4s 225: learn: 4.7592261 total: 4.48s remaining: 15.3s 226: learn: 4.7569032 total: 4.48s remaining: 15.3s 227: learn: 4.7538145 total: 4.49s remaining: 15.2s 228: learn: 4.7505458 total: 4.49s remaining: 15.1s 229: learn: 4.7471202 total: 4.58s remaining: 15.3s 230: learn: 4.7427672 total: 4.58s remaining: 15.2s 231: learn: 4.7381377 total: 4.58s remaining: 15.2s 232: learn: 4.7329565 total: 4.58s remaining: 15.1s 233: learn: 4.7301634 total: 4.59s remaining: 15s 234: learn: 4.7276423 total: 4.67s remaining: 15.2s 235: learn: 4.7248806 total: 4.68s remaining: 15.1s 236: learn: 4.7185111 total: 4.68s remaining: 15.1s 237: learn: 4.7131278 total: 4.68s remaining: 15s 238: learn: 4.7085998 total: 4.69s remaining: 14.9s 239: learn: 4.7058274 total: 4.77s remaining: 15.1s 240: learn: 4.6987729 total: 4.78s remaining: 15s 241: learn: 4.6961160 total: 4.78s remaining: 15s 242: learn: 4.6909503 total: 4.78s remaining: 14.9s 243: learn: 4.6871121 total: 4.8s remaining: 14.9s 244: learn: 4.6827706 total: 4.87s remaining: 15s 245: learn: 4.6793887 total: 4.87s remaining: 14.9s 246: learn: 4.6767052 total: 4.88s remaining: 14.9s 247: learn: 4.6713101 total: 4.88s remaining: 14.8s 248: learn: 4.6677946 total: 4.89s remaining: 14.7s 249: learn: 4.6623191 total: 4.96s remaining: 14.9s 250: learn: 4.6588669 total: 4.97s remaining: 14.8s 251: learn: 4.6548923 total: 4.97s remaining: 14.8s 252: learn: 4.6495989 total: 4.97s remaining: 14.7s 253: learn: 4.6431577 total: 4.98s remaining: 14.6s 254: learn: 4.6401480 total: 5.06s remaining: 14.8s 255: learn: 4.6341419 total: 5.07s remaining: 14.7s 256: learn: 4.6278715 total: 5.07s remaining: 14.7s 257: learn: 4.6241677 total: 5.07s remaining: 14.6s 258: learn: 4.6174414 total: 5.08s remaining: 14.5s 259: learn: 4.6127705 total: 5.16s remaining: 14.7s 260: learn: 4.6088329 total: 5.16s remaining: 14.6s 261: learn: 4.6015811 total: 5.17s remaining: 14.6s 262: learn: 4.5993583 total: 5.17s remaining: 14.5s 263: learn: 4.5959346 total: 5.17s remaining: 14.4s 264: learn: 4.5913155 total: 5.26s remaining: 14.6s 265: learn: 4.5875700 total: 5.26s remaining: 14.5s 266: learn: 4.5821308 total: 5.27s remaining: 14.5s 267: learn: 4.5779484 total: 5.27s remaining: 14.4s 268: learn: 4.5741180 total: 5.28s remaining: 14.3s 269: learn: 4.5699804 total: 5.36s remaining: 14.5s 270: learn: 4.5667011 total: 5.36s remaining: 14.4s 271: learn: 4.5605928 total: 5.36s remaining: 14.4s 272: learn: 4.5558893 total: 5.36s remaining: 14.3s 273: learn: 4.5528896 total: 5.37s remaining: 14.2s 274: learn: 4.5472374 total: 5.45s remaining: 14.4s 275: learn: 4.5445202 total: 5.46s remaining: 14.3s 276: learn: 4.5396557 total: 5.46s remaining: 14.2s 277: learn: 4.5358135 total: 5.46s remaining: 14.2s 278: learn: 4.5313459 total: 5.55s remaining: 14.3s 279: learn: 4.5273423 total: 5.55s remaining: 14.3s 280: learn: 4.5206371 total: 5.55s remaining: 14.2s 281: learn: 4.5166285 total: 5.56s remaining: 14.2s 282: learn: 4.5112767 total: 5.56s remaining: 14.1s 283: learn: 4.5073618 total: 5.65s remaining: 14.2s 284: learn: 4.5020713 total: 5.65s remaining: 14.2s 285: learn: 4.4986843 total: 5.65s remaining: 14.1s 286: learn: 4.4944704 total: 5.66s remaining: 14s 287: learn: 4.4887032 total: 5.66s remaining: 14s 288: learn: 4.4838997 total: 5.67s remaining: 13.9s 289: learn: 4.4804188 total: 5.75s remaining: 14.1s 290: learn: 4.4784412 total: 5.75s remaining: 14s 291: learn: 4.4750195 total: 5.75s remaining: 13.9s 292: learn: 4.4712746 total: 5.75s remaining: 13.9s 293: learn: 4.4667650 total: 5.76s remaining: 13.8s 294: learn: 4.4607543 total: 5.84s remaining: 14s 295: learn: 4.4555890 total: 5.85s remaining: 13.9s 296: learn: 4.4535759 total: 5.85s remaining: 13.8s 297: learn: 4.4479889 total: 5.85s remaining: 13.8s 298: learn: 4.4436427 total: 5.86s remaining: 13.7s 299: learn: 4.4404406 total: 5.94s remaining: 13.9s 300: learn: 4.4341531 total: 5.95s remaining: 13.8s 301: learn: 4.4296892 total: 5.95s remaining: 13.7s 302: learn: 4.4252461 total: 5.95s remaining: 13.7s 303: learn: 4.4213359 total: 5.96s remaining: 13.6s 304: learn: 4.4163384 total: 6.04s remaining: 13.8s 305: learn: 4.4128352 total: 6.04s remaining: 13.7s 306: learn: 4.4090996 total: 6.04s remaining: 13.6s 307: learn: 4.4064145 total: 6.05s remaining: 13.6s 308: learn: 4.4049520 total: 6.05s remaining: 13.5s 309: learn: 4.4025284 total: 6.14s remaining: 13.7s 310: learn: 4.4004192 total: 6.14s remaining: 13.6s 311: learn: 4.3989059 total: 6.14s remaining: 13.5s 312: learn: 4.3941600 total: 6.15s remaining: 13.5s 313: learn: 4.3909085 total: 6.16s remaining: 13.5s 314: learn: 4.3884150 total: 6.23s remaining: 13.6s 315: learn: 4.3838816 total: 6.24s remaining: 13.5s 316: learn: 4.3801980 total: 6.24s remaining: 13.4s 317: learn: 4.3747378 total: 6.24s remaining: 13.4s 318: learn: 4.3680260 total: 6.25s remaining: 13.3s 319: learn: 4.3632477 total: 6.33s remaining: 13.5s 320: learn: 4.3625440 total: 6.33s remaining: 13.4s 321: learn: 4.3580639 total: 6.34s remaining: 13.3s 322: learn: 4.3550094 total: 6.34s remaining: 13.3s 323: learn: 4.3514399 total: 6.34s remaining: 13.2s 324: learn: 4.3487689 total: 6.43s remaining: 13.4s 325: learn: 4.3417666 total: 6.43s remaining: 13.3s 326: learn: 4.3383467 total: 6.43s remaining: 13.2s 327: learn: 4.3322835 total: 6.44s remaining: 13.2s 328: learn: 4.3275025 total: 6.44s remaining: 13.1s 329: learn: 4.3234267 total: 6.53s remaining: 13.3s 330: learn: 4.3196456 total: 6.53s remaining: 13.2s 331: learn: 4.3157418 total: 6.53s remaining: 13.1s 332: learn: 4.3116234 total: 6.54s remaining: 13.1s 333: learn: 4.3050365 total: 6.54s remaining: 13s 334: learn: 4.3020212 total: 6.63s remaining: 13.2s 335: learn: 4.2997644 total: 6.63s remaining: 13.1s 336: learn: 4.2956510 total: 6.63s remaining: 13s 337: learn: 4.2923453 total: 6.63s remaining: 13s 338: learn: 4.2893912 total: 6.64s remaining: 12.9s 339: learn: 4.2837924 total: 6.72s remaining: 13.1s 340: learn: 4.2786004 total: 6.72s remaining: 13s 341: learn: 4.2766528 total: 6.73s remaining: 12.9s 342: learn: 4.2748298 total: 6.73s remaining: 12.9s 343: learn: 4.2715049 total: 6.82s remaining: 13s 344: learn: 4.2669223 total: 6.82s remaining: 13s 345: learn: 4.2616741 total: 6.83s remaining: 12.9s 346: learn: 4.2581641 total: 6.83s remaining: 12.9s 347: learn: 4.2539284 total: 6.84s remaining: 12.8s 348: learn: 4.2521872 total: 6.92s remaining: 12.9s 349: learn: 4.2473718 total: 6.92s remaining: 12.9s 350: learn: 4.2450427 total: 6.92s remaining: 12.8s 351: learn: 4.2393681 total: 6.93s remaining: 12.8s 352: learn: 4.2349908 total: 7.02s remaining: 12.9s 353: learn: 4.2316683 total: 7.02s remaining: 12.8s 354: learn: 4.2294995 total: 7.02s remaining: 12.8s 355: learn: 4.2250415 total: 7.02s remaining: 12.7s 356: learn: 4.2217354 total: 7.03s remaining: 12.7s 357: learn: 4.2175437 total: 7.12s remaining: 12.8s 358: learn: 4.2125398 total: 7.12s remaining: 12.7s 359: learn: 4.2097134 total: 7.13s remaining: 12.7s 360: learn: 4.2018761 total: 7.13s remaining: 12.6s 361: learn: 4.1975462 total: 7.21s remaining: 12.7s 362: learn: 4.1942797 total: 7.21s remaining: 12.7s 363: learn: 4.1907273 total: 7.22s remaining: 12.6s 364: learn: 4.1862434 total: 7.22s remaining: 12.6s 365: learn: 4.1839770 total: 7.22s remaining: 12.5s 366: learn: 4.1797920 total: 7.31s remaining: 12.6s 367: learn: 4.1745241 total: 7.31s remaining: 12.6s 368: learn: 4.1712157 total: 7.31s remaining: 12.5s 369: learn: 4.1659542 total: 7.32s remaining: 12.5s 370: learn: 4.1636786 total: 7.32s remaining: 12.4s 371: learn: 4.1596716 total: 7.33s remaining: 12.4s 372: learn: 4.1550049 total: 7.4s remaining: 12.4s 373: learn: 4.1509016 total: 7.41s remaining: 12.4s 374: learn: 4.1450710 total: 7.41s remaining: 12.4s 375: learn: 4.1419712 total: 7.42s remaining: 12.3s 376: learn: 4.1381351 total: 7.5s remaining: 12.4s 377: learn: 4.1349879 total: 7.5s remaining: 12.3s 378: learn: 4.1291376 total: 7.5s remaining: 12.3s 379: learn: 4.1233669 total: 7.51s remaining: 12.3s 380: learn: 4.1198785 total: 7.52s remaining: 12.2s 381: learn: 4.1145537 total: 7.6s remaining: 12.3s 382: learn: 4.1098613 total: 7.6s remaining: 12.2s 383: learn: 4.1067455 total: 7.6s remaining: 12.2s 384: learn: 4.1039428 total: 7.61s remaining: 12.2s 385: learn: 4.1000862 total: 7.69s remaining: 12.2s 386: learn: 4.0957939 total: 7.7s remaining: 12.2s 387: learn: 4.0925956 total: 7.7s remaining: 12.1s 388: learn: 4.0912750 total: 7.7s remaining: 12.1s 389: learn: 4.0883566 total: 7.7s remaining: 12.1s 390: learn: 4.0848798 total: 7.71s remaining: 12s 391: learn: 4.0831652 total: 7.79s remaining: 12.1s 392: learn: 4.0769909 total: 7.8s remaining: 12s 393: learn: 4.0734556 total: 7.8s remaining: 12s 394: learn: 4.0689413 total: 7.8s remaining: 12s 395: learn: 4.0674611 total: 7.89s remaining: 12s 396: learn: 4.0661354 total: 7.89s remaining: 12s 397: learn: 4.0626335 total: 7.9s remaining: 11.9s 398: learn: 4.0581185 total: 7.9s remaining: 11.9s 399: learn: 4.0546182 total: 7.91s remaining: 11.9s 400: learn: 4.0494440 total: 7.99s remaining: 11.9s 401: learn: 4.0467084 total: 7.99s remaining: 11.9s 402: learn: 4.0437444 total: 8s remaining: 11.8s 403: learn: 4.0407245 total: 8s remaining: 11.8s 404: learn: 4.0372642 total: 8s remaining: 11.8s 405: learn: 4.0339581 total: 8.09s remaining: 11.8s 406: learn: 4.0314237 total: 8.09s remaining: 11.8s 407: learn: 4.0291123 total: 8.09s remaining: 11.7s 408: learn: 4.0256872 total: 8.1s remaining: 11.7s 409: learn: 4.0220849 total: 8.1s remaining: 11.7s 410: learn: 4.0188100 total: 8.18s remaining: 11.7s 411: learn: 4.0158882 total: 8.19s remaining: 11.7s 412: learn: 4.0119280 total: 8.19s remaining: 11.6s 413: learn: 4.0080036 total: 8.19s remaining: 11.6s 414: learn: 4.0060991 total: 8.2s remaining: 11.6s 415: learn: 4.0026295 total: 8.28s remaining: 11.6s 416: learn: 4.0007553 total: 8.29s remaining: 11.6s 417: learn: 3.9994706 total: 8.29s remaining: 11.5s 418: learn: 3.9956060 total: 8.29s remaining: 11.5s 419: learn: 3.9886537 total: 8.38s remaining: 11.6s 420: learn: 3.9867591 total: 8.38s remaining: 11.5s 421: learn: 3.9842122 total: 8.38s remaining: 11.5s 422: learn: 3.9811326 total: 8.38s remaining: 11.4s 423: learn: 3.9778463 total: 8.39s remaining: 11.4s 424: learn: 3.9754097 total: 8.39s remaining: 11.4s 425: learn: 3.9721072 total: 8.48s remaining: 11.4s 426: learn: 3.9677454 total: 8.48s remaining: 11.4s 427: learn: 3.9656854 total: 8.48s remaining: 11.3s 428: learn: 3.9628612 total: 8.48s remaining: 11.3s 429: learn: 3.9598889 total: 8.49s remaining: 11.3s 430: learn: 3.9555624 total: 8.57s remaining: 11.3s 431: learn: 3.9527864 total: 8.58s remaining: 11.3s 432: learn: 3.9510360 total: 8.58s remaining: 11.2s 433: learn: 3.9483118 total: 8.58s remaining: 11.2s 434: learn: 3.9459516 total: 8.59s remaining: 11.2s 435: learn: 3.9441995 total: 8.67s remaining: 11.2s 436: learn: 3.9415756 total: 8.67s remaining: 11.2s 437: learn: 3.9366793 total: 8.68s remaining: 11.1s 438: learn: 3.9307614 total: 8.68s remaining: 11.1s 439: learn: 3.9287469 total: 8.69s remaining: 11.1s 440: learn: 3.9250111 total: 8.77s remaining: 11.1s 441: learn: 3.9221957 total: 8.77s remaining: 11.1s 442: learn: 3.9196190 total: 8.78s remaining: 11s 443: learn: 3.9179502 total: 8.78s remaining: 11s 444: learn: 3.9149726 total: 8.78s remaining: 11s 445: learn: 3.9117242 total: 8.87s remaining: 11s 446: learn: 3.9080126 total: 8.87s remaining: 11s 447: learn: 3.9046539 total: 8.87s remaining: 10.9s 448: learn: 3.9029134 total: 8.87s remaining: 10.9s 449: learn: 3.9009247 total: 8.88s remaining: 10.9s 450: learn: 3.8992076 total: 8.96s remaining: 10.9s 451: learn: 3.8957683 total: 8.97s remaining: 10.9s 452: learn: 3.8938480 total: 8.97s remaining: 10.8s 453: learn: 3.8911098 total: 8.97s remaining: 10.8s 454: learn: 3.8902656 total: 8.98s remaining: 10.8s 455: learn: 3.8870731 total: 9.06s remaining: 10.8s 456: learn: 3.8858632 total: 9.06s remaining: 10.8s 457: learn: 3.8845429 total: 9.07s remaining: 10.7s 458: learn: 3.8799293 total: 9.07s remaining: 10.7s 459: learn: 3.8786298 total: 9.08s remaining: 10.7s 460: learn: 3.8771816 total: 9.16s remaining: 10.7s 461: learn: 3.8732192 total: 9.16s remaining: 10.7s 462: learn: 3.8710574 total: 9.16s remaining: 10.6s 463: learn: 3.8689366 total: 9.17s remaining: 10.6s 464: learn: 3.8671273 total: 9.17s remaining: 10.6s 465: learn: 3.8637306 total: 9.26s remaining: 10.6s 466: learn: 3.8598419 total: 9.26s remaining: 10.6s 467: learn: 3.8567917 total: 9.26s remaining: 10.5s 468: learn: 3.8538146 total: 9.27s remaining: 10.5s 469: learn: 3.8515353 total: 9.35s remaining: 10.5s 470: learn: 3.8495452 total: 9.36s remaining: 10.5s 471: learn: 3.8456474 total: 9.36s remaining: 10.5s 472: learn: 3.8428045 total: 9.36s remaining: 10.4s 473: learn: 3.8397443 total: 9.36s remaining: 10.4s 474: learn: 3.8377518 total: 9.37s remaining: 10.4s 475: learn: 3.8349231 total: 9.45s remaining: 10.4s 476: learn: 3.8333713 total: 9.45s remaining: 10.4s 477: learn: 3.8317569 total: 9.46s remaining: 10.3s 478: learn: 3.8300623 total: 9.46s remaining: 10.3s 479: learn: 3.8284024 total: 9.47s remaining: 10.3s 480: learn: 3.8273959 total: 9.55s remaining: 10.3s 481: learn: 3.8252996 total: 9.55s remaining: 10.3s 482: learn: 3.8235241 total: 9.55s remaining: 10.2s 483: learn: 3.8214497 total: 9.56s remaining: 10.2s 484: learn: 3.8202073 total: 9.56s remaining: 10.2s 485: learn: 3.8153884 total: 9.65s remaining: 10.2s 486: learn: 3.8136865 total: 9.65s remaining: 10.2s 487: learn: 3.8104433 total: 9.65s remaining: 10.1s 488: learn: 3.8081962 total: 9.65s remaining: 10.1s 489: learn: 3.8041561 total: 9.66s remaining: 10.1s 490: learn: 3.8032791 total: 9.75s remaining: 10.1s 491: learn: 3.8002779 total: 9.75s remaining: 10.1s 492: learn: 3.7974712 total: 9.75s remaining: 10s 493: learn: 3.7965829 total: 9.75s remaining: 9.99s 494: learn: 3.7935984 total: 9.76s remaining: 9.96s 495: learn: 3.7921962 total: 9.84s remaining: 10s 496: learn: 3.7873762 total: 9.85s remaining: 9.96s 497: learn: 3.7856692 total: 9.85s remaining: 9.93s 498: learn: 3.7849548 total: 9.85s remaining: 9.89s 499: learn: 3.7826048 total: 9.86s remaining: 9.86s 500: learn: 3.7817893 total: 9.94s remaining: 9.9s 501: learn: 3.7798693 total: 9.94s remaining: 9.86s 502: learn: 3.7761611 total: 9.95s remaining: 9.83s 503: learn: 3.7730293 total: 9.95s remaining: 9.79s 504: learn: 3.7714301 total: 9.95s remaining: 9.75s 505: learn: 3.7698978 total: 10s remaining: 9.8s 506: learn: 3.7677057 total: 10s remaining: 9.76s 507: learn: 3.7661161 total: 10s remaining: 9.73s 508: learn: 3.7648347 total: 10s remaining: 9.69s 509: learn: 3.7611307 total: 10.1s remaining: 9.66s 510: learn: 3.7584314 total: 10.1s remaining: 9.7s 511: learn: 3.7567833 total: 10.1s remaining: 9.66s 512: learn: 3.7551805 total: 10.1s remaining: 9.63s 513: learn: 3.7522163 total: 10.1s remaining: 9.59s 514: learn: 3.7504819 total: 10.1s remaining: 9.55s 515: learn: 3.7483574 total: 10.2s remaining: 9.6s 516: learn: 3.7452892 total: 10.2s remaining: 9.56s 517: learn: 3.7412042 total: 10.2s remaining: 9.53s 518: learn: 3.7392695 total: 10.2s remaining: 9.49s 519: learn: 3.7378030 total: 10.2s remaining: 9.46s 520: learn: 3.7363342 total: 10.3s remaining: 9.5s 521: learn: 3.7345634 total: 10.3s remaining: 9.46s 522: learn: 3.7321662 total: 10.3s remaining: 9.43s 523: learn: 3.7311070 total: 10.3s remaining: 9.39s 524: learn: 3.7296406 total: 10.3s remaining: 9.36s 525: learn: 3.7284312 total: 10.3s remaining: 9.32s 526: learn: 3.7249652 total: 10.4s remaining: 9.36s 527: learn: 3.7242044 total: 10.4s remaining: 9.32s 528: learn: 3.7235733 total: 10.4s remaining: 9.29s 529: learn: 3.7195998 total: 10.4s remaining: 9.26s 530: learn: 3.7186223 total: 10.4s remaining: 9.22s 531: learn: 3.7172148 total: 10.5s remaining: 9.26s 532: learn: 3.7150550 total: 10.5s remaining: 9.22s 533: learn: 3.7136453 total: 10.5s remaining: 9.19s 534: learn: 3.7108118 total: 10.5s remaining: 9.16s 535: learn: 3.7093073 total: 10.5s remaining: 9.13s 536: learn: 3.7082805 total: 10.6s remaining: 9.16s 537: learn: 3.7060521 total: 10.6s remaining: 9.13s 538: learn: 3.7049623 total: 10.6s remaining: 9.09s 539: learn: 3.7018914 total: 10.6s remaining: 9.06s 540: learn: 3.7007568 total: 10.6s remaining: 9.02s 541: learn: 3.6986173 total: 10.7s remaining: 9.06s 542: learn: 3.6960773 total: 10.7s remaining: 9.03s 543: learn: 3.6935799 total: 10.7s remaining: 8.99s 544: learn: 3.6900813 total: 10.7s remaining: 8.96s 545: learn: 3.6889124 total: 10.7s remaining: 8.92s 546: learn: 3.6865814 total: 10.8s remaining: 8.96s 547: learn: 3.6850859 total: 10.8s remaining: 8.93s 548: learn: 3.6816916 total: 10.8s remaining: 8.89s 549: learn: 3.6792062 total: 10.8s remaining: 8.86s 550: learn: 3.6764268 total: 10.8s remaining: 8.82s 551: learn: 3.6732831 total: 10.9s remaining: 8.86s 552: learn: 3.6722013 total: 10.9s remaining: 8.82s 553: learn: 3.6713029 total: 10.9s remaining: 8.79s 554: learn: 3.6698550 total: 10.9s remaining: 8.76s 555: learn: 3.6663527 total: 10.9s remaining: 8.72s 556: learn: 3.6651872 total: 10.9s remaining: 8.7s 557: learn: 3.6606142 total: 11s remaining: 8.72s 558: learn: 3.6590152 total: 11s remaining: 8.69s 559: learn: 3.6567410 total: 11s remaining: 8.66s 560: learn: 3.6540511 total: 11s remaining: 8.63s 561: learn: 3.6525069 total: 11.1s remaining: 8.66s 562: learn: 3.6491564 total: 11.1s remaining: 8.63s 563: learn: 3.6476297 total: 11.1s remaining: 8.59s 564: learn: 3.6465462 total: 11.1s remaining: 8.56s 565: learn: 3.6448051 total: 11.1s remaining: 8.53s 566: learn: 3.6415337 total: 11.1s remaining: 8.5s 567: learn: 3.6384531 total: 11.2s remaining: 8.53s 568: learn: 3.6362624 total: 11.2s remaining: 8.49s 569: learn: 3.6346312 total: 11.2s remaining: 8.46s 570: learn: 3.6325152 total: 11.2s remaining: 8.43s 571: learn: 3.6309543 total: 11.2s remaining: 8.4s 572: learn: 3.6304172 total: 11.3s remaining: 8.43s 573: learn: 3.6282196 total: 11.3s remaining: 8.39s 574: learn: 3.6266656 total: 11.3s remaining: 8.36s 575: learn: 3.6246413 total: 11.3s remaining: 8.33s 576: learn: 3.6218372 total: 11.4s remaining: 8.36s 577: learn: 3.6199659 total: 11.4s remaining: 8.33s 578: learn: 3.6176220 total: 11.4s remaining: 8.3s 579: learn: 3.6164343 total: 11.4s remaining: 8.26s 580: learn: 3.6144863 total: 11.4s remaining: 8.23s 581: learn: 3.6133771 total: 11.5s remaining: 8.26s 582: learn: 3.6117889 total: 11.5s remaining: 8.23s 583: learn: 3.6106600 total: 11.5s remaining: 8.2s 584: learn: 3.6087033 total: 11.5s remaining: 8.16s 585: learn: 3.6060387 total: 11.5s remaining: 8.13s 586: learn: 3.6042227 total: 11.5s remaining: 8.1s 587: learn: 3.6028092 total: 11.6s remaining: 8.13s 588: learn: 3.6011954 total: 11.6s remaining: 8.1s 589: learn: 3.5999982 total: 11.6s remaining: 8.06s 590: learn: 3.5997258 total: 11.6s remaining: 8.03s 591: learn: 3.5967138 total: 11.6s remaining: 8s 592: learn: 3.5958755 total: 11.7s remaining: 8.03s 593: learn: 3.5934605 total: 11.7s remaining: 8s 594: learn: 3.5925477 total: 11.7s remaining: 7.97s 595: learn: 3.5919440 total: 11.7s remaining: 7.93s 596: learn: 3.5900084 total: 11.7s remaining: 7.91s 597: learn: 3.5892290 total: 11.8s remaining: 7.93s 598: learn: 3.5877425 total: 11.8s remaining: 7.9s 599: learn: 3.5844309 total: 11.8s remaining: 7.87s 600: learn: 3.5810486 total: 11.8s remaining: 7.84s 601: learn: 3.5773400 total: 11.9s remaining: 7.86s 602: learn: 3.5760316 total: 11.9s remaining: 7.83s 603: learn: 3.5737789 total: 11.9s remaining: 7.8s 604: learn: 3.5727233 total: 11.9s remaining: 7.77s 605: learn: 3.5715228 total: 11.9s remaining: 7.75s 606: learn: 3.5688508 total: 12s remaining: 7.76s 607: learn: 3.5660763 total: 12s remaining: 7.73s 608: learn: 3.5637492 total: 12s remaining: 7.7s 609: learn: 3.5628672 total: 12s remaining: 7.67s 610: learn: 3.5598081 total: 12s remaining: 7.64s 611: learn: 3.5584112 total: 12.1s remaining: 7.67s 612: learn: 3.5559394 total: 12.1s remaining: 7.64s 613: learn: 3.5552895 total: 12.1s remaining: 7.61s 614: learn: 3.5531618 total: 12.1s remaining: 7.58s 615: learn: 3.5503725 total: 12.2s remaining: 7.59s 616: learn: 3.5482991 total: 12.2s remaining: 7.57s 617: learn: 3.5469162 total: 12.2s remaining: 7.54s 618: learn: 3.5461291 total: 12.2s remaining: 7.5s 619: learn: 3.5453465 total: 12.2s remaining: 7.47s 620: learn: 3.5437542 total: 12.3s remaining: 7.49s 621: learn: 3.5414621 total: 12.3s remaining: 7.46s 622: learn: 3.5400041 total: 12.3s remaining: 7.43s 623: learn: 3.5388662 total: 12.3s remaining: 7.41s 624: learn: 3.5373892 total: 12.3s remaining: 7.38s 625: learn: 3.5358684 total: 12.4s remaining: 7.39s 626: learn: 3.5331817 total: 12.4s remaining: 7.37s 627: learn: 3.5317113 total: 12.4s remaining: 7.33s 628: learn: 3.5314592 total: 12.4s remaining: 7.3s 629: learn: 3.5297615 total: 12.4s remaining: 7.28s 630: learn: 3.5281356 total: 12.5s remaining: 7.29s 631: learn: 3.5260987 total: 12.5s remaining: 7.26s 632: learn: 3.5250746 total: 12.5s remaining: 7.24s 633: learn: 3.5237136 total: 12.5s remaining: 7.21s 634: learn: 3.5221614 total: 12.5s remaining: 7.18s 635: learn: 3.5209886 total: 12.6s remaining: 7.19s 636: learn: 3.5204814 total: 12.6s remaining: 7.16s 637: learn: 3.5193343 total: 12.6s remaining: 7.13s 638: learn: 3.5182138 total: 12.6s remaining: 7.11s 639: learn: 3.5164131 total: 12.6s remaining: 7.08s 640: learn: 3.5155470 total: 12.7s remaining: 7.09s 641: learn: 3.5142737 total: 12.7s remaining: 7.06s 642: learn: 3.5131941 total: 12.7s remaining: 7.04s 643: learn: 3.5114180 total: 12.7s remaining: 7.01s 644: learn: 3.5104208 total: 12.7s remaining: 6.98s 645: learn: 3.5092384 total: 12.8s remaining: 6.99s 646: learn: 3.5082901 total: 12.8s remaining: 6.96s 647: learn: 3.5071604 total: 12.8s remaining: 6.93s 648: learn: 3.5050310 total: 12.8s remaining: 6.91s 649: learn: 3.5038809 total: 12.8s remaining: 6.88s 650: learn: 3.5016077 total: 12.9s remaining: 6.89s 651: learn: 3.4987192 total: 12.9s remaining: 6.87s 652: learn: 3.4977137 total: 12.9s remaining: 6.84s 653: learn: 3.4955634 total: 12.9s remaining: 6.81s 654: learn: 3.4933200 total: 12.9s remaining: 6.78s 655: learn: 3.4915519 total: 13s remaining: 6.79s 656: learn: 3.4889138 total: 13s remaining: 6.77s 657: learn: 3.4864485 total: 13s remaining: 6.74s 658: learn: 3.4852677 total: 13s remaining: 6.71s 659: learn: 3.4837828 total: 13.1s remaining: 6.72s 660: learn: 3.4829241 total: 13.1s remaining: 6.7s 661: learn: 3.4820559 total: 13.1s remaining: 6.67s 662: learn: 3.4805210 total: 13.1s remaining: 6.64s 663: learn: 3.4787329 total: 13.1s remaining: 6.61s 664: learn: 3.4772271 total: 13.2s remaining: 6.63s 665: learn: 3.4761143 total: 13.2s remaining: 6.6s 666: learn: 3.4740960 total: 13.2s remaining: 6.57s 667: learn: 3.4716803 total: 13.2s remaining: 6.54s 668: learn: 3.4701737 total: 13.2s remaining: 6.51s 669: learn: 3.4699194 total: 13.3s remaining: 6.53s 670: learn: 3.4685933 total: 13.3s remaining: 6.5s 671: learn: 3.4669390 total: 13.3s remaining: 6.47s 672: learn: 3.4638777 total: 13.3s remaining: 6.44s 673: learn: 3.4631283 total: 13.3s remaining: 6.42s 674: learn: 3.4611957 total: 13.3s remaining: 6.43s 675: learn: 3.4606208 total: 13.4s remaining: 6.4s 676: learn: 3.4584662 total: 13.4s remaining: 6.37s 677: learn: 3.4577831 total: 13.4s remaining: 6.34s 678: learn: 3.4557525 total: 13.4s remaining: 6.32s 679: learn: 3.4539224 total: 13.4s remaining: 6.33s 680: learn: 3.4509009 total: 13.4s remaining: 6.3s 681: learn: 3.4497278 total: 13.5s remaining: 6.27s 682: learn: 3.4479337 total: 13.5s remaining: 6.24s 683: learn: 3.4455338 total: 13.5s remaining: 6.22s 684: learn: 3.4449417 total: 13.5s remaining: 6.23s 685: learn: 3.4444453 total: 13.5s remaining: 6.2s 686: learn: 3.4427153 total: 13.6s remaining: 6.17s 687: learn: 3.4406000 total: 13.6s remaining: 6.14s 688: learn: 3.4392924 total: 13.6s remaining: 6.12s 689: learn: 3.4382639 total: 13.6s remaining: 6.13s 690: learn: 3.4374775 total: 13.6s remaining: 6.1s 691: learn: 3.4364144 total: 13.6s remaining: 6.07s 692: learn: 3.4354031 total: 13.7s remaining: 6.05s 693: learn: 3.4348887 total: 13.7s remaining: 6.02s 694: learn: 3.4332960 total: 13.7s remaining: 6.03s 695: learn: 3.4329480 total: 13.7s remaining: 6s 696: learn: 3.4318210 total: 13.7s remaining: 5.98s 697: learn: 3.4300438 total: 13.7s remaining: 5.95s 698: learn: 3.4289454 total: 13.8s remaining: 5.96s 699: learn: 3.4276542 total: 13.8s remaining: 5.93s 700: learn: 3.4262781 total: 13.8s remaining: 5.9s 701: learn: 3.4244482 total: 13.8s remaining: 5.88s 702: learn: 3.4238634 total: 13.8s remaining: 5.85s 703: learn: 3.4231684 total: 13.9s remaining: 5.86s 704: learn: 3.4222185 total: 13.9s remaining: 5.83s 705: learn: 3.4214530 total: 13.9s remaining: 5.8s 706: learn: 3.4194276 total: 13.9s remaining: 5.78s 707: learn: 3.4184285 total: 14s remaining: 5.75s 708: learn: 3.4181808 total: 14s remaining: 5.76s 709: learn: 3.4158372 total: 14s remaining: 5.73s 710: learn: 3.4129137 total: 14s remaining: 5.71s 711: learn: 3.4112063 total: 14s remaining: 5.68s 712: learn: 3.4104808 total: 14.1s remaining: 5.66s 713: learn: 3.4092005 total: 14.1s remaining: 5.66s 714: learn: 3.4082539 total: 14.1s remaining: 5.63s 715: learn: 3.4061717 total: 14.1s remaining: 5.61s 716: learn: 3.4035046 total: 14.1s remaining: 5.58s 717: learn: 3.4014236 total: 14.1s remaining: 5.56s 718: learn: 3.4006329 total: 14.2s remaining: 5.56s 719: learn: 3.3995283 total: 14.2s remaining: 5.54s 720: learn: 3.3978196 total: 14.2s remaining: 5.51s 721: learn: 3.3960545 total: 14.2s remaining: 5.48s 722: learn: 3.3935368 total: 14.3s remaining: 5.49s 723: learn: 3.3911667 total: 14.3s remaining: 5.46s 724: learn: 3.3891393 total: 14.3s remaining: 5.44s 725: learn: 3.3871146 total: 14.3s remaining: 5.41s 726: learn: 3.3858376 total: 14.3s remaining: 5.38s 727: learn: 3.3850437 total: 14.3s remaining: 5.36s 728: learn: 3.3841250 total: 14.4s remaining: 5.36s 729: learn: 3.3835264 total: 14.4s remaining: 5.34s 730: learn: 3.3826066 total: 14.4s remaining: 5.31s 731: learn: 3.3811678 total: 14.4s remaining: 5.28s 732: learn: 3.3777990 total: 14.4s remaining: 5.26s 733: learn: 3.3763171 total: 14.5s remaining: 5.26s 734: learn: 3.3750248 total: 14.5s remaining: 5.24s 735: learn: 3.3746756 total: 14.5s remaining: 5.21s 736: learn: 3.3733322 total: 14.5s remaining: 5.18s 737: learn: 3.3715282 total: 14.5s remaining: 5.16s 738: learn: 3.3708568 total: 14.6s remaining: 5.16s 739: learn: 3.3699546 total: 14.6s remaining: 5.14s 740: learn: 3.3690532 total: 14.6s remaining: 5.11s 741: learn: 3.3668951 total: 14.6s remaining: 5.09s 742: learn: 3.3647964 total: 14.6s remaining: 5.06s 743: learn: 3.3618177 total: 14.7s remaining: 5.06s 744: learn: 3.3602851 total: 14.7s remaining: 5.04s 745: learn: 3.3587076 total: 14.7s remaining: 5.01s 746: learn: 3.3576963 total: 14.7s remaining: 4.99s 747: learn: 3.3573048 total: 14.7s remaining: 4.96s 748: learn: 3.3562879 total: 14.7s remaining: 4.94s 749: learn: 3.3552186 total: 14.8s remaining: 4.94s 750: learn: 3.3538874 total: 14.8s remaining: 4.91s 751: learn: 3.3524014 total: 14.8s remaining: 4.89s 752: learn: 3.3508676 total: 14.8s remaining: 4.86s 753: learn: 3.3494016 total: 14.9s remaining: 4.87s 754: learn: 3.3475248 total: 14.9s remaining: 4.84s 755: learn: 3.3467518 total: 14.9s remaining: 4.81s 756: learn: 3.3460847 total: 14.9s remaining: 4.79s 757: learn: 3.3446721 total: 14.9s remaining: 4.76s 758: learn: 3.3441657 total: 15s remaining: 4.77s 759: learn: 3.3422403 total: 15s remaining: 4.74s 760: learn: 3.3408967 total: 15s remaining: 4.71s 761: learn: 3.3392488 total: 15s remaining: 4.69s 762: learn: 3.3381024 total: 15s remaining: 4.67s 763: learn: 3.3377391 total: 15.1s remaining: 4.67s 764: learn: 3.3356044 total: 15.1s remaining: 4.64s 765: learn: 3.3349119 total: 15.1s remaining: 4.62s 766: learn: 3.3334900 total: 15.1s remaining: 4.59s 767: learn: 3.3316682 total: 15.1s remaining: 4.57s 768: learn: 3.3308483 total: 15.2s remaining: 4.57s 769: learn: 3.3295313 total: 15.2s remaining: 4.54s 770: learn: 3.3285986 total: 15.2s remaining: 4.52s 771: learn: 3.3270096 total: 15.2s remaining: 4.49s 772: learn: 3.3257082 total: 15.2s remaining: 4.47s 773: learn: 3.3249572 total: 15.3s remaining: 4.47s 774: learn: 3.3242294 total: 15.3s remaining: 4.44s 775: learn: 3.3232504 total: 15.3s remaining: 4.42s 776: learn: 3.3224960 total: 15.3s remaining: 4.39s 777: learn: 3.3210173 total: 15.3s remaining: 4.37s 778: learn: 3.3197188 total: 15.4s remaining: 4.37s 779: learn: 3.3191239 total: 15.4s remaining: 4.34s 780: learn: 3.3188810 total: 15.4s remaining: 4.32s 781: learn: 3.3175516 total: 15.4s remaining: 4.29s 782: learn: 3.3161376 total: 15.4s remaining: 4.27s 783: learn: 3.3157570 total: 15.5s remaining: 4.27s 784: learn: 3.3149482 total: 15.5s remaining: 4.25s 785: learn: 3.3147086 total: 15.5s remaining: 4.22s 786: learn: 3.3144920 total: 15.5s remaining: 4.2s 787: learn: 3.3137537 total: 15.5s remaining: 4.17s 788: learn: 3.3134988 total: 15.6s remaining: 4.17s 789: learn: 3.3122346 total: 15.6s remaining: 4.15s 790: learn: 3.3120463 total: 15.6s remaining: 4.12s 791: learn: 3.3114500 total: 15.6s remaining: 4.1s 792: learn: 3.3108517 total: 15.6s remaining: 4.07s 793: learn: 3.3093168 total: 15.7s remaining: 4.07s 794: learn: 3.3074970 total: 15.7s remaining: 4.05s 795: learn: 3.3058097 total: 15.7s remaining: 4.02s 796: learn: 3.3035428 total: 15.7s remaining: 4s 797: learn: 3.3023733 total: 15.7s remaining: 3.98s 798: learn: 3.3009091 total: 15.8s remaining: 3.97s 799: learn: 3.2996240 total: 15.8s remaining: 3.95s 800: learn: 3.2991118 total: 15.8s remaining: 3.92s 801: learn: 3.2980528 total: 15.8s remaining: 3.9s 802: learn: 3.2975762 total: 15.8s remaining: 3.88s 803: learn: 3.2947891 total: 15.9s remaining: 3.87s 804: learn: 3.2935298 total: 15.9s remaining: 3.85s 805: learn: 3.2919772 total: 15.9s remaining: 3.83s 806: learn: 3.2894361 total: 15.9s remaining: 3.8s 807: learn: 3.2879214 total: 15.9s remaining: 3.78s 808: learn: 3.2864700 total: 16s remaining: 3.77s 809: learn: 3.2847087 total: 16s remaining: 3.75s 810: learn: 3.2835180 total: 16s remaining: 3.73s 811: learn: 3.2826170 total: 16s remaining: 3.7s 812: learn: 3.2780154 total: 16s remaining: 3.68s 813: learn: 3.2776817 total: 16.1s remaining: 3.67s 814: learn: 3.2763416 total: 16.1s remaining: 3.65s 815: learn: 3.2731236 total: 16.1s remaining: 3.63s 816: learn: 3.2720981 total: 16.1s remaining: 3.6s 817: learn: 3.2714399 total: 16.1s remaining: 3.58s 818: learn: 3.2698429 total: 16.2s remaining: 3.58s 819: learn: 3.2686656 total: 16.2s remaining: 3.55s 820: learn: 3.2675157 total: 16.2s remaining: 3.53s 821: learn: 3.2663124 total: 16.2s remaining: 3.51s 822: learn: 3.2650838 total: 16.2s remaining: 3.48s 823: learn: 3.2644593 total: 16.3s remaining: 3.48s 824: learn: 3.2633756 total: 16.3s remaining: 3.45s 825: learn: 3.2621100 total: 16.3s remaining: 3.43s 826: learn: 3.2611301 total: 16.3s remaining: 3.41s 827: learn: 3.2604969 total: 16.3s remaining: 3.38s 828: learn: 3.2598528 total: 16.3s remaining: 3.36s 829: learn: 3.2585778 total: 16.4s remaining: 3.35s 830: learn: 3.2583633 total: 16.4s remaining: 3.33s 831: learn: 3.2568287 total: 16.4s remaining: 3.31s 832: learn: 3.2551263 total: 16.4s remaining: 3.28s 833: learn: 3.2536031 total: 16.5s remaining: 3.28s 834: learn: 3.2528220 total: 16.5s remaining: 3.25s 835: learn: 3.2526506 total: 16.5s remaining: 3.23s 836: learn: 3.2518332 total: 16.5s remaining: 3.21s 837: learn: 3.2502660 total: 16.5s remaining: 3.19s 838: learn: 3.2488832 total: 16.6s remaining: 3.18s 839: learn: 3.2474279 total: 16.6s remaining: 3.16s 840: learn: 3.2466913 total: 16.6s remaining: 3.13s 841: learn: 3.2452360 total: 16.6s remaining: 3.11s 842: learn: 3.2439861 total: 16.6s remaining: 3.09s 843: learn: 3.2429466 total: 16.7s remaining: 3.08s 844: learn: 3.2422359 total: 16.7s remaining: 3.06s 845: learn: 3.2411155 total: 16.7s remaining: 3.04s 846: learn: 3.2397654 total: 16.7s remaining: 3.01s 847: learn: 3.2389070 total: 16.7s remaining: 2.99s 848: learn: 3.2383056 total: 16.8s remaining: 2.98s 849: learn: 3.2374602 total: 16.8s remaining: 2.96s 850: learn: 3.2354166 total: 16.8s remaining: 2.94s 851: learn: 3.2350600 total: 16.8s remaining: 2.91s 852: learn: 3.2332189 total: 16.8s remaining: 2.89s 853: learn: 3.2329853 total: 16.9s remaining: 2.88s 854: learn: 3.2325272 total: 16.9s remaining: 2.86s 855: learn: 3.2315728 total: 16.9s remaining: 2.84s 856: learn: 3.2311266 total: 16.9s remaining: 2.81s 857: learn: 3.2294944 total: 16.9s remaining: 2.79s 858: learn: 3.2280008 total: 17s remaining: 2.78s 859: learn: 3.2275044 total: 17s remaining: 2.76s 860: learn: 3.2272417 total: 17s remaining: 2.74s 861: learn: 3.2267300 total: 17s remaining: 2.72s 862: learn: 3.2256492 total: 17s remaining: 2.69s 863: learn: 3.2232784 total: 17.1s remaining: 2.69s 864: learn: 3.2228192 total: 17.1s remaining: 2.66s 865: learn: 3.2219042 total: 17.1s remaining: 2.64s 866: learn: 3.2212497 total: 17.1s remaining: 2.62s 867: learn: 3.2206534 total: 17.1s remaining: 2.6s 868: learn: 3.2194853 total: 17.2s remaining: 2.59s 869: learn: 3.2176513 total: 17.2s remaining: 2.56s 870: learn: 3.2168033 total: 17.2s remaining: 2.54s 871: learn: 3.2157532 total: 17.2s remaining: 2.52s 872: learn: 3.2151843 total: 17.2s remaining: 2.5s 873: learn: 3.2144320 total: 17.3s remaining: 2.49s 874: learn: 3.2141366 total: 17.3s remaining: 2.46s 875: learn: 3.2136375 total: 17.3s remaining: 2.44s 876: learn: 3.2130221 total: 17.3s remaining: 2.42s 877: learn: 3.2114371 total: 17.3s remaining: 2.4s 878: learn: 3.2105926 total: 17.4s remaining: 2.39s 879: learn: 3.2095392 total: 17.4s remaining: 2.37s 880: learn: 3.2082689 total: 17.4s remaining: 2.34s 881: learn: 3.2068575 total: 17.4s remaining: 2.32s 882: learn: 3.2056287 total: 17.4s remaining: 2.3s 883: learn: 3.2038353 total: 17.4s remaining: 2.29s 884: learn: 3.2033481 total: 17.5s remaining: 2.27s 885: learn: 3.2027182 total: 17.5s remaining: 2.25s 886: learn: 3.2015936 total: 17.5s remaining: 2.22s 887: learn: 3.1999213 total: 17.5s remaining: 2.2s 888: learn: 3.1996003 total: 17.5s remaining: 2.19s 889: learn: 3.1987540 total: 17.5s remaining: 2.17s 890: learn: 3.1984901 total: 17.6s remaining: 2.15s 891: learn: 3.1979027 total: 17.6s remaining: 2.13s 892: learn: 3.1976357 total: 17.6s remaining: 2.1s 893: learn: 3.1968788 total: 17.6s remaining: 2.09s 894: learn: 3.1960936 total: 17.6s remaining: 2.07s 895: learn: 3.1953993 total: 17.6s remaining: 2.05s 896: learn: 3.1947000 total: 17.7s remaining: 2.03s 897: learn: 3.1940194 total: 17.7s remaining: 2s 898: learn: 3.1937301 total: 17.7s remaining: 1.99s 899: learn: 3.1917774 total: 17.7s remaining: 1.97s 900: learn: 3.1911578 total: 17.7s remaining: 1.95s 901: learn: 3.1908421 total: 17.7s remaining: 1.93s 902: learn: 3.1903207 total: 17.8s remaining: 1.91s 903: learn: 3.1899629 total: 17.8s remaining: 1.89s 904: learn: 3.1886532 total: 17.8s remaining: 1.87s 905: learn: 3.1881580 total: 17.8s remaining: 1.85s 906: learn: 3.1871411 total: 17.8s remaining: 1.83s 907: learn: 3.1861274 total: 17.9s remaining: 1.81s 908: learn: 3.1855236 total: 17.9s remaining: 1.79s 909: learn: 3.1839121 total: 17.9s remaining: 1.77s 910: learn: 3.1831046 total: 17.9s remaining: 1.75s 911: learn: 3.1821596 total: 17.9s remaining: 1.73s 912: learn: 3.1807786 total: 17.9s remaining: 1.71s 913: learn: 3.1796501 total: 18s remaining: 1.7s 914: learn: 3.1789270 total: 18s remaining: 1.68s 915: learn: 3.1786925 total: 18s remaining: 1.65s 916: learn: 3.1765193 total: 18s remaining: 1.63s 917: learn: 3.1763683 total: 18.1s remaining: 1.62s 918: learn: 3.1755257 total: 18.1s remaining: 1.6s 919: learn: 3.1746936 total: 18.1s remaining: 1.58s 920: learn: 3.1727213 total: 18.1s remaining: 1.55s 921: learn: 3.1711244 total: 18.1s remaining: 1.53s 922: learn: 3.1692580 total: 18.2s remaining: 1.52s 923: learn: 3.1682127 total: 18.2s remaining: 1.5s 924: learn: 3.1655832 total: 18.2s remaining: 1.48s 925: learn: 3.1647554 total: 18.2s remaining: 1.46s 926: learn: 3.1630771 total: 18.2s remaining: 1.44s 927: learn: 3.1623723 total: 18.3s remaining: 1.42s 928: learn: 3.1617142 total: 18.4s remaining: 1.4s 929: learn: 3.1601876 total: 18.4s remaining: 1.38s 930: learn: 3.1593342 total: 18.4s remaining: 1.36s 931: learn: 3.1591008 total: 18.4s remaining: 1.34s 932: learn: 3.1581037 total: 18.4s remaining: 1.32s 933: learn: 3.1568661 total: 18.4s remaining: 1.3s 934: learn: 3.1558634 total: 18.4s remaining: 1.28s 935: learn: 3.1539188 total: 18.5s remaining: 1.27s 936: learn: 3.1518152 total: 18.5s remaining: 1.25s 937: learn: 3.1506616 total: 18.5s remaining: 1.22s 938: learn: 3.1489909 total: 18.5s remaining: 1.2s 939: learn: 3.1482410 total: 18.5s remaining: 1.18s 940: learn: 3.1470818 total: 18.6s remaining: 1.17s 941: learn: 3.1461282 total: 18.6s remaining: 1.15s 942: learn: 3.1456470 total: 18.6s remaining: 1.13s 943: learn: 3.1450137 total: 18.6s remaining: 1.1s 944: learn: 3.1429732 total: 18.6s remaining: 1.08s 945: learn: 3.1421996 total: 18.7s remaining: 1.07s 946: learn: 3.1412305 total: 18.7s remaining: 1.05s 947: learn: 3.1404965 total: 18.7s remaining: 1.03s 948: learn: 3.1398505 total: 18.7s remaining: 1.01s 949: learn: 3.1379280 total: 18.7s remaining: 986ms 950: learn: 3.1368665 total: 18.8s remaining: 969ms 951: learn: 3.1357422 total: 18.8s remaining: 949ms 952: learn: 3.1350627 total: 18.8s remaining: 928ms 953: learn: 3.1333398 total: 18.8s remaining: 908ms 954: learn: 3.1317962 total: 18.8s remaining: 887ms 955: learn: 3.1313479 total: 18.9s remaining: 870ms 956: learn: 3.1309767 total: 18.9s remaining: 850ms 957: learn: 3.1300771 total: 18.9s remaining: 829ms 958: learn: 3.1290127 total: 18.9s remaining: 809ms 959: learn: 3.1286735 total: 18.9s remaining: 789ms 960: learn: 3.1272208 total: 19s remaining: 772ms 961: learn: 3.1262658 total: 19s remaining: 751ms 962: learn: 3.1243670 total: 19s remaining: 731ms 963: learn: 3.1239742 total: 19s remaining: 710ms 964: learn: 3.1217433 total: 19s remaining: 690ms 965: learn: 3.1215300 total: 19.1s remaining: 673ms 966: learn: 3.1198694 total: 19.1s remaining: 652ms 967: learn: 3.1185202 total: 19.1s remaining: 632ms 968: learn: 3.1172165 total: 19.1s remaining: 612ms 969: learn: 3.1157237 total: 19.1s remaining: 591ms 970: learn: 3.1148788 total: 19.2s remaining: 574ms 971: learn: 3.1142763 total: 19.2s remaining: 553ms 972: learn: 3.1122783 total: 19.2s remaining: 533ms 973: learn: 3.1114193 total: 19.2s remaining: 513ms 974: learn: 3.1100147 total: 19.2s remaining: 493ms 975: learn: 3.1093744 total: 19.3s remaining: 475ms 976: learn: 3.1092969 total: 19.3s remaining: 455ms 977: learn: 3.1085145 total: 19.3s remaining: 434ms 978: learn: 3.1079519 total: 19.3s remaining: 414ms 979: learn: 3.1057407 total: 19.4s remaining: 396ms 980: learn: 3.1053785 total: 19.4s remaining: 376ms 981: learn: 3.1039714 total: 19.4s remaining: 356ms 982: learn: 3.1028927 total: 19.4s remaining: 336ms 983: learn: 3.1014381 total: 19.4s remaining: 316ms 984: learn: 3.1004328 total: 19.4s remaining: 296ms 985: learn: 3.1002543 total: 19.5s remaining: 277ms 986: learn: 3.0993387 total: 19.5s remaining: 257ms 987: learn: 3.0989281 total: 19.5s remaining: 237ms 988: learn: 3.0986195 total: 19.5s remaining: 217ms 989: learn: 3.0982980 total: 19.5s remaining: 197ms 990: learn: 3.0971505 total: 19.6s remaining: 178ms 991: learn: 3.0966882 total: 19.6s remaining: 158ms 992: learn: 3.0965667 total: 19.6s remaining: 138ms 993: learn: 3.0951912 total: 19.6s remaining: 118ms 994: learn: 3.0945439 total: 19.6s remaining: 98.5ms 995: learn: 3.0938761 total: 19.7s remaining: 79.1ms 996: learn: 3.0923819 total: 19.7s remaining: 59.3ms 997: learn: 3.0921748 total: 19.7s remaining: 39.5ms 998: learn: 3.0913114 total: 19.7s remaining: 19.7ms 999: learn: 3.0900058 total: 19.7s remaining: 0us . . Grid Search offered worse results over default settings | The default config trains in less than 1/100th of the time compared to running the grid search method | . print(&#39;CatBoostRegressor MAE:&#39;, MAE(target_test, pred_2)) . CatBoostRegressor MAE: 5.221709765858424 . Model 3 - Ridge . model_3 = Ridge(random_state=54321) . param_grid = {&#39;alpha&#39; : [0.001, 0.01, 0.1, 1, 10, 100, 1000]} grid = GridSearchCV(estimator=model_3, param_grid=param_grid, scoring=&#39;neg_mean_absolute_error&#39;, cv=4, verbose=1, n_jobs=-1) grid.fit(features_train, target_train) model_3.set_params(**grid.best_params_) model_3.fit(features_train, target_train) pred_3 = model_3.predict(features_test) . Fitting 4 folds for each of 7 candidates, totalling 28 fits . [Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers. [Parallel(n_jobs=-1)]: Done 28 out of 28 | elapsed: 1.9s finished /opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal. DeprecationWarning) . print(&#39;Ridge MAE:&#39;, MAE(target_test, pred_3)) . Ridge MAE: 5.98106353055716 . Conclusion . Linear Regression performed great compared to our sanity test model. | CatBoost trained quickly and gave us amazing results. Initially I ran a grid search for catboost but this produced worse results and slowed the model training down tremendously | Ridge was used as an alternative for Linear Regression as it performs better when some of the variables are interdependent. A quick grid search was done still arriving at very good results | All of the variables used can be derived primarily through sensors which makes automation a lot more viable | Our final model for production will be CatBoost as it performed significantly better than our other models with an MAE of 5.22 | . Report: . Problem Statement: . In order to optimize production costs, the steel plant, Steelproof, has decided to reduce their energy consumption at the steel processing stage. . Initial Information: . Data: . Additive materials (Bulk and Wire) + timestamps | Heating times and power usage | Temp readings + timestamps | Inert Gas usage | . Questions: . Question: Is any of the data not immediately available during production? Response: Intermediate temperatures cannot be used in the model, the sensors are unable to quickly determine these temperatures. . Question: For the Bulk materials and wire, are the NaNs unknown values or the automatic value for when that material wasn&#39;t used? Response: When the material wasn&#39;t used . Plan: . Isolate the target variable into its own series. | Remove date-time variables after deriving a duration variable from electrode data | Merge remaining data into a single dataframe where each iteration is only one row. | Build a variety of Regression models and test them | Solution: . We stuck to the plan for the most part. | We completely removed the timestamps for the bulk, wire, and electrode data. | Isolated the target variable and merged the remaining data into one dataframe. | Our biggest deviation from the plan was calculating the entire process duration instead of the heating duration . | Seperating the first and and final temp readings for each iteration was the trickier but most crucial part of the process . | After that merging the data was the final hurdle but was simplified by having a key column to merge on . | We tested 3 models against a dummy model to try and find the best one: . Linear Regression | CatBoost Regressor | Ridge | | The dummy model (sanity test) scored just over an 8 MAE | Linear Regression and Ridge performed adequately with MAE scores just under 6 | We tried CatBoost with some parameter tuning and the basic format - the base model performed better ## Results: | Catboost overall was our best model. The average error rate (MAE) was down to 5.22 with CatBoost | Model training time is usally a concern with CatBoost but since we are using the base model CatBoost runs quickly | .",
            "url": "https://nicholas-j-snyder.github.io/portfolio/fastpages/jupyter/2020/12/14/Steel-Processing.html",
            "relUrl": "/fastpages/jupyter/2020/12/14/Steel-Processing.html",
            "date": " • Dec 14, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Video Game Sales",
            "content": "Data description . Name | Platform | Year_of_Release | Genre | NA_sales (North American sales in USD million) | EU_sales (sales in Europe in USD million) | JP_sales (sales in Japan in USD million) | Other_sales (sales in other countries in USD million) | Critic_Score (maximum of 100) | User_Score (maximum of 10) | Rating (ESRB) | . Step 1. Open the data file and study the general information . import pandas as pd import matplotlib as plt import numpy as np from scipy import stats as st import math . games = pd.read_csv(&#39;/datasets/games.csv&#39;) . year = &#39;Year_of_Release&#39; . I debated changing the name inside the dataframe to just year, but decided against it to avoid confusing it with year of sales . games.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16715 entries, 0 to 16714 Data columns (total 11 columns): Name 16713 non-null object Platform 16715 non-null object Year_of_Release 16446 non-null float64 Genre 16713 non-null object NA_sales 16715 non-null float64 EU_sales 16715 non-null float64 JP_sales 16715 non-null float64 Other_sales 16715 non-null float64 Critic_Score 8137 non-null float64 User_Score 10014 non-null object Rating 9949 non-null object dtypes: float64(6), object(5) memory usage: 1.4+ MB . games.head() . Name Platform Year_of_Release Genre NA_sales EU_sales JP_sales Other_sales Critic_Score User_Score Rating . 0 | Wii Sports | Wii | 2006.0 | Sports | 41.36 | 28.96 | 3.77 | 8.45 | 76.0 | 8 | E | . 1 | Super Mario Bros. | NES | 1985.0 | Platform | 29.08 | 3.58 | 6.81 | 0.77 | NaN | NaN | NaN | . 2 | Mario Kart Wii | Wii | 2008.0 | Racing | 15.68 | 12.76 | 3.79 | 3.29 | 82.0 | 8.3 | E | . 3 | Wii Sports Resort | Wii | 2009.0 | Sports | 15.61 | 10.93 | 3.28 | 2.95 | 80.0 | 8 | E | . 4 | Pokemon Red/Pokemon Blue | GB | 1996.0 | Role-Playing | 11.27 | 8.89 | 10.22 | 1.00 | NaN | NaN | NaN | . display(games[(games[&#39;Rating&#39;].isna()) &amp; (games[year] &gt;= 2015)].count()) games[(games[&#39;Rating&#39;].isna()) &amp; (games[year] &lt;= 1994)].count() . Name 513 Platform 513 Year_of_Release 513 Genre 513 NA_sales 513 EU_sales 513 JP_sales 513 Other_sales 513 Critic_Score 28 User_Score 42 Rating 0 dtype: int64 . Name 481 Platform 483 Year_of_Release 483 Genre 481 NA_sales 483 EU_sales 483 JP_sales 483 Other_sales 483 Critic_Score 0 User_Score 0 Rating 0 dtype: int64 . . #games.duplicated().value_counts() #games[games[&#39;Name&#39;] ==&#39;Need for Speed: Most Wanted&#39;] . Conclusion . Only 483 games were released before the ESRB was created (1994). So the missing values appear to be random and not because of existing before the agency was created. Game reviews and user ratings also weren&#39;t a popular thing to do until later on in video games existence. | &#39;User_Score&#39; has both numeric and string values which we will need to address if we want to do mathematical calculations using those rows. | Overall the ratings and scores for each game are the least reliable data. | There is data for the same game multiple times, yet the values are not the same. Upon further inspection this appears to be from being released on multiple consoles and being re-released on new consoles later on. | . Step 2. Prepare the data . Replace the column names (make them lowercase). | Convert the data to the required types. | Describe the columns where the data types have been changed and why. | If necessary, decide how to deal with missing values: Explain why you filled in the missing values as you did or why you decided to leave them blank. | Why do you think the values are missing? Give possible reasons. | Pay attention to the abbreviation TBD (to be determined). Specify how you intend to handle such cases. | . | Calculate the total sales (the sum of sales in all regions) for each game and put these values in a separate column | . games.columns = games.columns.str.lower() year = year.lower() year_filter = games[year] != games[year].min() games.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16715 entries, 0 to 16714 Data columns (total 11 columns): name 16713 non-null object platform 16715 non-null object year_of_release 16446 non-null float64 genre 16713 non-null object na_sales 16715 non-null float64 eu_sales 16715 non-null float64 jp_sales 16715 non-null float64 other_sales 16715 non-null float64 critic_score 8137 non-null float64 user_score 10014 non-null object rating 9949 non-null object dtypes: float64(6), object(5) memory usage: 1.4+ MB . . games[year] = games[year].fillna(1970) games[year] = pd.to_datetime(games[year], format=&#39;%Y&#39;) games.info() #games.head() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16715 entries, 0 to 16714 Data columns (total 11 columns): name 16713 non-null object platform 16715 non-null object year_of_release 16715 non-null datetime64[ns] genre 16713 non-null object na_sales 16715 non-null float64 eu_sales 16715 non-null float64 jp_sales 16715 non-null float64 other_sales 16715 non-null float64 critic_score 8137 non-null float64 user_score 10014 non-null object rating 9949 non-null object dtypes: datetime64[ns](1), float64(5), object(5) memory usage: 1.4+ MB . games[&#39;user_score&#39;].where(games[&#39;user_score&#39;] != &#39;tbd&#39;, other=-2, inplace=True) #games[&#39;user_score&#39;].value_counts() games[&#39;user_score&#39;] = games[&#39;user_score&#39;].astype(&#39;float&#39;) . games = games.fillna(-1) games.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16715 entries, 0 to 16714 Data columns (total 11 columns): name 16715 non-null object platform 16715 non-null object year_of_release 16715 non-null datetime64[ns] genre 16715 non-null object na_sales 16715 non-null float64 eu_sales 16715 non-null float64 jp_sales 16715 non-null float64 other_sales 16715 non-null float64 critic_score 16715 non-null float64 user_score 16715 non-null float64 rating 16715 non-null object dtypes: datetime64[ns](1), float64(6), object(4) memory usage: 1.4+ MB . # columns = [&#39;na_sales&#39;, &#39;eu_sales&#39;, &#39;jp_sales&#39;, &#39;other_sales&#39;] # total= 0 # for column in columns: # total += row[column] # return total . #games[&#39;total_sales&#39;] = games.apply(total_sales, axis=1) . Okay, lets dive into that. You made an implementation that looks neat - you only need to change the list columns in order to sum any amount of columns you want. Nevertheless, method apply is used for any kind of arbitrary functions, and to work correctly despite of the function structure it would involve described calculation for every row. It is actually not a very simple thing caused by the raw complexity of apply method, but let&#39;s call it vector application. Another way would be implementing pure vector operations, like in the example below. As we see, it is slightly faster, but we also have a very small dataset with simple features, the difference could be much more on practice. So, the main thing we need to conclude here, that instead of using N vector applications it is more efficient to stick to several vector operations (in our case - 3 additive operations). Thank you for explaining this, I know that computation time definitely comes into play with larger and larger dataframes and will be something I need to recognize in my method of approach. I see now that I was overthinking things and actually making this more complicated than it needed to be #games[&#39;total_sales_v2&#39;] = games[&#39;na_sales&#39;]+games[&#39;eu_sales&#39;]+games[&#39;jp_sales&#39;]+games[&#39;other_sales&#39;] . games[&#39;total_sales&#39;] = games[&#39;na_sales&#39;] + games[&#39;eu_sales&#39;] + games[&#39;jp_sales&#39;] + games[&#39;other_sales&#39;] . games.head() . name platform year_of_release genre na_sales eu_sales jp_sales other_sales critic_score user_score rating total_sales . 0 | Wii Sports | Wii | 2006-01-01 | Sports | 41.36 | 28.96 | 3.77 | 8.45 | 76.0 | 8.0 | E | 82.54 | . 1 | Super Mario Bros. | NES | 1985-01-01 | Platform | 29.08 | 3.58 | 6.81 | 0.77 | -1.0 | -1.0 | -1 | 40.24 | . 2 | Mario Kart Wii | Wii | 2008-01-01 | Racing | 15.68 | 12.76 | 3.79 | 3.29 | 82.0 | 8.3 | E | 35.52 | . 3 | Wii Sports Resort | Wii | 2009-01-01 | Sports | 15.61 | 10.93 | 3.28 | 2.95 | 80.0 | 8.0 | E | 32.77 | . 4 | Pokemon Red/Pokemon Blue | GB | 1996-01-01 | Role-Playing | 11.27 | 8.89 | 10.22 | 1.00 | -1.0 | -1.0 | -1 | 31.38 | . Conclusion . column names changed to lowercase | year of release converted to datetime object | -1 used to fill empty values | &#39;tbd&#39; replaced for user_score with -2 so that we can use numerical operations on the column and identify them from the original NaN columns (but still be able to filter them out easily) | total sales calculated | . Step 3. Analyze the data . [x] Look at how many games were released in different years. Is the data for every period significant? | [x] Look at how sales varied from platform to platform. Choose the platforms with the greatest total sales and build a distribution based on data for each year. Find platforms that used to be popular but now have zero sales. How long does it generally take for new platforms to appear and old ones to fade? | [x] Determine what period you should take data for. To do so, look at your answers to the previous questions. The data should allow you to build a prognosis for 2017. | [x] Work only with the data that you&#39;ve decided is relevant. Disregard the data for previous years. | [x] Which platforms are leading in sales? Which ones are growing or shrinking? Select several potentially profitable platforms. | [x] Build a box plot for the global sales of all games, broken down by platform. Are the differences in sales significant? What about average sales on various platforms? Describe your findings. | [x] Take a look at how user and professional reviews affect sales for one popular platform (you choose). Build a scatter plot and calculate the correlation between reviews and sales. Draw conclusions. | [x] Keeping your conclusions in mind, compare the sales of the same games on other platforms. | [x] Take a look at the general distribution of games by genre. What can we say about the most profitable genres? Can you generalize about genres with high and low sales? | . releases_per_year = ( games[year_filter] .pivot_table(index=[year], values=&#39;name&#39;, aggfunc=&#39;count&#39;) ) releases_per_year.index = releases_per_year.index.year releases_per_year.columns = [&#39;number_of_releases&#39;] releases_per_year.plot(kind=&#39;bar&#39;, figsize=(8,8)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa7b50253d0&gt; . sales_per_platform = ( games .pivot_table(index=[&#39;platform&#39;], values=[&#39;total_sales&#39;, &#39;na_sales&#39;, &#39;eu_sales&#39;, &#39;jp_sales&#39;], aggfunc=&#39;sum&#39;) .sort_values(&#39;total_sales&#39;, ascending=False) ) sales_per_platform.sort_values(&#39;total_sales&#39;, ascending=True)[&#39;total_sales&#39;].plot(kind=&#39;barh&#39;, figsize=(8,8)) sales_per_platform[&#39;total_sales&#39;].head() . platform PS2 1255.77 X360 971.42 PS3 939.65 Wii 907.51 DS 806.12 Name: total_sales, dtype: float64 . Good. If you want your text labels to be even easier to read - rotate the plot. If we are plotting our data with a loop - why don&#39;t we also aggregate it with a loop? Or, may be, inside the same loop we have. These cells above look extremely repetetive. Consolidated everything into 1 loop, I thought it looked clunky before but I didnt think about just creating the tables with the loop as well. I was worried about needing those tables later on but I really didnt. platforms = sales_per_platform.head().index.values print(platforms) for platform in platforms: platform_table = ( games[(year_filter) &amp; (games[&#39;platform&#39;] == platform)] .pivot_table(index=[year], values=&#39;total_sales&#39;, aggfunc=&#39;sum&#39;) ) platform_table.index = platform_table.index.year platform_table.columns = [(platform + &#39; Sales&#39;)] platform_table.plot(kind=&#39;bar&#39;) . [&#39;PS2&#39; &#39;X360&#39; &#39;PS3&#39; &#39;Wii&#39; &#39;DS&#39;] . outdated_platforms = np.append(platforms, [&#39;PSP&#39;]) games_new = games[games[year] &gt;= pd.to_datetime(2007, format=&#39;%Y&#39;)] . new_sales_per_platform = ( games_new .pivot_table(index=[&#39;platform&#39;], values=[&#39;total_sales&#39;, &#39;na_sales&#39;, &#39;eu_sales&#39;, &#39;jp_sales&#39;], aggfunc=&#39;sum&#39;) .sort_values(&#39;total_sales&#39;, ascending=False) ) new_sales_per_platform.sort_values(&#39;total_sales&#39;, ascending=True)[&#39;total_sales&#39;].plot(kind=&#39;barh&#39;, figsize=(8,8)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa7b20f6190&gt; . platform_filter = r&#39; b(?:{}) b&#39;.format(&#39;|&#39;.join(outdated_platforms)) new_platforms = new_sales_per_platform[~(new_sales_per_platform.index.str.contains(platform_filter))].head(6).index.values print(new_platforms) for platform in new_platforms: platform_table = ( games_new[(games_new[&#39;platform&#39;] == platform)] .pivot_table(index=[year], values=&#39;total_sales&#39;, aggfunc=&#39;sum&#39;) ) platform_table.index = platform_table.index.year platform_table.columns = [(platform + &#39; Sales&#39;)] platform_table.plot(kind=&#39;bar&#39;) . [&#39;PS4&#39; &#39;3DS&#39; &#39;PC&#39; &#39;XOne&#39; &#39;WiiU&#39; &#39;PSV&#39;] . all_pc_sales = ( games[(year_filter) &amp; (games[&#39;platform&#39;] == &#39;PC&#39;)] .pivot_table(index=[year], values=&#39;total_sales&#39;, aggfunc=&#39;sum&#39;) ) all_pc_sales.index = all_pc_sales.index.year all_pc_sales.columns = [&#39;pc_sales&#39;] all_pc_sales.plot(kind=&#39;bar&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa7b21cb290&gt; . games.boxplot(&#39;total_sales&#39;, by=&#39;platform&#39;, figsize=(15,13), showfliers=False) games_new.boxplot(&#39;total_sales&#39;, by=&#39;platform&#39;, figsize=(15,13), showfliers=False) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa7b232d310&gt; . We don&#39;t really delve into our outliers, we don&#39;t know their names and everyrthing, so, I suppose, to see our distribution ranges more clearly we don&#39;t really need them on a plot. Setting showfliers=False might help with that. That does look alot cleaner with removing the outliers. That was part of the reason I made the charts so big in the first place. So you could see some of the distribution ranges still, but I like this way alot better ps2_games = games[games[&#39;platform&#39;] == &#39;PS2&#39;] ps2_games[ps2_games[&#39;user_score&#39;] &gt;= 0].plot.scatter(x=&#39;user_score&#39;, y=&#39;total_sales&#39;) ps2_games[ps2_games[&#39;critic_score&#39;] &gt;= 0].plot.scatter(x=&#39;critic_score&#39;, y=&#39;total_sales&#39;) ps2_games[[&#39;user_score&#39;,&#39;total_sales&#39;, &#39;critic_score&#39;]].corr() . user_score total_sales critic_score . user_score | 1.000000 | 0.258433 | 0.787248 | . total_sales | 0.258433 | 1.000000 | 0.299548 | . critic_score | 0.787248 | 0.299548 | 1.000000 | . For the PS2 user_score had very little correlation to game sales. But someone generally can&#39;t give their opinion on a game until after they&#39;ve bought and played it. So, even if a game isn&#39;t well liked, if theres enough hype/demand plenty of copies will be bought before the gaming community can give their full opinions. Critics opinions also had very little impact on game sales but were heavily correlated with user scores. | . Would be so great to see our conclusions right under the plot, so there would be no need to look for in two screens away. Would it be better for me to break my conclusions per section up and list them piece by piece? I&#39;ve always just tried to keep them consolidated into neat bullet points. Admittedly this time I think I had so much extra redundant code that everything was a bit of a chore to get through multi_platform_games = ( games[&#39;name&#39;] .value_counts() .sort_values(ascending=False) .head() ) multi_platform_games . Need for Speed: Most Wanted 12 LEGO Marvel Super Heroes 9 Madden NFL 07 9 Ratatouille 9 FIFA 14 9 Name: name, dtype: int64 . for game in multi_platform_games.index: ( games[games[&#39;name&#39;] == game] .pivot_table(index=[&#39;platform&#39;], values=&#39;total_sales&#39;, aggfunc=&#39;sum&#39;) .plot(kind=&#39;bar&#39;, figsize=(6,6), title=game) ) . games_per_genre = ( games[games[&#39;genre&#39;] != -1] .pivot_table(index=[&#39;genre&#39;], values=&#39;name&#39;, aggfunc=&#39;count&#39;) .sort_values(&#39;name&#39;, ascending=False) ) games_per_genre.columns = [&#39;Number of Releases&#39;] sales_per_genre = ( games[games[&#39;genre&#39;] != -1] .pivot_table(index=[&#39;genre&#39;], values=[&#39;total_sales&#39;, &#39;na_sales&#39;, &#39;eu_sales&#39;,&#39;jp_sales&#39;], aggfunc=&#39;sum&#39;) .sort_values(&#39;total_sales&#39;, ascending=False) ) . sales_per_genre[&#39;total_sales&#39;].plot(kind=&#39;bar&#39;, figsize=(15,6), title=&#39;All Time Sales&#39;) games_per_genre.plot(kind=&#39;bar&#39;, figsize=(15,6), title=&#39;All Time Releases per Genre&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa7b189ff10&gt; . new_games_per_genre = ( games_new[games_new[&#39;genre&#39;] != -1] .pivot_table(index=[&#39;genre&#39;], values=&#39;name&#39;, aggfunc=&#39;count&#39;) .sort_values(&#39;name&#39;, ascending=False) ) new_games_per_genre.columns = [&#39;Number of Releases&#39;] new_sales_per_genre = ( games_new[games_new[&#39;genre&#39;] != -1] .pivot_table(index=[&#39;genre&#39;], values=[&#39;total_sales&#39;, &#39;na_sales&#39;, &#39;eu_sales&#39;,&#39;jp_sales&#39;], aggfunc=&#39;sum&#39;) .sort_values(&#39;total_sales&#39;, ascending=False) ) . new_sales_per_genre[&#39;total_sales&#39;].plot(kind=&#39;bar&#39;, figsize=(15,6), title=&#39;Sales from 2007-2016&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa7b1dfca50&gt; . new_games_per_genre.plot(kind=&#39;bar&#39;, figsize=(15,6), title=&#39;2007-2016 Releases per Genre&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa7b1e68990&gt; . Conclusion . Pre-1994 games were not released in nearly the volume of modern day. 2005-2011 had a huge uptick in the volume of games released overall Examining the platforms with the highest amount of sales, we can see that platforms have about a 10yr lifespan. | To prepare for 2017 we should look at data from 2007 onwards | PS3 and X360 are still leading in sales along with the Wii, PS2, and DS but we have already determined them to be at end of life. I also weed out the PSP as it is superceded by the PSV and sales have dropped to nothing for the platform. | All game sales seemed to have taken a hit for 2016 but the PS4, XOne, 3DS, WiiU, and PSV still appear to have some life in them. PC games appear to be an outlier in that they produce steady sales that rise and fall over time and are not subject to the normal lifespan of a platform but make them a safe bet as well. | When comparing games sales globally by platform. Some games drastically outperform the rest making many times over what a less popular game would make in sales. And some platforms make more money overall, in their lifespan, than multiple platforms combined. | For the PS2 user_score had very little correlation to game sales. But someone generally can&#39;t give their opinion on a game until after they&#39;ve bought and played it. So, even if a game isnt well liked, if theres enough hype/demand plenty of copies will be bought before the gaming community can give their full opinions. Critics opinions also had very little impact on game sales but were heavily correlated with user scores. | Games can have heavily varying sales across platforms. If a game did well, from a brief glance, the most popular platforms had the largest spikes in sales. | Action games are still the most released titles, but shooter games have increased in market share for more recent years even though the number of releases have went down. Puzzle and Strategy games overall have been poor performers | . | . Step 4. Create a user profile for each region . For each region (NA, EU, JP), determine: . The top five platforms. Describe variations in their market shares from region to region. | The top five genres. Explain the difference. | Do ESRB ratings affect sales in individual regions? | . regions = [&#39;na_sales&#39;, &#39;eu_sales&#39;,&#39;jp_sales&#39;] . Please, work over those repetetive parts, they require fixing, expecially since you already made a good structure of your transformations, you just need to unify that with a loop. . upd. Great! Combined the 6 tables into 2 tables, then realized I basically already made these tables earlier and deleted the repeats ( sales_per_genre[regions] .plot(kind=&#39;bar&#39;, figsize=(15,10), subplots=True,legend=True, title=&#39;Regional Sales by Genre - All Data&#39;) ) . array([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fa7b2405cd0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fa7b19a49d0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fa7b2045cd0&gt;], dtype=object) . ( sales_per_platform[regions] .plot(kind=&#39;bar&#39;, figsize=(15,10), subplots=True,legend=True, title=&#39;Regional Sales by Platform- All Data&#39;) ) . array([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fa7b1f58d10&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fa7b2384e10&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fa7b1f47a50&gt;], dtype=object) . There is a wide-spreaded argument about using or avoiding pie charts in a general case. Some people say that every pie chart is easily replaced with a bar chart (this is my opinion too). For example, we see these label overlaps that are not possible on barcharts. Also, comparing bar sizes is basically easier for our brain than comparing sector angles. Another thing I can mention - color consistency. In your case - we have blue for Action in Europe and NA, but for RPG in Japan. If some people, who we are sharing the results of our work would not notice that - that is a potential cause for a management mistake (which may become a huge cost for company). One of the techniques to prevent that is merging your plots of similar nature to one matplotlib object (meaning to the same color palette). You do that below. I could not find anything on making a bie chart, just pictures on what they are supposed to look like. So I&#39;ve tried replacing them with barcharts for now. I have mixed feelings about their effectiveness in my current implementation. With horizontal bar charts the ranges are all uniform but only appear on the lowest graph. On a normal bar chart the ranges are all different making the volume of sales look similar when its not even close. I stuck with the normal bar charts, for now, since we&#39;re comparing their distributions and not volume You are right, this is always a matter of choice, you did fine anyway. ( new_sales_per_genre[regions] .plot(kind=&#39;bar&#39;, figsize=(15,10), subplots=True, legend=True, title=&#39;Sales by Genre per Region - Recent Data&#39;) ) . array([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fa7b191c3d0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fa7b21cbed0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fa7b1c37310&gt;], dtype=object) . ( new_sales_per_genre[regions] .plot(kind=&#39;bar&#39;, figsize=(15,10), subplots=True, legend=True, title=&#39;Sales by Genre per Region - Recent Data&#39;) ) . array([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fa7b1877c50&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fa7b146abd0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fa7b14b1fd0&gt;], dtype=object) . rating_table = ( games_new[games_new[&#39;rating&#39;] != -1] .pivot_table(index=&#39;rating&#39;, values=[&#39;na_sales&#39;, &#39;eu_sales&#39;, &#39;jp_sales&#39;], aggfunc=&#39;sum&#39;) ) rating_table.plot(kind=&#39;bar&#39;, figsize=(15,10), subplots=True, legend=True, title=&#39;Regional Sales by ESRB Rating&#39;) . array([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fa7b1b3d250&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fa7b1b476d0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fa7b1775d50&gt;], dtype=object) . Conclusion . Market share of different platforms varies from region to region. In Japan they favor more portable platforms like the DS and 3DS and the X360 (a US Platform) has almost no presence. Where in western culture (NA and EU) the X360 is a major platform and portable consoles don&#39;t dominate the market. | NA favors Action, Shooters and Sport games which is pretty on par with our societal values. With Role-playing games just edging into the top 5 as well. In the EU they also favor Action, Shooter and Sport games but also like racing games a bit more than the other regions possibly due to Formula 1. In JP Role-Playing games are a huge part of the market with over a 30% market share. Follow it up with a decent interest in action games and then the rest of the genres start to blur together in market share. A lot of Role-Playing games made for JP are released in the US and we call them JRPG&#39;s. | ESRB ratings and sales seem to be pretty consistent among the different regions with the one major variation being that T(een) games do better in JP and M(ature) games sell better in NA and EU. But its roughly a 50/50 split of overall sales for games suitable for younger individuals and games more suited towards teens and adults | . Step 5. Test the following hypotheses: . Average user ratings of the Xbox One and PC platforms are the same. | Average user ratings for the Action and Sports genres are different. Set the alpha threshold value yourself. Explain: | How you formulated the null and alternative hypotheses | What significance level you chose to test the hypotheses, and why | . Test 1 - All Data . H0 (null hypothesis) = Average user ratings of the Xbox One and PC platforms are the same | H1 (alternative hypothesis) = Average user ratings of the Xbox One and PC platforms are different | . Hypotheses are formulated correctly. xone_games = games[(games[&#39;platform&#39;] == &#39;XOne&#39;) &amp; (games[&#39;user_score&#39;] &gt; -1)] pc_games = games[(games[&#39;platform&#39;] == &#39;PC&#39;) &amp; (games[&#39;user_score&#39;] &gt; -1)] var_samp_1 = np.var(xone_games[&#39;user_score&#39;]) var_samp_2 = np.var(pc_games[&#39;user_score&#39;]) print(f&quot;Variance sample 1: {var_samp_1:.2f} nVariance sample 2: {var_samp_2:.2f}&quot;) alpha = 0.05 # 5% difference between variances percent_diff = (abs(var_samp_1-var_samp_2)/((var_samp_1 + var_samp_2)/2)) print(&quot;Percentage difference: {:.2f}&quot;.format(percent_diff)) if (percent_diff &lt; alpha): print(&quot;There is no difference bwtween the variances&quot;) else: print(&quot;There is a difference between the variances&quot;) . Variance sample 1: 1.90 Variance sample 2: 2.34 Percentage difference: 0.21 There is a difference between the variances . Glad that you didn&#39;t forget to check that! Well done! results = st.ttest_ind(xone_games[&#39;user_score&#39;], pc_games[&#39;user_score&#39;], equal_var = False) print(&#39;p-value: &#39;, results.pvalue) if (results.pvalue &lt; alpha): print(&quot;We reject the null hypothesis&quot;) else: print(&quot;We can&#39;t reject the null hypothesis&quot;) . p-value: 4.935072360183565e-06 We reject the null hypothesis . Test 1.1 - Recent Data . H0 (null hypothesis) = Average user ratings of the Xbox One and PC platforms are the same | H1 (alternative hypothesis) = Average user ratings of the Xbox One and PC platforms are different | . xone_games = games_new[(games_new[&#39;platform&#39;] == &#39;XOne&#39;) &amp; (games_new[&#39;user_score&#39;] &gt; -1)] pc_games = games_new[(games_new[&#39;platform&#39;] == &#39;PC&#39;) &amp; (games_new[&#39;user_score&#39;] &gt; -1)] var_samp_1 = np.var(xone_games[&#39;user_score&#39;]) var_samp_2 = np.var(pc_games[&#39;user_score&#39;]) print(f&quot;Variance sample 1: {var_samp_1:.2f} nVariance sample 2: {var_samp_2:.2f}&quot;) alpha = 0.05 # 5% difference between variances percent_diff = (abs(var_samp_1-var_samp_2)/((var_samp_1 + var_samp_2)/2)) print(&quot;Percentage difference: {:.2f}&quot;.format(percent_diff)) if (percent_diff &lt; alpha): print(&quot;There is no difference bwtween the variances&quot;) else: print(&quot;There is a difference between the variances&quot;) . Variance sample 1: 1.90 Variance sample 2: 2.34 Percentage difference: 0.21 There is a difference between the variances . results = st.ttest_ind(xone_games[&#39;user_score&#39;], pc_games[&#39;user_score&#39;], equal_var = False) print(&#39;p-value: &#39;, results.pvalue) if (results.pvalue &lt; alpha): print(&quot;We reject the null hypothesis&quot;) else: print(&quot;We can&#39;t reject the null hypothesis&quot;) . p-value: 0.04401892858926169 We reject the null hypothesis . Test 2 - All Data . H0 (null hypothesis) = Average user ratings for the Action and Sports genres are the same | H1 (alternative hypothesis) = Average user ratings for the Action and Sports genres are different | . action_games = games[(games[&#39;genre&#39;] == &#39;Action&#39;) &amp; (games[&#39;user_score&#39;] &gt; -1)] sports_games = games[(games[&#39;genre&#39;] == &#39;Sports&#39;) &amp; (games[&#39;user_score&#39;] &gt; -1)] var_samp_1 = np.var(action_games[&#39;user_score&#39;]) var_samp_2 = np.var(sports_games[&#39;user_score&#39;]) print(f&quot;Variance sample 1: {var_samp_1:.2f} nVariance sample 2: {var_samp_2:.2f}&quot;) alpha = 0.05 # 5% difference between variances percent_diff = (abs(var_samp_1-var_samp_2)/((var_samp_1 + var_samp_2)/2)) print(&quot;Percentage difference: {:.2f}&quot;.format(percent_diff)) if (percent_diff &lt; alpha): print(&quot;There is no difference bwtween the variances&quot;) else: print(&quot;There is a difference between the variances&quot;) . Variance sample 1: 2.03 Variance sample 2: 2.59 Percentage difference: 0.24 There is a difference between the variances . results = st.ttest_ind(action_games[&#39;user_score&#39;], sports_games[&#39;user_score&#39;], equal_var = False) print(&#39;p-value: &#39;, results.pvalue) if (results.pvalue &lt; alpha): print(&quot;We reject the null hypothesis&quot;) else: print(&quot;We can&#39;t reject the null hypothesis&quot;) . p-value: 0.11483818791498286 We can&#39;t reject the null hypothesis . Test 2.1 - Recent Data . H0 (null hypothesis) = Average user ratings for the Action and Sports genres are the same | H1 (alternative hypothesis) = Average user ratings for the Action and Sports genres are different | . action_games = games_new[(games_new[&#39;genre&#39;] == &#39;Action&#39;) &amp; (games_new[&#39;user_score&#39;] &gt; -1)] sports_games = games_new[(games_new[&#39;genre&#39;] == &#39;Sports&#39;) &amp; (games_new[&#39;user_score&#39;] &gt; -1)] var_samp_1 = np.var(action_games[&#39;user_score&#39;]) var_samp_2 = np.var(sports_games[&#39;user_score&#39;]) print(f&quot;Variance sample 1: {var_samp_1:.2f} nVariance sample 2: {var_samp_2:.2f}&quot;) alpha = 0.05 # 5% difference between variances percent_diff = (abs(var_samp_1-var_samp_2)/((var_samp_1 + var_samp_2)/2)) print(&quot;Percentage difference: {:.2f}&quot;.format(percent_diff)) if (percent_diff &lt; alpha): print(&quot;There is no difference bwtween the variances&quot;) else: print(&quot;There is a difference between the variances&quot;) . Variance sample 1: 1.83 Variance sample 2: 2.81 Percentage difference: 0.42 There is a difference between the variances . results = st.ttest_ind(action_games[&#39;user_score&#39;], sports_games[&#39;user_score&#39;], equal_var = False) print(&#39;p-value: &#39;, results.pvalue) if (results.pvalue &lt; alpha): print(&quot;We reject the null hypothesis&quot;) else: print(&quot;We can&#39;t reject the null hypothesis&quot;) . p-value: 4.540665510055229e-11 We reject the null hypothesis . Testing is done correctly. Conclusion . For an alpha threshold I used 5%. This is the most commonly used level in statistics. We were taught this and I did some searching as a double check. | I always set the null hypothesis as the two variables are equal. This is the default standard in statistics. | For the 1st test, as seen above, we rejected the null hypothesis that the user ratings between Xbox One and PC games were the same. Even with filtering out older PC games by using more recent data we still rejected the null hypothesis. | For the 2nd test we again set the null hypothesis that sports and action games had equal user ratings. This time we received some interesting results. When testing all the data we couldn&#39;t reject the null hypothesis, but when we ran the test again only using more recent data we rejected the null hypothesis. | . Step 6. Write a general conclusion . Sales numbers for the dataset were reliable but user ratings, critic scores and ESRB Ratings left much to be desired. | Games are not being mass produced as quickly anymore and there was quite a 5 yr boom in game production from 2008-2012. | The worlds taste in video games has changed over time and we can see a clear difference between the games eastern and western cultures enjoy. This is seen by how much different the market share of different genres is. | Successful game platforms have about a 10yr lifecycle, with the exception of the PC, before sales fall off completely. | Games appear to have massive hits and complete duds just like the movie industry causing a huge swing in potential sales for a game. | Japan appears to prefer portable handheld consoles more than NA and the EU | . Okay, you&#39;ve already shown a great work, I especially like your strict conclusion making. Please make necessary corrections and revise my commentary. See you! . upd. Thank you for that time and effort you commited to that project. Best wishes at further projects!",
            "url": "https://nicholas-j-snyder.github.io/portfolio/fastpages/jupyter/2020/05/31/Video-Game-Sales.html",
            "relUrl": "/fastpages/jupyter/2020/05/31/Video-Game-Sales.html",
            "date": " • May 31, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://nicholas-j-snyder.github.io/portfolio/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://nicholas-j-snyder.github.io/portfolio/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://nicholas-j-snyder.github.io/portfolio/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://nicholas-j-snyder.github.io/portfolio/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}