{
  
    
        "post0": {
            "title": "Allcorrect Games",
            "content": "Problem Statement . Allcorrect Games is looking to improve the speed at which they identify potential customers. | The current bottleneck is manually labeling reviews into 4 categories | We will attempt to resolve this using machine learning | . Data Description . id - unique identififer | mark - our RL, YL, L+, or L- label RL ‚Äì localization request; | L+ ‚Äì good localization; | L- ‚Äì bad localization; | YL ‚Äì localization exists. | . | review - The reviews to be examined | . Import and examine the data . import warnings warnings.filterwarnings(&quot;ignore&quot;) import math import numpy as np import pandas as pd import matplotlib import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline %config InlineBackend.figure_format = &#39;png&#39; # the next line provides graphs of better quality on HiDPI screens %config InlineBackend.figure_format = &#39;retina&#39; plt.style.use(&#39;seaborn&#39;) from tqdm.auto import tqdm tqdm.pandas() import re import spacy import torch import transformers from sklearn.metrics import classification_report,accuracy_score from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.model_selection import GridSearchCV from catboost import Pool, CatBoostClassifier . STATE = 12345 USE_GPU = True SOURCE_FILE = &#39;C:/Users/The Ogre/datascience/allcorrectgames/reviews.xlsx&#39; . torch.cuda.is_available() . True . df_reviews = pd.read_excel(SOURCE_FILE, engine=&#39;openpyxl&#39;) . df_reviews.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 51800 entries, 0 to 51799 Data columns (total 3 columns): # Column Non-Null Count Dtype -- -- 0 id 51800 non-null int64 1 mark 51800 non-null object 2 review 51800 non-null object dtypes: int64(1), object(2) memory usage: 1.2+ MB . df_reviews.head() . id mark review . 0 6720 | RL | It&#39;s not Turkish, it&#39;s a lie, but I recommend ... | . 1 43313 | RL | You don&#39;t have Korean | . 2 26549 | RL | Very nice, only if it were in Italian it would... | . 3 42306 | RL | The game is nice but when it comes to Turkish ... | . 4 32331 | RL | Amazing work, hope to achieve 100% Chinese tra... | . df_reviews[&#39;mark&#39;].value_counts() . RL 42684 L- 5474 YL 2961 L+ 666 Rl 10 l- 3 yl 1 Yl 1 Name: mark, dtype: int64 . df_reviews[&#39;mark&#39;].value_counts().plot(kind=&#39;bar&#39;) . &lt;AxesSubplot:&gt; . Categories need cleaning up, need to make them all Capital Letters | . df_reviews[&#39;id&#39;].duplicated().value_counts() . False 51800 Name: id, dtype: int64 . df_reviews[df_reviews[&#39;review&#39;].str.len() &lt;= 5][&#39;review&#39;].value_counts() . üëç 2 üòïüòïüòï 1 ?? 1 . 1 Plz 1 When? 1 FROM 1 Pls 1 Good! 1 Great 1 üëø 1 Ôºå 1 .. 1 ... 1 üëçüëçüëç 1 Now 1 üá©üá∞‚úåÔ∏è 1 üôèüèΩ 1 Name: review, dtype: int64 . Removing these reviews. They are not long enough to have any value and appear to be errors in the initial algorithm gathering the reviews | . df_reviews[&#39;review&#39;].duplicated().value_counts() . False 48515 True 3285 Name: review, dtype: int64 . df_reviews[df_reviews[&#39;review&#39;].duplicated()][&#39;review&#39;].value_counts().head() . Would like to have Thai language 181 No Russian language 85 Turkish language please 62 Add Russian 60 Please add the Arabic language 54 Name: review, dtype: int64 . Duplicates area a natural occurence in the data set, so we are going to leave them | . Clean the Data . df_reviews = df_reviews[df_reviews[&#39;review&#39;].str.len() &gt; 5] df_reviews.reset_index(drop=True, inplace=True) . df_reviews[&#39;mark&#39;] = df_reviews[&#39;mark&#39;].str.upper() . df_reviews[&#39;mark&#39;].value_counts().plot(kind=&#39;bar&#39;) df_reviews[&#39;mark&#39;].value_counts() . RL 42676 L- 5477 YL 2962 L+ 666 Name: mark, dtype: int64 . All labels have been properly corrected | . contractions = { &quot;ain&#39;t&quot;: &quot;am not&quot;, &quot;aren&#39;t&quot;: &quot;are not&quot;, &quot;can&#39;t&quot;: &quot;cannot&quot;, &quot;can&#39;t&#39;ve&quot;: &quot;cannot have&quot;, &quot;&#39;cause&quot;: &quot;because&quot;, &quot;could&#39;ve&quot;: &quot;could have&quot;, &quot;couldn&#39;t&quot;: &quot;could not&quot;, &quot;couldn&#39;t&#39;ve&quot;: &quot;could not have&quot;, &quot;didn&#39;t&quot;: &quot;did not&quot;, &quot;doesn&#39;t&quot;: &quot;does not&quot;, &quot;don&#39;t&quot;: &quot;do not&quot;, &quot;hadn&#39;t&quot;: &quot;had not&quot;, &quot;hadn&#39;t&#39;ve&quot;: &quot;had not have&quot;, &quot;hasn&#39;t&quot;: &quot;has not&quot;, &quot;haven&#39;t&quot;: &quot;have not&quot;, &quot;he&#39;d&quot;: &quot;he would&quot;, &quot;he&#39;d&#39;ve&quot;: &quot;he would have&quot;, &quot;he&#39;ll&quot;: &quot;he will&quot;, &quot;he&#39;ll&#39;ve&quot;: &quot;he will have&quot;, &quot;he&#39;s&quot;: &quot;he is&quot;, &quot;how&#39;d&quot;: &quot;how did&quot;, &quot;how&#39;d&#39;y&quot;: &quot;how do you&quot;, &quot;how&#39;ll&quot;: &quot;how will&quot;, &quot;how&#39;s&quot;: &quot;how does&quot;, &quot;i&#39;d&quot;: &quot;i would&quot;, &quot;i&#39;d&#39;ve&quot;: &quot;i would have&quot;, &quot;i&#39;ll&quot;: &quot;i will&quot;, &quot;i&#39;ll&#39;ve&quot;: &quot;i will have&quot;, &quot;i&#39;m&quot;: &quot;i am&quot;, &quot;i&#39;ve&quot;: &quot;i have&quot;, &quot;isn&#39;t&quot;: &quot;is not&quot;, &quot;it&#39;d&quot;: &quot;it would&quot;, &quot;it&#39;d&#39;ve&quot;: &quot;it would have&quot;, &quot;it&#39;ll&quot;: &quot;it will&quot;, &quot;it&#39;ll&#39;ve&quot;: &quot;it will have&quot;, &quot;it&#39;s&quot;: &quot;it is&quot;, &quot;let&#39;s&quot;: &quot;let us&quot;, &quot;ma&#39;am&quot;: &quot;madam&quot;, &quot;mayn&#39;t&quot;: &quot;may not&quot;, &quot;might&#39;ve&quot;: &quot;might have&quot;, &quot;mightn&#39;t&quot;: &quot;might not&quot;, &quot;mightn&#39;t&#39;ve&quot;: &quot;might not have&quot;, &quot;must&#39;ve&quot;: &quot;must have&quot;, &quot;mustn&#39;t&quot;: &quot;must not&quot;, &quot;mustn&#39;t&#39;ve&quot;: &quot;must not have&quot;, &quot;needn&#39;t&quot;: &quot;need not&quot;, &quot;needn&#39;t&#39;ve&quot;: &quot;need not have&quot;, &quot;o&#39;clock&quot;: &quot;of the clock&quot;, &quot;oughtn&#39;t&quot;: &quot;ought not&quot;, &quot;oughtn&#39;t&#39;ve&quot;: &quot;ought not have&quot;, &quot;shan&#39;t&quot;: &quot;shall not&quot;, &quot;sha&#39;n&#39;t&quot;: &quot;shall not&quot;, &quot;shan&#39;t&#39;ve&quot;: &quot;shall not have&quot;, &quot;she&#39;d&quot;: &quot;she would&quot;, &quot;she&#39;d&#39;ve&quot;: &quot;she would have&quot;, &quot;she&#39;ll&quot;: &quot;she will&quot;, &quot;she&#39;ll&#39;ve&quot;: &quot;she will have&quot;, &quot;she&#39;s&quot;: &quot;she is&quot;, &quot;should&#39;ve&quot;: &quot;should have&quot;, &quot;shouldn&#39;t&quot;: &quot;should not&quot;, &quot;shouldn&#39;t&#39;ve&quot;: &quot;should not have&quot;, &quot;so&#39;ve&quot;: &quot;so have&quot;, &quot;so&#39;s&quot;: &quot;so is&quot;, &quot;that&#39;d&quot;: &quot;that would&quot;, &quot;that&#39;d&#39;ve&quot;: &quot;that would have&quot;, &quot;that&#39;s&quot;: &quot;that is&quot;, &quot;there&#39;d&quot;: &quot;there would&quot;, &quot;there&#39;d&#39;ve&quot;: &quot;there would have&quot;, &quot;there&#39;s&quot;: &quot;there is&quot;, &quot;they&#39;d&quot;: &quot;they would&quot;, &quot;they&#39;d&#39;ve&quot;: &quot;they would have&quot;, &quot;they&#39;ll&quot;: &quot;they will&quot;, &quot;they&#39;ll&#39;ve&quot;: &quot;they will have&quot;, &quot;they&#39;re&quot;: &quot;they are&quot;, &quot;they&#39;ve&quot;: &quot;they have&quot;, &quot;to&#39;ve&quot;: &quot;to have&quot;, &quot;wasn&#39;t&quot;: &quot;was not&quot;, &quot; u &quot;: &quot; you &quot;, &quot; ur &quot;: &quot; your &quot;, &quot; n &quot;: &quot; and &quot;, &quot;won&#39;t&quot;: &quot;would not&quot;, &#39;dis &#39;: &#39;this &#39;, &#39;bak &#39;: &#39;back &#39;, &#39;brng&#39;: &#39;bring&#39;} . def cont_to_exp(x): if type(x) is str: for key in contractions: value = contractions[key] x = x.replace(key, value) return x else: return x . def clear_text(text): text = text.lower() text = re.sub(r&quot;[^a-z&#39;]+&quot;, &quot; &quot;, text) return &quot; &quot;.join(text.split()) . df_reviews[&#39;review_norm&#39;] = df_reviews[&#39;review&#39;].progress_apply(clear_text) . . df_reviews[&#39;review_norm&#39;] = df_reviews[&#39;review_norm&#39;].progress_apply(cont_to_exp) . . df_reviews[&#39;review_norm&#39;].head() . 0 it is not turkish it is a lie but i recommend ... 1 you do not have korean 2 very nice only if it were in italian it would ... 3 the game is nice but when it comes to turkish ... 4 amazing work hope to achieve chinese translation Name: review_norm, dtype: object . Reviews are now all lower case and contractions have been removed to simplify vectorization | . Sampling . Both upsampling and downsampling were attempted on this dataset | No improvement to results were achieved so the code was removed to declutter | . Split the Data . train, test = train_test_split(df_reviews, test_size=0.25, stratify = df_reviews[&#39;mark&#39;], random_state=STATE) . X_train = train.drop([&#39;id&#39;, &#39;review&#39;, &#39;mark&#39;], axis=1) y_train = train[&#39;mark&#39;] X_test = test.drop([&#39;id&#39;, &#39;review&#39;, &#39;mark&#39;], axis=1) y_test = test[&#39;mark&#39;] . display(X_train.shape[0]) X_test.shape[0] . 38835 . 12946 . y_train.value_counts() . RL 32006 L- 4108 YL 2221 L+ 500 Name: mark, dtype: int64 . y_test.value_counts() . RL 10670 L- 1369 YL 741 L+ 166 Name: mark, dtype: int64 . Logistic Regression Model . Count Vectorizer . corpus = X_train[&#39;review_norm&#39;] count_vect = CountVectorizer(stop_words=&#39;english&#39;, ngram_range=(2,3), max_features=30000) X_train_1 = count_vect.fit_transform(corpus) . corpus = X_test[&#39;review_norm&#39;] X_test_1 = count_vect.transform(corpus) . grid={ &quot;penalty&quot;:[&quot;l2&quot;], &quot;fit_intercept&quot;: [True, False], &quot;random_state&quot;: [STATE], &quot;solver&quot;: [&quot;newton-cg&quot;, &quot;lbfgs&quot;, &quot;liblinear&quot;, &quot;sag&quot;, &quot;saga&quot;], &quot;max_iter&quot;: [1000], &quot;multi_class&quot;: [&quot;ovr&quot;, &quot;multinomial&quot;], &quot;n_jobs&quot;: [-1], } . model_lr = LogisticRegression() lr_cv=GridSearchCV(model_lr,grid,cv=5) lr_cv.fit(X_train_1 ,y_train) print(&quot;Tuned Hyperparameters:&quot;, lr_cv.best_params_) print(&quot;Accuracy:&quot;, lr_cv.best_score_) . Tuned Hyperparameters: {&#39;fit_intercept&#39;: True, &#39;max_iter&#39;: 1000, &#39;multi_class&#39;: &#39;multinomial&#39;, &#39;n_jobs&#39;: -1, &#39;penalty&#39;: &#39;l2&#39;, &#39;random_state&#39;: 12345, &#39;solver&#39;: &#39;saga&#39;} Accuracy: 0.8754731556585554 . This cell can take hours to run | If significant changes to preprocessing or data, please run again | . model_lr = LogisticRegression(**lr_cv.best_params_) model_lr.fit(X_train_1, y_train) . LogisticRegression(max_iter=1000, multi_class=&#39;multinomial&#39;, n_jobs=-1, random_state=12345, solver=&#39;saga&#39;) . pred = model_lr.predict(X_test_1) print(classification_report(y_test,pred)) print(accuracy_score(y_test,pred)) . precision recall f1-score support L+ 0.65 0.19 0.30 166 L- 0.80 0.52 0.63 1369 RL 0.89 0.98 0.94 10670 YL 0.58 0.17 0.27 741 accuracy 0.88 12946 macro avg 0.73 0.47 0.53 12946 weighted avg 0.86 0.88 0.86 12946 0.8791132396106905 . These are strong results for Logistic Regression | Not enough unique data for the smaller categories it seems | . Tfidf Vectorizer . corpus = X_train[&#39;review_norm&#39;] tfidf_vect= TfidfVectorizer(stop_words=&#39;english&#39;, ngram_range=(2,3), max_features=30000) X_train_2 = tfidf_vect.fit_transform(corpus) . corpus = X_test[&#39;review_norm&#39;] X_test_2 = tfidf_vect.transform(corpus) . model_lr_2 = LogisticRegression() lr_cv=GridSearchCV(model_lr_2,grid,cv=5) lr_cv.fit(X_train_2 ,y_train) print(&quot;Tuned Hyperparameters:&quot;, lr_cv.best_params_) print(&quot;Accuracy:&quot;, lr_cv.best_score_) . Tuned Hyperparameters: {&#39;fit_intercept&#39;: True, &#39;max_iter&#39;: 1000, &#39;multi_class&#39;: &#39;multinomial&#39;, &#39;n_jobs&#39;: -1, &#39;penalty&#39;: &#39;l2&#39;, &#39;random_state&#39;: 12345, &#39;solver&#39;: &#39;saga&#39;} Accuracy: 0.8592764259044676 . This cell can take hours to run | Results are hardcoded to avoid rerunning hours of calculations | If significant changes to preprocessing or data, please run again | . model_lr_2 = LogisticRegression(**lr_cv.best_params_) model_lr_2.fit(X_train_2, y_train) . LogisticRegression(max_iter=1000, multi_class=&#39;multinomial&#39;, n_jobs=-1, random_state=12345, solver=&#39;saga&#39;) . pred = model_lr_2.predict(X_test_2) print(classification_report(y_test,pred)) print(accuracy_score(y_test,pred)) . precision recall f1-score support L+ 0.60 0.05 0.10 166 L- 0.84 0.44 0.58 1369 RL 0.87 0.99 0.93 10670 YL 0.84 0.05 0.09 741 accuracy 0.87 12946 macro avg 0.79 0.38 0.42 12946 weighted avg 0.86 0.87 0.83 12946 0.869457747566816 . Significantly worse results on the smaller categories | Count Vectorizer is the clear winner here | . Catboost Model . text_features = [&#39;review_norm&#39;] . train_pool = Pool( X_train, y_train, text_features=text_features, feature_names=list(X_train) ) valid_pool = Pool( X_test, y_test, text_features=text_features, feature_names=list(X_train) ) catboost_params = { &#39;iterations&#39;: 5000, &#39;learning_rate&#39;: 0.03, &#39;eval_metric&#39;: &#39;TotalF1&#39;, &#39;task_type&#39;: &#39;GPU&#39; if USE_GPU else &#39;CPU&#39;, &#39;early_stopping_rounds&#39;: 2000, &#39;use_best_model&#39;: True, &#39;verbose&#39;: 500, &#39;random_state&#39;: STATE } . model_cb = CatBoostClassifier(**catboost_params) model_cb.fit(train_pool, eval_set=valid_pool) . 0: learn: 0.8517705 test: 0.8532483 best: 0.8532483 (0) total: 28.7ms remaining: 2m 23s 500: learn: 0.8950334 test: 0.8904317 best: 0.8904748 (499) total: 10.8s remaining: 1m 37s 1000: learn: 0.9092785 test: 0.8978792 best: 0.8983240 (947) total: 21.7s remaining: 1m 26s 1500: learn: 0.9170555 test: 0.9014397 best: 0.9017088 (1487) total: 32.6s remaining: 1m 15s 2000: learn: 0.9232431 test: 0.9042233 best: 0.9043929 (1986) total: 43.5s remaining: 1m 5s 2500: learn: 0.9278779 test: 0.9054554 best: 0.9057675 (2439) total: 54.4s remaining: 54.4s 3000: learn: 0.9318730 test: 0.9062368 best: 0.9065412 (2887) total: 1m 5s remaining: 43.6s 3500: learn: 0.9347331 test: 0.9078687 best: 0.9079615 (3480) total: 1m 16s remaining: 32.7s 4000: learn: 0.9373805 test: 0.9088320 best: 0.9088320 (4000) total: 1m 27s remaining: 21.8s 4500: learn: 0.9400044 test: 0.9094047 best: 0.9098289 (4252) total: 1m 37s remaining: 10.9s 4999: learn: 0.9423276 test: 0.9102575 best: 0.9104906 (4982) total: 1m 48s remaining: 0us bestTest = 0.9104905816 bestIteration = 4982 Shrink model to first 4983 iterations. . &lt;catboost.core.CatBoostClassifier at 0x242871f2e48&gt; . pred = model_cb.predict(X_test) print(classification_report(y_test,pred)) print(accuracy_score(y_test,pred)) . precision recall f1-score support L+ 0.65 0.23 0.35 166 L- 0.80 0.79 0.79 1369 RL 0.94 0.98 0.96 10670 YL 0.77 0.43 0.55 741 accuracy 0.92 12946 macro avg 0.79 0.61 0.66 12946 weighted avg 0.91 0.92 0.91 12946 0.9181986714042948 . Very strong results for RL category | The trend appears to be that with more data points the models accuracy increases in a category | . Experiment on Multiple Model Usage . Phase 1 - Localization Requests . reviews_set_1 = df_reviews.copy() . for i in range(len(reviews_set_1)): if reviews_set_1[&#39;mark&#39;][i] != &#39;RL&#39;: reviews_set_1[&#39;mark&#39;][i] = &#39;YL&#39; . reviews_set_1[&#39;mark&#39;].value_counts() . RL 42676 YL 9105 Name: mark, dtype: int64 . X1 = reviews_set_1.drop([&#39;id&#39;, &#39;review&#39;, &#39;mark&#39;], axis=1) y1 = reviews_set_1[&#39;mark&#39;] . X1_train, X1_test, y1_train, y1_test = train_test_split(X1,y1, test_size=0.25, stratify = y1) . train_pool_2 = Pool( X1_train, y1_train, text_features=text_features, feature_names=list(X1_train) ) valid_pool_2 = Pool( X1_test, y1_test, text_features=text_features, feature_names=list(X1_train) ) . model1 = CatBoostClassifier(**catboost_params) model1.fit(train_pool_2, eval_set=valid_pool_2) . 0: learn: 0.8866448 test: 0.8891357 best: 0.8891357 (0) total: 60.6ms remaining: 5m 2s 500: learn: 0.9226201 test: 0.9155004 best: 0.9156253 (494) total: 28.6s remaining: 4m 16s 1000: learn: 0.9332266 test: 0.9222860 best: 0.9222860 (1000) total: 56.7s remaining: 3m 46s 1500: learn: 0.9405186 test: 0.9246324 best: 0.9248068 (1487) total: 1m 24s remaining: 3m 17s 2000: learn: 0.9457609 test: 0.9258364 best: 0.9260103 (1830) total: 1m 52s remaining: 2m 48s 2500: learn: 0.9501938 test: 0.9264900 best: 0.9266340 (2435) total: 2m 20s remaining: 2m 19s 3000: learn: 0.9533795 test: 0.9268223 best: 0.9275135 (2844) total: 2m 47s remaining: 1m 51s 3500: learn: 0.9567446 test: 0.9272541 best: 0.9275135 (2844) total: 3m 15s remaining: 1m 23s 4000: learn: 0.9593883 test: 0.9279158 best: 0.9281178 (3914) total: 3m 43s remaining: 55.8s 4500: learn: 0.9618747 test: 0.9285048 best: 0.9285048 (4491) total: 4m 11s remaining: 27.9s 4999: learn: 0.9639499 test: 0.9284468 best: 0.9287630 (4944) total: 4m 38s remaining: 0us bestTest = 0.9287630212 bestIteration = 4944 Shrink model to first 4945 iterations. . &lt;catboost.core.CatBoostClassifier at 0x242871f2948&gt; . pred1 = model1.predict(X1_test) print(classification_report(y1_test,pred1)) print(accuracy_score(y1_test,pred1)) . precision recall f1-score support RL 0.95 0.97 0.96 10670 YL 0.84 0.75 0.79 2276 accuracy 0.93 12946 macro avg 0.89 0.86 0.87 12946 weighted avg 0.93 0.93 0.93 12946 0.9304032133477522 . Phase 2 - Localization Reviews . reviews_set_2 = df_reviews.copy() . reviews_set_2 = reviews_set_2[reviews_set_2[&#39;mark&#39;] != &#39;RL&#39;] . reviews_set_2[&#39;mark&#39;].value_counts() . L- 5477 YL 2962 L+ 666 Name: mark, dtype: int64 . X2 = reviews_set_2.drop([&#39;id&#39;, &#39;review&#39;, &#39;mark&#39;], axis=1) y2 = reviews_set_2[&#39;mark&#39;] . X2_train, X2_test, y2_train, y2_test = train_test_split(X2,y2, test_size=0.25, stratify = y2) . train_pool_3 = Pool( X2_train, y2_train, text_features=text_features, feature_names=list(X2_train) ) valid_pool_3 = Pool( X2_test, y2_test, text_features=text_features, feature_names=list(X2_train) ) . model2 = CatBoostClassifier(**catboost_params) model2.fit(train_pool_3, eval_set=valid_pool_3) . 0: learn: 0.7968397 test: 0.8152103 best: 0.8152103 (0) total: 25.9ms remaining: 2m 9s 500: learn: 0.8476542 test: 0.8385124 best: 0.8385909 (493) total: 10s remaining: 1m 29s 1000: learn: 0.8767694 test: 0.8500063 best: 0.8500063 (995) total: 20.1s remaining: 1m 20s 1500: learn: 0.8962401 test: 0.8518733 best: 0.8529896 (1485) total: 30.3s remaining: 1m 10s 2000: learn: 0.9091538 test: 0.8553469 best: 0.8558676 (1950) total: 40.1s remaining: 1m 2500: learn: 0.9181506 test: 0.8573688 best: 0.8573688 (2474) total: 49.8s remaining: 49.7s 3000: learn: 0.9274049 test: 0.8598072 best: 0.8598072 (2976) total: 59.6s remaining: 39.7s 3500: learn: 0.9361894 test: 0.8619213 best: 0.8622723 (3151) total: 1m 9s remaining: 29.7s 4000: learn: 0.9431237 test: 0.8620891 best: 0.8634842 (3565) total: 1m 19s remaining: 19.9s 4500: learn: 0.9481487 test: 0.8619907 best: 0.8634842 (3565) total: 1m 29s remaining: 9.97s 4999: learn: 0.9529232 test: 0.8618240 best: 0.8634842 (3565) total: 1m 39s remaining: 0us bestTest = 0.8634842157 bestIteration = 3565 Shrink model to first 3566 iterations. . &lt;catboost.core.CatBoostClassifier at 0x2428c32b1c8&gt; . pred2 = model2.predict(X2_test) print(classification_report(y2_test,pred2)) print(accuracy_score(y2_test,pred2)) . precision recall f1-score support L+ 0.69 0.31 0.43 166 L- 0.89 0.95 0.92 1370 YL 0.85 0.86 0.86 741 accuracy 0.87 2277 macro avg 0.81 0.71 0.74 2277 weighted avg 0.87 0.87 0.86 2277 0.873078612209047 . Strong results so far, significantly better than a single Logistic Regression model | It appears, as mentioned before, more data points are needed to improve | Errors in the first model will compound into the second model under this approach | . Final Conclusion . Catboost was our strongest performing model with a 96% accuracy for RL (Localization Requests) | No model performed well at predicting the minority targets of YL, L+, and L-. | We need more data! The dataset is heavily imbalanced at this time. | We can potentially negate 80% of the manual labeling work by relying on the single Catboost model | If the minority categories are more important, the 2 model approach yielded better results on this front. | Without more data our next steps for seeking improvement are creating a Deep Learning/Neural Network | . Implementation . A python file has been provided to Allcorrect Games, once run they can call label_catboost_excel(File_Path, Model_Path) in the terminal. | The script will add a predictions column to the dataset using the catboost model from our single model approach | .",
            "url": "https://nicholas-j-snyder.github.io/portfolio/fastpages/jupyter/2021/07/04/Allcorrect-Games.html",
            "relUrl": "/fastpages/jupyter/2021/07/04/Allcorrect-Games.html",
            "date": " ‚Ä¢ Jul 4, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Steel Processing",
            "content": "Problem Statement: . In order to optimize production costs, the steel plant, Steelproof, has decided to reduce their energy consumption at the steel processing stage. . Initial Information: . Data: . Additive materials (Bulk and Wire) + timestamps | Heating times and power usage | Temp readings + timestamps | Inert Gas usage | . Questions: . Question: Is any of the data not immediately available during production? Response: Intermediate temperatures cannot be used in the model, the sensors are unable to quickly determine these temperatures. . Question: For the Bulk materials and wire, are the NaNs unknown values or the automatic value for when that material wasn&#39;t used? Response: When the material wasn&#39;t used . Plan: . Isolate the target variable into its own series. | Remove date-time variables after deriving a duration variable from electrode data | Merge remaining data into a single dataframe where each iteration is only one row. | Build a variety of Regression models and test them | Solution: . We stuck to the plan for the most part. | We completely removed the timestamps for the bulk, wire, and electrode data. | Isolated the target variable and merged the remaining data into one dataframe. | Our biggest deviation from the plan was calculating the entire process duration instead of the heating duration . | Seperating the first and and final temp readings for each iteration was the trickier but most crucial part of the process . | After that merging the data was the final hurdle but was simplified by having a key column to merge on . | We tested 3 models against a dummy model to try and find the best one: . Linear Regression | CatBoost Regressor | Ridge | | The dummy model (sanity test) scored just over an 8 MAE | Linear Regression and Ridge performed adequately with MAE scores just under 6 | We tried CatBoost with some parameter tuning and the basic format - the base model performed better | . Results: . Catboost overall was our best model. The average error rate (MAE) was down to 5.22 with CatBoost | Model training time is usally a concern with CatBoost but since we are using the base model CatBoost runs quickly | . Init . import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.metrics import mean_absolute_error as MAE from sklearn.model_selection import train_test_split from sklearn.dummy import DummyRegressor from sklearn.linear_model import LinearRegression from sklearn.linear_model import Ridge from catboost import CatBoostRegressor from sklearn.model_selection import GridSearchCV . data_elec = pd.read_csv(&#39;/datasets/data_arc_en.csv&#39;) data_bulk = pd.read_csv(&#39;/datasets/data_bulk_en.csv&#39;) data_temp = pd.read_csv(&#39;/datasets/data_temp_en.csv&#39;) data_wire = pd.read_csv(&#39;/datasets/data_wire_en.csv&#39;) data_gas = pd.read_csv(&#39;/datasets/data_gas_en.csv&#39;) . . Cleaning &amp; Merging . data_temp[&#39;Sampling time&#39;] = pd.to_datetime(data_temp[&#39;Sampling time&#39;]) final_temp = (data_temp .drop_duplicates([&#39;key&#39;], keep=&#39;last&#39;) .reset_index(drop=True) .rename(columns={&#39;Temperature&#39;: &#39;Final Temp&#39;, &#39;Sampling time&#39;: &#39;End Time&#39;})) initial_temp = (data_temp .drop_duplicates([&#39;key&#39;]) .reset_index(drop=True) .rename(columns={&#39;Temperature&#39;: &#39;Initial Temp&#39;, &#39;Sampling time&#39;: &#39;Start Time&#39;})) final_temp[&#39;Duration&#39;] = (final_temp[&#39;End Time&#39;] - initial_temp[&#39;Start Time&#39;]).dt.total_seconds() initial_temp = initial_temp.drop([&#39;Start Time&#39;], axis=1) final_temp = final_temp.drop([&#39;End Time&#39;], axis=1) display(initial_temp.head()) final_temp.head() . key Initial Temp . 0 | 1 | 1571.0 | . 1 | 2 | 1581.0 | . 2 | 3 | 1596.0 | . 3 | 4 | 1601.0 | . 4 | 5 | 1576.0 | . key Final Temp Duration . 0 | 1 | 1613.0 | 861.0 | . 1 | 2 | 1602.0 | 1305.0 | . 2 | 3 | 1599.0 | 1300.0 | . 3 | 4 | 1625.0 | 388.0 | . 4 | 5 | 1602.0 | 762.0 | . data_elec = (data_elec .pivot_table(index=&#39;key&#39;, values=[&#39;Active power&#39;, &#39;Reactive power&#39;], aggfunc=&#39;sum&#39;) .reset_index()) data_elec.head() . key Active power Reactive power . 0 | 1 | 4.878147 | 3.183241 | . 1 | 2 | 3.052598 | 1.998112 | . 2 | 3 | 2.525882 | 1.599076 | . 3 | 4 | 3.209250 | 2.060298 | . 4 | 5 | 3.347173 | 2.252643 | . data_bulk = data_bulk.fillna(0) data_wire = data_wire.fillna(0) . data_all = (initial_temp .merge(final_temp, on=&#39;key&#39;, how=&#39;outer&#39;) .merge(data_bulk, on=&#39;key&#39;, how=&#39;outer&#39;) .merge(data_wire, on=&#39;key&#39;, how=&#39;outer&#39;) .merge(data_elec, on=&#39;key&#39;, how=&#39;outer&#39;) .merge(data_gas, on=&#39;key&#39;, how=&#39;outer&#39;) .drop(&#39;key&#39;, axis = 1)) . data_all.info() data_all.head() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 3241 entries, 0 to 3240 Data columns (total 30 columns): Initial Temp 3216 non-null float64 Final Temp 2477 non-null float64 Duration 3216 non-null float64 Bulk 1 3129 non-null float64 Bulk 2 3129 non-null float64 Bulk 3 3129 non-null float64 Bulk 4 3129 non-null float64 Bulk 5 3129 non-null float64 Bulk 6 3129 non-null float64 Bulk 7 3129 non-null float64 Bulk 8 3129 non-null float64 Bulk 9 3129 non-null float64 Bulk 10 3129 non-null float64 Bulk 11 3129 non-null float64 Bulk 12 3129 non-null float64 Bulk 13 3129 non-null float64 Bulk 14 3129 non-null float64 Bulk 15 3129 non-null float64 Wire 1 3081 non-null float64 Wire 2 3081 non-null float64 Wire 3 3081 non-null float64 Wire 4 3081 non-null float64 Wire 5 3081 non-null float64 Wire 6 3081 non-null float64 Wire 7 3081 non-null float64 Wire 8 3081 non-null float64 Wire 9 3081 non-null float64 Active power 3214 non-null float64 Reactive power 3214 non-null float64 Gas 1 3239 non-null float64 dtypes: float64(30) memory usage: 784.9 KB . Initial Temp Final Temp Duration Bulk 1 Bulk 2 Bulk 3 Bulk 4 Bulk 5 Bulk 6 Bulk 7 ... Wire 3 Wire 4 Wire 5 Wire 6 Wire 7 Wire 8 Wire 9 Active power Reactive power Gas 1 . 0 | 1571.0 | 1613.0 | 861.0 | 0.0 | 0.0 | 0.0 | 43.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 4.878147 | 3.183241 | 29.749986 | . 1 | 1581.0 | 1602.0 | 1305.0 | 0.0 | 0.0 | 0.0 | 73.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 3.052598 | 1.998112 | 12.555561 | . 2 | 1596.0 | 1599.0 | 1300.0 | 0.0 | 0.0 | 0.0 | 34.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2.525882 | 1.599076 | 28.554793 | . 3 | 1601.0 | 1625.0 | 388.0 | 0.0 | 0.0 | 0.0 | 81.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 3.209250 | 2.060298 | 18.841219 | . 4 | 1576.0 | 1602.0 | 762.0 | 0.0 | 0.0 | 0.0 | 78.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 3.347173 | 2.252643 | 5.413692 | . 5 rows √ó 30 columns . . data_all = data_all.dropna() data_all.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 2329 entries, 0 to 2476 Data columns (total 30 columns): Initial Temp 2329 non-null float64 Final Temp 2329 non-null float64 Duration 2329 non-null float64 Bulk 1 2329 non-null float64 Bulk 2 2329 non-null float64 Bulk 3 2329 non-null float64 Bulk 4 2329 non-null float64 Bulk 5 2329 non-null float64 Bulk 6 2329 non-null float64 Bulk 7 2329 non-null float64 Bulk 8 2329 non-null float64 Bulk 9 2329 non-null float64 Bulk 10 2329 non-null float64 Bulk 11 2329 non-null float64 Bulk 12 2329 non-null float64 Bulk 13 2329 non-null float64 Bulk 14 2329 non-null float64 Bulk 15 2329 non-null float64 Wire 1 2329 non-null float64 Wire 2 2329 non-null float64 Wire 3 2329 non-null float64 Wire 4 2329 non-null float64 Wire 5 2329 non-null float64 Wire 6 2329 non-null float64 Wire 7 2329 non-null float64 Wire 8 2329 non-null float64 Wire 9 2329 non-null float64 Active power 2329 non-null float64 Reactive power 2329 non-null float64 Gas 1 2329 non-null float64 dtypes: float64(30) memory usage: 564.1 KB . . Train/Test Split . features = data_all.drop(&#39;Final Temp&#39;, axis=1) target = data_all[&#39;Final Temp&#39;] . features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.2, random_state=54321) . display(features_train.shape) display(target_train.shape) display(features_test.shape) target_test.shape . (1863, 29) . (1863,) . (466, 29) . (466,) . Model 0 - Sanity Test . model_0 = DummyRegressor(strategy=&#39;mean&#39;) . model_0.fit(features_train, target_train) pred_0 = model_0.predict(features_test) . print(&#39;Sanity Test MAE:&#39;, MAE(target_test, pred_0)) . Sanity Test MAE: 8.025588660128683 . Model 1 - LinearRegression . model_1 = LinearRegression() . model_1.fit(features_train, target_train) pred_1 = model_1.predict(features_test) . print(&#39;Linear Regression MAE:&#39;, MAE(target_test, pred_1)) . Linear Regression MAE: 5.9500027174198 . Model 2 - CatBoost . model_2 = CatBoostRegressor(random_state=54321, loss_function=&#39;MAE&#39;) . #params = {&#39;learning_rate&#39;: [0.03, 0.05, 0.1], # &#39;depth&#39;: [4, 6, 10], # &#39;l2_leaf_reg&#39;: [1, 3, 5, 7, 9]} # #model_2.grid_search(params, # X=features_train, # y=target_train, # refit=True, # partition_random_seed= 12345) model_2.fit(features_train, target_train) pred_2 = model_2.predict(features_test) . 0: learn: 7.9989684 total: 50.1ms remaining: 50.1s 1: learn: 7.9565330 total: 52.7ms remaining: 26.3s 2: learn: 7.8866288 total: 55ms remaining: 18.3s 3: learn: 7.8106662 total: 57.7ms remaining: 14.4s 4: learn: 7.7578909 total: 79.7ms remaining: 15.9s 5: learn: 7.6960067 total: 82.1ms remaining: 13.6s 6: learn: 7.6343002 total: 84.3ms remaining: 12s 7: learn: 7.5814495 total: 86.7ms remaining: 10.8s 8: learn: 7.5321962 total: 89.2ms remaining: 9.82s 9: learn: 7.4817674 total: 96.7ms remaining: 9.57s 10: learn: 7.4338809 total: 178ms remaining: 16s 11: learn: 7.3868121 total: 181ms remaining: 14.9s 12: learn: 7.3400538 total: 184ms remaining: 13.9s 13: learn: 7.2879249 total: 186ms remaining: 13.1s 14: learn: 7.2449540 total: 193ms remaining: 12.7s 15: learn: 7.1894576 total: 277ms remaining: 17.1s 16: learn: 7.1415323 total: 280ms remaining: 16.2s 17: learn: 7.1078082 total: 282ms remaining: 15.4s 18: learn: 7.0719712 total: 285ms remaining: 14.7s 19: learn: 7.0252944 total: 298ms remaining: 14.6s 20: learn: 6.9934865 total: 374ms remaining: 17.4s 21: learn: 6.9572568 total: 377ms remaining: 16.7s 22: learn: 6.9196134 total: 379ms remaining: 16.1s 23: learn: 6.8806717 total: 381ms remaining: 15.5s 24: learn: 6.8404567 total: 389ms remaining: 15.2s 25: learn: 6.8082277 total: 471ms remaining: 17.7s 26: learn: 6.7740479 total: 476ms remaining: 17.1s 27: learn: 6.7423628 total: 479ms remaining: 16.6s 28: learn: 6.7187170 total: 481ms remaining: 16.1s 29: learn: 6.6867719 total: 568ms remaining: 18.4s 30: learn: 6.6627984 total: 571ms remaining: 17.9s 31: learn: 6.6434860 total: 574ms remaining: 17.4s 32: learn: 6.6079620 total: 576ms remaining: 16.9s 33: learn: 6.5747595 total: 584ms remaining: 16.6s 34: learn: 6.5535987 total: 668ms remaining: 18.4s 35: learn: 6.5362917 total: 670ms remaining: 17.9s 36: learn: 6.5166561 total: 673ms remaining: 17.5s 37: learn: 6.4875137 total: 676ms remaining: 17.1s 38: learn: 6.4600186 total: 683ms remaining: 16.8s 39: learn: 6.4376929 total: 765ms remaining: 18.3s 40: learn: 6.4169295 total: 767ms remaining: 17.9s 41: learn: 6.3894418 total: 769ms remaining: 17.5s 42: learn: 6.3653935 total: 772ms remaining: 17.2s 43: learn: 6.3428477 total: 779ms remaining: 16.9s 44: learn: 6.3238255 total: 863ms remaining: 18.3s 45: learn: 6.3081973 total: 865ms remaining: 17.9s 46: learn: 6.2870123 total: 867ms remaining: 17.6s 47: learn: 6.2712817 total: 872ms remaining: 17.3s 48: learn: 6.2558996 total: 959ms remaining: 18.6s 49: learn: 6.2380085 total: 962ms remaining: 18.3s 50: learn: 6.2203729 total: 964ms remaining: 17.9s 51: learn: 6.1988614 total: 967ms remaining: 17.6s 52: learn: 6.1780180 total: 975ms remaining: 17.4s 53: learn: 6.1645581 total: 1.06s remaining: 18.5s 54: learn: 6.1484354 total: 1.06s remaining: 18.2s 55: learn: 6.1281721 total: 1.06s remaining: 17.9s 56: learn: 6.1146503 total: 1.06s remaining: 17.6s 57: learn: 6.0998612 total: 1.15s remaining: 18.8s 58: learn: 6.0824644 total: 1.16s remaining: 18.5s 59: learn: 6.0669359 total: 1.16s remaining: 18.2s 60: learn: 6.0547116 total: 1.16s remaining: 17.9s 61: learn: 6.0394568 total: 1.16s remaining: 17.6s 62: learn: 6.0223344 total: 1.17s remaining: 17.4s 63: learn: 6.0011750 total: 1.25s remaining: 18.3s 64: learn: 5.9849108 total: 1.25s remaining: 18.1s 65: learn: 5.9701874 total: 1.26s remaining: 17.8s 66: learn: 5.9551243 total: 1.26s remaining: 17.5s 67: learn: 5.9306202 total: 1.27s remaining: 17.4s 68: learn: 5.9135860 total: 1.35s remaining: 18.2s 69: learn: 5.9006360 total: 1.35s remaining: 18s 70: learn: 5.8903074 total: 1.36s remaining: 17.7s 71: learn: 5.8852777 total: 1.36s remaining: 17.5s 72: learn: 5.8705188 total: 1.37s remaining: 17.4s 73: learn: 5.8594710 total: 1.46s remaining: 18.3s 74: learn: 5.8445154 total: 1.47s remaining: 18.1s 75: learn: 5.8315849 total: 1.47s remaining: 17.9s 76: learn: 5.8115439 total: 1.47s remaining: 17.7s 77: learn: 5.8001994 total: 1.55s remaining: 18.3s 78: learn: 5.7874286 total: 1.55s remaining: 18.1s 79: learn: 5.7757598 total: 1.55s remaining: 17.9s 80: learn: 5.7640241 total: 1.56s remaining: 17.7s 81: learn: 5.7537453 total: 1.57s remaining: 17.6s 82: learn: 5.7436947 total: 1.64s remaining: 18.1s 83: learn: 5.7250444 total: 1.64s remaining: 17.9s 84: learn: 5.7146712 total: 1.65s remaining: 17.7s 85: learn: 5.7001721 total: 1.65s remaining: 17.5s 86: learn: 5.6855273 total: 1.66s remaining: 17.4s 87: learn: 5.6786800 total: 1.74s remaining: 18s 88: learn: 5.6681231 total: 1.74s remaining: 17.8s 89: learn: 5.6554205 total: 1.75s remaining: 17.6s 90: learn: 5.6454602 total: 1.75s remaining: 17.5s 91: learn: 5.6352421 total: 1.76s remaining: 17.4s 92: learn: 5.6234342 total: 1.84s remaining: 17.9s 93: learn: 5.6153725 total: 1.84s remaining: 17.8s 94: learn: 5.6056223 total: 1.84s remaining: 17.6s 95: learn: 5.5963865 total: 1.85s remaining: 17.4s 96: learn: 5.5874907 total: 1.85s remaining: 17.3s 97: learn: 5.5794233 total: 1.94s remaining: 17.8s 98: learn: 5.5681365 total: 1.94s remaining: 17.6s 99: learn: 5.5565902 total: 1.94s remaining: 17.5s 100: learn: 5.5476485 total: 1.94s remaining: 17.3s 101: learn: 5.5381966 total: 2.03s remaining: 17.9s 102: learn: 5.5326418 total: 2.04s remaining: 17.7s 103: learn: 5.5238155 total: 2.04s remaining: 17.6s 104: learn: 5.5180262 total: 2.04s remaining: 17.4s 105: learn: 5.5094624 total: 2.04s remaining: 17.2s 106: learn: 5.4990685 total: 2.05s remaining: 17.1s 107: learn: 5.4895483 total: 2.13s remaining: 17.6s 108: learn: 5.4809529 total: 2.13s remaining: 17.5s 109: learn: 5.4767501 total: 2.14s remaining: 17.3s 110: learn: 5.4663553 total: 2.14s remaining: 17.1s 111: learn: 5.4566162 total: 2.14s remaining: 17s 112: learn: 5.4485638 total: 2.23s remaining: 17.5s 113: learn: 5.4416017 total: 2.23s remaining: 17.4s 114: learn: 5.4327902 total: 2.23s remaining: 17.2s 115: learn: 5.4230518 total: 2.24s remaining: 17.1s 116: learn: 5.4116698 total: 2.25s remaining: 16.9s 117: learn: 5.4035858 total: 2.33s remaining: 17.4s 118: learn: 5.3960060 total: 2.33s remaining: 17.3s 119: learn: 5.3872724 total: 2.33s remaining: 17.1s 120: learn: 5.3780943 total: 2.33s remaining: 17s 121: learn: 5.3753177 total: 2.34s remaining: 16.9s 122: learn: 5.3682899 total: 2.42s remaining: 17.3s 123: learn: 5.3613156 total: 2.43s remaining: 17.1s 124: learn: 5.3513967 total: 2.43s remaining: 17s 125: learn: 5.3430297 total: 2.43s remaining: 16.9s 126: learn: 5.3391559 total: 2.44s remaining: 16.8s 127: learn: 5.3298846 total: 2.52s remaining: 17.2s 128: learn: 5.3219533 total: 2.53s remaining: 17.1s 129: learn: 5.3130333 total: 2.53s remaining: 16.9s 130: learn: 5.3040000 total: 2.53s remaining: 16.8s 131: learn: 5.2953496 total: 2.62s remaining: 17.2s 132: learn: 5.2870989 total: 2.62s remaining: 17.1s 133: learn: 5.2821485 total: 2.62s remaining: 17s 134: learn: 5.2767381 total: 2.63s remaining: 16.8s 135: learn: 5.2704406 total: 2.64s remaining: 16.7s 136: learn: 5.2636590 total: 2.72s remaining: 17.1s 137: learn: 5.2580650 total: 2.72s remaining: 17s 138: learn: 5.2531861 total: 2.72s remaining: 16.9s 139: learn: 5.2440501 total: 2.73s remaining: 16.7s 140: learn: 5.2368854 total: 2.73s remaining: 16.7s 141: learn: 5.2303881 total: 2.82s remaining: 17s 142: learn: 5.2236966 total: 2.82s remaining: 16.9s 143: learn: 5.2186475 total: 2.82s remaining: 16.8s 144: learn: 5.2084361 total: 2.82s remaining: 16.6s 145: learn: 5.2009415 total: 2.83s remaining: 16.6s 146: learn: 5.1971680 total: 2.91s remaining: 16.9s 147: learn: 5.1905406 total: 2.92s remaining: 16.8s 148: learn: 5.1867698 total: 2.92s remaining: 16.7s 149: learn: 5.1796599 total: 2.92s remaining: 16.6s 150: learn: 5.1732887 total: 2.93s remaining: 16.5s 151: learn: 5.1686048 total: 3.01s remaining: 16.8s 152: learn: 5.1638773 total: 3.01s remaining: 16.7s 153: learn: 5.1522659 total: 3.02s remaining: 16.6s 154: learn: 5.1464580 total: 3.02s remaining: 16.5s 155: learn: 5.1432412 total: 3.03s remaining: 16.4s 156: learn: 5.1370482 total: 3.11s remaining: 16.7s 157: learn: 5.1330465 total: 3.11s remaining: 16.6s 158: learn: 5.1269448 total: 3.12s remaining: 16.5s 159: learn: 5.1230983 total: 3.12s remaining: 16.4s 160: learn: 5.1167087 total: 3.13s remaining: 16.3s 161: learn: 5.1106685 total: 3.21s remaining: 16.6s 162: learn: 5.1019416 total: 3.21s remaining: 16.5s 163: learn: 5.0979975 total: 3.21s remaining: 16.4s 164: learn: 5.0910596 total: 3.21s remaining: 16.3s 165: learn: 5.0837986 total: 3.22s remaining: 16.2s 166: learn: 5.0786427 total: 3.31s remaining: 16.5s 167: learn: 5.0749346 total: 3.31s remaining: 16.4s 168: learn: 5.0713508 total: 3.31s remaining: 16.3s 169: learn: 5.0626808 total: 3.31s remaining: 16.2s 170: learn: 5.0590199 total: 3.32s remaining: 16.1s 171: learn: 5.0511884 total: 3.4s remaining: 16.4s 172: learn: 5.0436095 total: 3.41s remaining: 16.3s 173: learn: 5.0385636 total: 3.41s remaining: 16.2s 174: learn: 5.0335001 total: 3.41s remaining: 16.1s 175: learn: 5.0285511 total: 3.5s remaining: 16.4s 176: learn: 5.0215091 total: 3.5s remaining: 16.3s 177: learn: 5.0149400 total: 3.5s remaining: 16.2s 178: learn: 5.0094276 total: 3.51s remaining: 16.1s 179: learn: 5.0040345 total: 3.52s remaining: 16s 180: learn: 4.9997440 total: 3.6s remaining: 16.3s 181: learn: 4.9952747 total: 3.6s remaining: 16.2s 182: learn: 4.9913883 total: 3.6s remaining: 16.1s 183: learn: 4.9856592 total: 3.61s remaining: 16s 184: learn: 4.9790543 total: 3.62s remaining: 15.9s 185: learn: 4.9709138 total: 3.7s remaining: 16.2s 186: learn: 4.9665312 total: 3.7s remaining: 16.1s 187: learn: 4.9575560 total: 3.7s remaining: 16s 188: learn: 4.9506248 total: 3.7s remaining: 15.9s 189: learn: 4.9444609 total: 3.79s remaining: 16.2s 190: learn: 4.9418450 total: 3.8s remaining: 16.1s 191: learn: 4.9357947 total: 3.8s remaining: 16s 192: learn: 4.9278965 total: 3.8s remaining: 15.9s 193: learn: 4.9246260 total: 3.8s remaining: 15.8s 194: learn: 4.9220296 total: 3.81s remaining: 15.7s 195: learn: 4.9184070 total: 3.89s remaining: 16s 196: learn: 4.9100968 total: 3.89s remaining: 15.9s 197: learn: 4.9061396 total: 3.9s remaining: 15.8s 198: learn: 4.8992746 total: 3.9s remaining: 15.7s 199: learn: 4.8916824 total: 3.91s remaining: 15.6s 200: learn: 4.8864691 total: 3.99s remaining: 15.9s 201: learn: 4.8801288 total: 3.99s remaining: 15.8s 202: learn: 4.8760833 total: 4s remaining: 15.7s 203: learn: 4.8724722 total: 4s remaining: 15.6s 204: learn: 4.8670718 total: 4.09s remaining: 15.9s 205: learn: 4.8598153 total: 4.09s remaining: 15.8s 206: learn: 4.8515935 total: 4.09s remaining: 15.7s 207: learn: 4.8440654 total: 4.09s remaining: 15.6s 208: learn: 4.8405031 total: 4.1s remaining: 15.5s 209: learn: 4.8374213 total: 4.18s remaining: 15.7s 210: learn: 4.8354563 total: 4.19s remaining: 15.7s 211: learn: 4.8296779 total: 4.19s remaining: 15.6s 212: learn: 4.8236735 total: 4.19s remaining: 15.5s 213: learn: 4.8186799 total: 4.2s remaining: 15.4s 214: learn: 4.8123070 total: 4.28s remaining: 15.6s 215: learn: 4.8107865 total: 4.29s remaining: 15.6s 216: learn: 4.8045297 total: 4.29s remaining: 15.5s 217: learn: 4.7979472 total: 4.29s remaining: 15.4s 218: learn: 4.7867515 total: 4.3s remaining: 15.3s 219: learn: 4.7822940 total: 4.38s remaining: 15.5s 220: learn: 4.7759049 total: 4.38s remaining: 15.5s 221: learn: 4.7717890 total: 4.39s remaining: 15.4s 222: learn: 4.7696340 total: 4.39s remaining: 15.3s 223: learn: 4.7662194 total: 4.4s remaining: 15.2s 224: learn: 4.7623178 total: 4.48s remaining: 15.4s 225: learn: 4.7592261 total: 4.48s remaining: 15.3s 226: learn: 4.7569032 total: 4.48s remaining: 15.3s 227: learn: 4.7538145 total: 4.49s remaining: 15.2s 228: learn: 4.7505458 total: 4.49s remaining: 15.1s 229: learn: 4.7471202 total: 4.58s remaining: 15.3s 230: learn: 4.7427672 total: 4.58s remaining: 15.2s 231: learn: 4.7381377 total: 4.58s remaining: 15.2s 232: learn: 4.7329565 total: 4.58s remaining: 15.1s 233: learn: 4.7301634 total: 4.59s remaining: 15s 234: learn: 4.7276423 total: 4.67s remaining: 15.2s 235: learn: 4.7248806 total: 4.68s remaining: 15.1s 236: learn: 4.7185111 total: 4.68s remaining: 15.1s 237: learn: 4.7131278 total: 4.68s remaining: 15s 238: learn: 4.7085998 total: 4.69s remaining: 14.9s 239: learn: 4.7058274 total: 4.77s remaining: 15.1s 240: learn: 4.6987729 total: 4.78s remaining: 15s 241: learn: 4.6961160 total: 4.78s remaining: 15s 242: learn: 4.6909503 total: 4.78s remaining: 14.9s 243: learn: 4.6871121 total: 4.8s remaining: 14.9s 244: learn: 4.6827706 total: 4.87s remaining: 15s 245: learn: 4.6793887 total: 4.87s remaining: 14.9s 246: learn: 4.6767052 total: 4.88s remaining: 14.9s 247: learn: 4.6713101 total: 4.88s remaining: 14.8s 248: learn: 4.6677946 total: 4.89s remaining: 14.7s 249: learn: 4.6623191 total: 4.96s remaining: 14.9s 250: learn: 4.6588669 total: 4.97s remaining: 14.8s 251: learn: 4.6548923 total: 4.97s remaining: 14.8s 252: learn: 4.6495989 total: 4.97s remaining: 14.7s 253: learn: 4.6431577 total: 4.98s remaining: 14.6s 254: learn: 4.6401480 total: 5.06s remaining: 14.8s 255: learn: 4.6341419 total: 5.07s remaining: 14.7s 256: learn: 4.6278715 total: 5.07s remaining: 14.7s 257: learn: 4.6241677 total: 5.07s remaining: 14.6s 258: learn: 4.6174414 total: 5.08s remaining: 14.5s 259: learn: 4.6127705 total: 5.16s remaining: 14.7s 260: learn: 4.6088329 total: 5.16s remaining: 14.6s 261: learn: 4.6015811 total: 5.17s remaining: 14.6s 262: learn: 4.5993583 total: 5.17s remaining: 14.5s 263: learn: 4.5959346 total: 5.17s remaining: 14.4s 264: learn: 4.5913155 total: 5.26s remaining: 14.6s 265: learn: 4.5875700 total: 5.26s remaining: 14.5s 266: learn: 4.5821308 total: 5.27s remaining: 14.5s 267: learn: 4.5779484 total: 5.27s remaining: 14.4s 268: learn: 4.5741180 total: 5.28s remaining: 14.3s 269: learn: 4.5699804 total: 5.36s remaining: 14.5s 270: learn: 4.5667011 total: 5.36s remaining: 14.4s 271: learn: 4.5605928 total: 5.36s remaining: 14.4s 272: learn: 4.5558893 total: 5.36s remaining: 14.3s 273: learn: 4.5528896 total: 5.37s remaining: 14.2s 274: learn: 4.5472374 total: 5.45s remaining: 14.4s 275: learn: 4.5445202 total: 5.46s remaining: 14.3s 276: learn: 4.5396557 total: 5.46s remaining: 14.2s 277: learn: 4.5358135 total: 5.46s remaining: 14.2s 278: learn: 4.5313459 total: 5.55s remaining: 14.3s 279: learn: 4.5273423 total: 5.55s remaining: 14.3s 280: learn: 4.5206371 total: 5.55s remaining: 14.2s 281: learn: 4.5166285 total: 5.56s remaining: 14.2s 282: learn: 4.5112767 total: 5.56s remaining: 14.1s 283: learn: 4.5073618 total: 5.65s remaining: 14.2s 284: learn: 4.5020713 total: 5.65s remaining: 14.2s 285: learn: 4.4986843 total: 5.65s remaining: 14.1s 286: learn: 4.4944704 total: 5.66s remaining: 14s 287: learn: 4.4887032 total: 5.66s remaining: 14s 288: learn: 4.4838997 total: 5.67s remaining: 13.9s 289: learn: 4.4804188 total: 5.75s remaining: 14.1s 290: learn: 4.4784412 total: 5.75s remaining: 14s 291: learn: 4.4750195 total: 5.75s remaining: 13.9s 292: learn: 4.4712746 total: 5.75s remaining: 13.9s 293: learn: 4.4667650 total: 5.76s remaining: 13.8s 294: learn: 4.4607543 total: 5.84s remaining: 14s 295: learn: 4.4555890 total: 5.85s remaining: 13.9s 296: learn: 4.4535759 total: 5.85s remaining: 13.8s 297: learn: 4.4479889 total: 5.85s remaining: 13.8s 298: learn: 4.4436427 total: 5.86s remaining: 13.7s 299: learn: 4.4404406 total: 5.94s remaining: 13.9s 300: learn: 4.4341531 total: 5.95s remaining: 13.8s 301: learn: 4.4296892 total: 5.95s remaining: 13.7s 302: learn: 4.4252461 total: 5.95s remaining: 13.7s 303: learn: 4.4213359 total: 5.96s remaining: 13.6s 304: learn: 4.4163384 total: 6.04s remaining: 13.8s 305: learn: 4.4128352 total: 6.04s remaining: 13.7s 306: learn: 4.4090996 total: 6.04s remaining: 13.6s 307: learn: 4.4064145 total: 6.05s remaining: 13.6s 308: learn: 4.4049520 total: 6.05s remaining: 13.5s 309: learn: 4.4025284 total: 6.14s remaining: 13.7s 310: learn: 4.4004192 total: 6.14s remaining: 13.6s 311: learn: 4.3989059 total: 6.14s remaining: 13.5s 312: learn: 4.3941600 total: 6.15s remaining: 13.5s 313: learn: 4.3909085 total: 6.16s remaining: 13.5s 314: learn: 4.3884150 total: 6.23s remaining: 13.6s 315: learn: 4.3838816 total: 6.24s remaining: 13.5s 316: learn: 4.3801980 total: 6.24s remaining: 13.4s 317: learn: 4.3747378 total: 6.24s remaining: 13.4s 318: learn: 4.3680260 total: 6.25s remaining: 13.3s 319: learn: 4.3632477 total: 6.33s remaining: 13.5s 320: learn: 4.3625440 total: 6.33s remaining: 13.4s 321: learn: 4.3580639 total: 6.34s remaining: 13.3s 322: learn: 4.3550094 total: 6.34s remaining: 13.3s 323: learn: 4.3514399 total: 6.34s remaining: 13.2s 324: learn: 4.3487689 total: 6.43s remaining: 13.4s 325: learn: 4.3417666 total: 6.43s remaining: 13.3s 326: learn: 4.3383467 total: 6.43s remaining: 13.2s 327: learn: 4.3322835 total: 6.44s remaining: 13.2s 328: learn: 4.3275025 total: 6.44s remaining: 13.1s 329: learn: 4.3234267 total: 6.53s remaining: 13.3s 330: learn: 4.3196456 total: 6.53s remaining: 13.2s 331: learn: 4.3157418 total: 6.53s remaining: 13.1s 332: learn: 4.3116234 total: 6.54s remaining: 13.1s 333: learn: 4.3050365 total: 6.54s remaining: 13s 334: learn: 4.3020212 total: 6.63s remaining: 13.2s 335: learn: 4.2997644 total: 6.63s remaining: 13.1s 336: learn: 4.2956510 total: 6.63s remaining: 13s 337: learn: 4.2923453 total: 6.63s remaining: 13s 338: learn: 4.2893912 total: 6.64s remaining: 12.9s 339: learn: 4.2837924 total: 6.72s remaining: 13.1s 340: learn: 4.2786004 total: 6.72s remaining: 13s 341: learn: 4.2766528 total: 6.73s remaining: 12.9s 342: learn: 4.2748298 total: 6.73s remaining: 12.9s 343: learn: 4.2715049 total: 6.82s remaining: 13s 344: learn: 4.2669223 total: 6.82s remaining: 13s 345: learn: 4.2616741 total: 6.83s remaining: 12.9s 346: learn: 4.2581641 total: 6.83s remaining: 12.9s 347: learn: 4.2539284 total: 6.84s remaining: 12.8s 348: learn: 4.2521872 total: 6.92s remaining: 12.9s 349: learn: 4.2473718 total: 6.92s remaining: 12.9s 350: learn: 4.2450427 total: 6.92s remaining: 12.8s 351: learn: 4.2393681 total: 6.93s remaining: 12.8s 352: learn: 4.2349908 total: 7.02s remaining: 12.9s 353: learn: 4.2316683 total: 7.02s remaining: 12.8s 354: learn: 4.2294995 total: 7.02s remaining: 12.8s 355: learn: 4.2250415 total: 7.02s remaining: 12.7s 356: learn: 4.2217354 total: 7.03s remaining: 12.7s 357: learn: 4.2175437 total: 7.12s remaining: 12.8s 358: learn: 4.2125398 total: 7.12s remaining: 12.7s 359: learn: 4.2097134 total: 7.13s remaining: 12.7s 360: learn: 4.2018761 total: 7.13s remaining: 12.6s 361: learn: 4.1975462 total: 7.21s remaining: 12.7s 362: learn: 4.1942797 total: 7.21s remaining: 12.7s 363: learn: 4.1907273 total: 7.22s remaining: 12.6s 364: learn: 4.1862434 total: 7.22s remaining: 12.6s 365: learn: 4.1839770 total: 7.22s remaining: 12.5s 366: learn: 4.1797920 total: 7.31s remaining: 12.6s 367: learn: 4.1745241 total: 7.31s remaining: 12.6s 368: learn: 4.1712157 total: 7.31s remaining: 12.5s 369: learn: 4.1659542 total: 7.32s remaining: 12.5s 370: learn: 4.1636786 total: 7.32s remaining: 12.4s 371: learn: 4.1596716 total: 7.33s remaining: 12.4s 372: learn: 4.1550049 total: 7.4s remaining: 12.4s 373: learn: 4.1509016 total: 7.41s remaining: 12.4s 374: learn: 4.1450710 total: 7.41s remaining: 12.4s 375: learn: 4.1419712 total: 7.42s remaining: 12.3s 376: learn: 4.1381351 total: 7.5s remaining: 12.4s 377: learn: 4.1349879 total: 7.5s remaining: 12.3s 378: learn: 4.1291376 total: 7.5s remaining: 12.3s 379: learn: 4.1233669 total: 7.51s remaining: 12.3s 380: learn: 4.1198785 total: 7.52s remaining: 12.2s 381: learn: 4.1145537 total: 7.6s remaining: 12.3s 382: learn: 4.1098613 total: 7.6s remaining: 12.2s 383: learn: 4.1067455 total: 7.6s remaining: 12.2s 384: learn: 4.1039428 total: 7.61s remaining: 12.2s 385: learn: 4.1000862 total: 7.69s remaining: 12.2s 386: learn: 4.0957939 total: 7.7s remaining: 12.2s 387: learn: 4.0925956 total: 7.7s remaining: 12.1s 388: learn: 4.0912750 total: 7.7s remaining: 12.1s 389: learn: 4.0883566 total: 7.7s remaining: 12.1s 390: learn: 4.0848798 total: 7.71s remaining: 12s 391: learn: 4.0831652 total: 7.79s remaining: 12.1s 392: learn: 4.0769909 total: 7.8s remaining: 12s 393: learn: 4.0734556 total: 7.8s remaining: 12s 394: learn: 4.0689413 total: 7.8s remaining: 12s 395: learn: 4.0674611 total: 7.89s remaining: 12s 396: learn: 4.0661354 total: 7.89s remaining: 12s 397: learn: 4.0626335 total: 7.9s remaining: 11.9s 398: learn: 4.0581185 total: 7.9s remaining: 11.9s 399: learn: 4.0546182 total: 7.91s remaining: 11.9s 400: learn: 4.0494440 total: 7.99s remaining: 11.9s 401: learn: 4.0467084 total: 7.99s remaining: 11.9s 402: learn: 4.0437444 total: 8s remaining: 11.8s 403: learn: 4.0407245 total: 8s remaining: 11.8s 404: learn: 4.0372642 total: 8s remaining: 11.8s 405: learn: 4.0339581 total: 8.09s remaining: 11.8s 406: learn: 4.0314237 total: 8.09s remaining: 11.8s 407: learn: 4.0291123 total: 8.09s remaining: 11.7s 408: learn: 4.0256872 total: 8.1s remaining: 11.7s 409: learn: 4.0220849 total: 8.1s remaining: 11.7s 410: learn: 4.0188100 total: 8.18s remaining: 11.7s 411: learn: 4.0158882 total: 8.19s remaining: 11.7s 412: learn: 4.0119280 total: 8.19s remaining: 11.6s 413: learn: 4.0080036 total: 8.19s remaining: 11.6s 414: learn: 4.0060991 total: 8.2s remaining: 11.6s 415: learn: 4.0026295 total: 8.28s remaining: 11.6s 416: learn: 4.0007553 total: 8.29s remaining: 11.6s 417: learn: 3.9994706 total: 8.29s remaining: 11.5s 418: learn: 3.9956060 total: 8.29s remaining: 11.5s 419: learn: 3.9886537 total: 8.38s remaining: 11.6s 420: learn: 3.9867591 total: 8.38s remaining: 11.5s 421: learn: 3.9842122 total: 8.38s remaining: 11.5s 422: learn: 3.9811326 total: 8.38s remaining: 11.4s 423: learn: 3.9778463 total: 8.39s remaining: 11.4s 424: learn: 3.9754097 total: 8.39s remaining: 11.4s 425: learn: 3.9721072 total: 8.48s remaining: 11.4s 426: learn: 3.9677454 total: 8.48s remaining: 11.4s 427: learn: 3.9656854 total: 8.48s remaining: 11.3s 428: learn: 3.9628612 total: 8.48s remaining: 11.3s 429: learn: 3.9598889 total: 8.49s remaining: 11.3s 430: learn: 3.9555624 total: 8.57s remaining: 11.3s 431: learn: 3.9527864 total: 8.58s remaining: 11.3s 432: learn: 3.9510360 total: 8.58s remaining: 11.2s 433: learn: 3.9483118 total: 8.58s remaining: 11.2s 434: learn: 3.9459516 total: 8.59s remaining: 11.2s 435: learn: 3.9441995 total: 8.67s remaining: 11.2s 436: learn: 3.9415756 total: 8.67s remaining: 11.2s 437: learn: 3.9366793 total: 8.68s remaining: 11.1s 438: learn: 3.9307614 total: 8.68s remaining: 11.1s 439: learn: 3.9287469 total: 8.69s remaining: 11.1s 440: learn: 3.9250111 total: 8.77s remaining: 11.1s 441: learn: 3.9221957 total: 8.77s remaining: 11.1s 442: learn: 3.9196190 total: 8.78s remaining: 11s 443: learn: 3.9179502 total: 8.78s remaining: 11s 444: learn: 3.9149726 total: 8.78s remaining: 11s 445: learn: 3.9117242 total: 8.87s remaining: 11s 446: learn: 3.9080126 total: 8.87s remaining: 11s 447: learn: 3.9046539 total: 8.87s remaining: 10.9s 448: learn: 3.9029134 total: 8.87s remaining: 10.9s 449: learn: 3.9009247 total: 8.88s remaining: 10.9s 450: learn: 3.8992076 total: 8.96s remaining: 10.9s 451: learn: 3.8957683 total: 8.97s remaining: 10.9s 452: learn: 3.8938480 total: 8.97s remaining: 10.8s 453: learn: 3.8911098 total: 8.97s remaining: 10.8s 454: learn: 3.8902656 total: 8.98s remaining: 10.8s 455: learn: 3.8870731 total: 9.06s remaining: 10.8s 456: learn: 3.8858632 total: 9.06s remaining: 10.8s 457: learn: 3.8845429 total: 9.07s remaining: 10.7s 458: learn: 3.8799293 total: 9.07s remaining: 10.7s 459: learn: 3.8786298 total: 9.08s remaining: 10.7s 460: learn: 3.8771816 total: 9.16s remaining: 10.7s 461: learn: 3.8732192 total: 9.16s remaining: 10.7s 462: learn: 3.8710574 total: 9.16s remaining: 10.6s 463: learn: 3.8689366 total: 9.17s remaining: 10.6s 464: learn: 3.8671273 total: 9.17s remaining: 10.6s 465: learn: 3.8637306 total: 9.26s remaining: 10.6s 466: learn: 3.8598419 total: 9.26s remaining: 10.6s 467: learn: 3.8567917 total: 9.26s remaining: 10.5s 468: learn: 3.8538146 total: 9.27s remaining: 10.5s 469: learn: 3.8515353 total: 9.35s remaining: 10.5s 470: learn: 3.8495452 total: 9.36s remaining: 10.5s 471: learn: 3.8456474 total: 9.36s remaining: 10.5s 472: learn: 3.8428045 total: 9.36s remaining: 10.4s 473: learn: 3.8397443 total: 9.36s remaining: 10.4s 474: learn: 3.8377518 total: 9.37s remaining: 10.4s 475: learn: 3.8349231 total: 9.45s remaining: 10.4s 476: learn: 3.8333713 total: 9.45s remaining: 10.4s 477: learn: 3.8317569 total: 9.46s remaining: 10.3s 478: learn: 3.8300623 total: 9.46s remaining: 10.3s 479: learn: 3.8284024 total: 9.47s remaining: 10.3s 480: learn: 3.8273959 total: 9.55s remaining: 10.3s 481: learn: 3.8252996 total: 9.55s remaining: 10.3s 482: learn: 3.8235241 total: 9.55s remaining: 10.2s 483: learn: 3.8214497 total: 9.56s remaining: 10.2s 484: learn: 3.8202073 total: 9.56s remaining: 10.2s 485: learn: 3.8153884 total: 9.65s remaining: 10.2s 486: learn: 3.8136865 total: 9.65s remaining: 10.2s 487: learn: 3.8104433 total: 9.65s remaining: 10.1s 488: learn: 3.8081962 total: 9.65s remaining: 10.1s 489: learn: 3.8041561 total: 9.66s remaining: 10.1s 490: learn: 3.8032791 total: 9.75s remaining: 10.1s 491: learn: 3.8002779 total: 9.75s remaining: 10.1s 492: learn: 3.7974712 total: 9.75s remaining: 10s 493: learn: 3.7965829 total: 9.75s remaining: 9.99s 494: learn: 3.7935984 total: 9.76s remaining: 9.96s 495: learn: 3.7921962 total: 9.84s remaining: 10s 496: learn: 3.7873762 total: 9.85s remaining: 9.96s 497: learn: 3.7856692 total: 9.85s remaining: 9.93s 498: learn: 3.7849548 total: 9.85s remaining: 9.89s 499: learn: 3.7826048 total: 9.86s remaining: 9.86s 500: learn: 3.7817893 total: 9.94s remaining: 9.9s 501: learn: 3.7798693 total: 9.94s remaining: 9.86s 502: learn: 3.7761611 total: 9.95s remaining: 9.83s 503: learn: 3.7730293 total: 9.95s remaining: 9.79s 504: learn: 3.7714301 total: 9.95s remaining: 9.75s 505: learn: 3.7698978 total: 10s remaining: 9.8s 506: learn: 3.7677057 total: 10s remaining: 9.76s 507: learn: 3.7661161 total: 10s remaining: 9.73s 508: learn: 3.7648347 total: 10s remaining: 9.69s 509: learn: 3.7611307 total: 10.1s remaining: 9.66s 510: learn: 3.7584314 total: 10.1s remaining: 9.7s 511: learn: 3.7567833 total: 10.1s remaining: 9.66s 512: learn: 3.7551805 total: 10.1s remaining: 9.63s 513: learn: 3.7522163 total: 10.1s remaining: 9.59s 514: learn: 3.7504819 total: 10.1s remaining: 9.55s 515: learn: 3.7483574 total: 10.2s remaining: 9.6s 516: learn: 3.7452892 total: 10.2s remaining: 9.56s 517: learn: 3.7412042 total: 10.2s remaining: 9.53s 518: learn: 3.7392695 total: 10.2s remaining: 9.49s 519: learn: 3.7378030 total: 10.2s remaining: 9.46s 520: learn: 3.7363342 total: 10.3s remaining: 9.5s 521: learn: 3.7345634 total: 10.3s remaining: 9.46s 522: learn: 3.7321662 total: 10.3s remaining: 9.43s 523: learn: 3.7311070 total: 10.3s remaining: 9.39s 524: learn: 3.7296406 total: 10.3s remaining: 9.36s 525: learn: 3.7284312 total: 10.3s remaining: 9.32s 526: learn: 3.7249652 total: 10.4s remaining: 9.36s 527: learn: 3.7242044 total: 10.4s remaining: 9.32s 528: learn: 3.7235733 total: 10.4s remaining: 9.29s 529: learn: 3.7195998 total: 10.4s remaining: 9.26s 530: learn: 3.7186223 total: 10.4s remaining: 9.22s 531: learn: 3.7172148 total: 10.5s remaining: 9.26s 532: learn: 3.7150550 total: 10.5s remaining: 9.22s 533: learn: 3.7136453 total: 10.5s remaining: 9.19s 534: learn: 3.7108118 total: 10.5s remaining: 9.16s 535: learn: 3.7093073 total: 10.5s remaining: 9.13s 536: learn: 3.7082805 total: 10.6s remaining: 9.16s 537: learn: 3.7060521 total: 10.6s remaining: 9.13s 538: learn: 3.7049623 total: 10.6s remaining: 9.09s 539: learn: 3.7018914 total: 10.6s remaining: 9.06s 540: learn: 3.7007568 total: 10.6s remaining: 9.02s 541: learn: 3.6986173 total: 10.7s remaining: 9.06s 542: learn: 3.6960773 total: 10.7s remaining: 9.03s 543: learn: 3.6935799 total: 10.7s remaining: 8.99s 544: learn: 3.6900813 total: 10.7s remaining: 8.96s 545: learn: 3.6889124 total: 10.7s remaining: 8.92s 546: learn: 3.6865814 total: 10.8s remaining: 8.96s 547: learn: 3.6850859 total: 10.8s remaining: 8.93s 548: learn: 3.6816916 total: 10.8s remaining: 8.89s 549: learn: 3.6792062 total: 10.8s remaining: 8.86s 550: learn: 3.6764268 total: 10.8s remaining: 8.82s 551: learn: 3.6732831 total: 10.9s remaining: 8.86s 552: learn: 3.6722013 total: 10.9s remaining: 8.82s 553: learn: 3.6713029 total: 10.9s remaining: 8.79s 554: learn: 3.6698550 total: 10.9s remaining: 8.76s 555: learn: 3.6663527 total: 10.9s remaining: 8.72s 556: learn: 3.6651872 total: 10.9s remaining: 8.7s 557: learn: 3.6606142 total: 11s remaining: 8.72s 558: learn: 3.6590152 total: 11s remaining: 8.69s 559: learn: 3.6567410 total: 11s remaining: 8.66s 560: learn: 3.6540511 total: 11s remaining: 8.63s 561: learn: 3.6525069 total: 11.1s remaining: 8.66s 562: learn: 3.6491564 total: 11.1s remaining: 8.63s 563: learn: 3.6476297 total: 11.1s remaining: 8.59s 564: learn: 3.6465462 total: 11.1s remaining: 8.56s 565: learn: 3.6448051 total: 11.1s remaining: 8.53s 566: learn: 3.6415337 total: 11.1s remaining: 8.5s 567: learn: 3.6384531 total: 11.2s remaining: 8.53s 568: learn: 3.6362624 total: 11.2s remaining: 8.49s 569: learn: 3.6346312 total: 11.2s remaining: 8.46s 570: learn: 3.6325152 total: 11.2s remaining: 8.43s 571: learn: 3.6309543 total: 11.2s remaining: 8.4s 572: learn: 3.6304172 total: 11.3s remaining: 8.43s 573: learn: 3.6282196 total: 11.3s remaining: 8.39s 574: learn: 3.6266656 total: 11.3s remaining: 8.36s 575: learn: 3.6246413 total: 11.3s remaining: 8.33s 576: learn: 3.6218372 total: 11.4s remaining: 8.36s 577: learn: 3.6199659 total: 11.4s remaining: 8.33s 578: learn: 3.6176220 total: 11.4s remaining: 8.3s 579: learn: 3.6164343 total: 11.4s remaining: 8.26s 580: learn: 3.6144863 total: 11.4s remaining: 8.23s 581: learn: 3.6133771 total: 11.5s remaining: 8.26s 582: learn: 3.6117889 total: 11.5s remaining: 8.23s 583: learn: 3.6106600 total: 11.5s remaining: 8.2s 584: learn: 3.6087033 total: 11.5s remaining: 8.16s 585: learn: 3.6060387 total: 11.5s remaining: 8.13s 586: learn: 3.6042227 total: 11.5s remaining: 8.1s 587: learn: 3.6028092 total: 11.6s remaining: 8.13s 588: learn: 3.6011954 total: 11.6s remaining: 8.1s 589: learn: 3.5999982 total: 11.6s remaining: 8.06s 590: learn: 3.5997258 total: 11.6s remaining: 8.03s 591: learn: 3.5967138 total: 11.6s remaining: 8s 592: learn: 3.5958755 total: 11.7s remaining: 8.03s 593: learn: 3.5934605 total: 11.7s remaining: 8s 594: learn: 3.5925477 total: 11.7s remaining: 7.97s 595: learn: 3.5919440 total: 11.7s remaining: 7.93s 596: learn: 3.5900084 total: 11.7s remaining: 7.91s 597: learn: 3.5892290 total: 11.8s remaining: 7.93s 598: learn: 3.5877425 total: 11.8s remaining: 7.9s 599: learn: 3.5844309 total: 11.8s remaining: 7.87s 600: learn: 3.5810486 total: 11.8s remaining: 7.84s 601: learn: 3.5773400 total: 11.9s remaining: 7.86s 602: learn: 3.5760316 total: 11.9s remaining: 7.83s 603: learn: 3.5737789 total: 11.9s remaining: 7.8s 604: learn: 3.5727233 total: 11.9s remaining: 7.77s 605: learn: 3.5715228 total: 11.9s remaining: 7.75s 606: learn: 3.5688508 total: 12s remaining: 7.76s 607: learn: 3.5660763 total: 12s remaining: 7.73s 608: learn: 3.5637492 total: 12s remaining: 7.7s 609: learn: 3.5628672 total: 12s remaining: 7.67s 610: learn: 3.5598081 total: 12s remaining: 7.64s 611: learn: 3.5584112 total: 12.1s remaining: 7.67s 612: learn: 3.5559394 total: 12.1s remaining: 7.64s 613: learn: 3.5552895 total: 12.1s remaining: 7.61s 614: learn: 3.5531618 total: 12.1s remaining: 7.58s 615: learn: 3.5503725 total: 12.2s remaining: 7.59s 616: learn: 3.5482991 total: 12.2s remaining: 7.57s 617: learn: 3.5469162 total: 12.2s remaining: 7.54s 618: learn: 3.5461291 total: 12.2s remaining: 7.5s 619: learn: 3.5453465 total: 12.2s remaining: 7.47s 620: learn: 3.5437542 total: 12.3s remaining: 7.49s 621: learn: 3.5414621 total: 12.3s remaining: 7.46s 622: learn: 3.5400041 total: 12.3s remaining: 7.43s 623: learn: 3.5388662 total: 12.3s remaining: 7.41s 624: learn: 3.5373892 total: 12.3s remaining: 7.38s 625: learn: 3.5358684 total: 12.4s remaining: 7.39s 626: learn: 3.5331817 total: 12.4s remaining: 7.37s 627: learn: 3.5317113 total: 12.4s remaining: 7.33s 628: learn: 3.5314592 total: 12.4s remaining: 7.3s 629: learn: 3.5297615 total: 12.4s remaining: 7.28s 630: learn: 3.5281356 total: 12.5s remaining: 7.29s 631: learn: 3.5260987 total: 12.5s remaining: 7.26s 632: learn: 3.5250746 total: 12.5s remaining: 7.24s 633: learn: 3.5237136 total: 12.5s remaining: 7.21s 634: learn: 3.5221614 total: 12.5s remaining: 7.18s 635: learn: 3.5209886 total: 12.6s remaining: 7.19s 636: learn: 3.5204814 total: 12.6s remaining: 7.16s 637: learn: 3.5193343 total: 12.6s remaining: 7.13s 638: learn: 3.5182138 total: 12.6s remaining: 7.11s 639: learn: 3.5164131 total: 12.6s remaining: 7.08s 640: learn: 3.5155470 total: 12.7s remaining: 7.09s 641: learn: 3.5142737 total: 12.7s remaining: 7.06s 642: learn: 3.5131941 total: 12.7s remaining: 7.04s 643: learn: 3.5114180 total: 12.7s remaining: 7.01s 644: learn: 3.5104208 total: 12.7s remaining: 6.98s 645: learn: 3.5092384 total: 12.8s remaining: 6.99s 646: learn: 3.5082901 total: 12.8s remaining: 6.96s 647: learn: 3.5071604 total: 12.8s remaining: 6.93s 648: learn: 3.5050310 total: 12.8s remaining: 6.91s 649: learn: 3.5038809 total: 12.8s remaining: 6.88s 650: learn: 3.5016077 total: 12.9s remaining: 6.89s 651: learn: 3.4987192 total: 12.9s remaining: 6.87s 652: learn: 3.4977137 total: 12.9s remaining: 6.84s 653: learn: 3.4955634 total: 12.9s remaining: 6.81s 654: learn: 3.4933200 total: 12.9s remaining: 6.78s 655: learn: 3.4915519 total: 13s remaining: 6.79s 656: learn: 3.4889138 total: 13s remaining: 6.77s 657: learn: 3.4864485 total: 13s remaining: 6.74s 658: learn: 3.4852677 total: 13s remaining: 6.71s 659: learn: 3.4837828 total: 13.1s remaining: 6.72s 660: learn: 3.4829241 total: 13.1s remaining: 6.7s 661: learn: 3.4820559 total: 13.1s remaining: 6.67s 662: learn: 3.4805210 total: 13.1s remaining: 6.64s 663: learn: 3.4787329 total: 13.1s remaining: 6.61s 664: learn: 3.4772271 total: 13.2s remaining: 6.63s 665: learn: 3.4761143 total: 13.2s remaining: 6.6s 666: learn: 3.4740960 total: 13.2s remaining: 6.57s 667: learn: 3.4716803 total: 13.2s remaining: 6.54s 668: learn: 3.4701737 total: 13.2s remaining: 6.51s 669: learn: 3.4699194 total: 13.3s remaining: 6.53s 670: learn: 3.4685933 total: 13.3s remaining: 6.5s 671: learn: 3.4669390 total: 13.3s remaining: 6.47s 672: learn: 3.4638777 total: 13.3s remaining: 6.44s 673: learn: 3.4631283 total: 13.3s remaining: 6.42s 674: learn: 3.4611957 total: 13.3s remaining: 6.43s 675: learn: 3.4606208 total: 13.4s remaining: 6.4s 676: learn: 3.4584662 total: 13.4s remaining: 6.37s 677: learn: 3.4577831 total: 13.4s remaining: 6.34s 678: learn: 3.4557525 total: 13.4s remaining: 6.32s 679: learn: 3.4539224 total: 13.4s remaining: 6.33s 680: learn: 3.4509009 total: 13.4s remaining: 6.3s 681: learn: 3.4497278 total: 13.5s remaining: 6.27s 682: learn: 3.4479337 total: 13.5s remaining: 6.24s 683: learn: 3.4455338 total: 13.5s remaining: 6.22s 684: learn: 3.4449417 total: 13.5s remaining: 6.23s 685: learn: 3.4444453 total: 13.5s remaining: 6.2s 686: learn: 3.4427153 total: 13.6s remaining: 6.17s 687: learn: 3.4406000 total: 13.6s remaining: 6.14s 688: learn: 3.4392924 total: 13.6s remaining: 6.12s 689: learn: 3.4382639 total: 13.6s remaining: 6.13s 690: learn: 3.4374775 total: 13.6s remaining: 6.1s 691: learn: 3.4364144 total: 13.6s remaining: 6.07s 692: learn: 3.4354031 total: 13.7s remaining: 6.05s 693: learn: 3.4348887 total: 13.7s remaining: 6.02s 694: learn: 3.4332960 total: 13.7s remaining: 6.03s 695: learn: 3.4329480 total: 13.7s remaining: 6s 696: learn: 3.4318210 total: 13.7s remaining: 5.98s 697: learn: 3.4300438 total: 13.7s remaining: 5.95s 698: learn: 3.4289454 total: 13.8s remaining: 5.96s 699: learn: 3.4276542 total: 13.8s remaining: 5.93s 700: learn: 3.4262781 total: 13.8s remaining: 5.9s 701: learn: 3.4244482 total: 13.8s remaining: 5.88s 702: learn: 3.4238634 total: 13.8s remaining: 5.85s 703: learn: 3.4231684 total: 13.9s remaining: 5.86s 704: learn: 3.4222185 total: 13.9s remaining: 5.83s 705: learn: 3.4214530 total: 13.9s remaining: 5.8s 706: learn: 3.4194276 total: 13.9s remaining: 5.78s 707: learn: 3.4184285 total: 14s remaining: 5.75s 708: learn: 3.4181808 total: 14s remaining: 5.76s 709: learn: 3.4158372 total: 14s remaining: 5.73s 710: learn: 3.4129137 total: 14s remaining: 5.71s 711: learn: 3.4112063 total: 14s remaining: 5.68s 712: learn: 3.4104808 total: 14.1s remaining: 5.66s 713: learn: 3.4092005 total: 14.1s remaining: 5.66s 714: learn: 3.4082539 total: 14.1s remaining: 5.63s 715: learn: 3.4061717 total: 14.1s remaining: 5.61s 716: learn: 3.4035046 total: 14.1s remaining: 5.58s 717: learn: 3.4014236 total: 14.1s remaining: 5.56s 718: learn: 3.4006329 total: 14.2s remaining: 5.56s 719: learn: 3.3995283 total: 14.2s remaining: 5.54s 720: learn: 3.3978196 total: 14.2s remaining: 5.51s 721: learn: 3.3960545 total: 14.2s remaining: 5.48s 722: learn: 3.3935368 total: 14.3s remaining: 5.49s 723: learn: 3.3911667 total: 14.3s remaining: 5.46s 724: learn: 3.3891393 total: 14.3s remaining: 5.44s 725: learn: 3.3871146 total: 14.3s remaining: 5.41s 726: learn: 3.3858376 total: 14.3s remaining: 5.38s 727: learn: 3.3850437 total: 14.3s remaining: 5.36s 728: learn: 3.3841250 total: 14.4s remaining: 5.36s 729: learn: 3.3835264 total: 14.4s remaining: 5.34s 730: learn: 3.3826066 total: 14.4s remaining: 5.31s 731: learn: 3.3811678 total: 14.4s remaining: 5.28s 732: learn: 3.3777990 total: 14.4s remaining: 5.26s 733: learn: 3.3763171 total: 14.5s remaining: 5.26s 734: learn: 3.3750248 total: 14.5s remaining: 5.24s 735: learn: 3.3746756 total: 14.5s remaining: 5.21s 736: learn: 3.3733322 total: 14.5s remaining: 5.18s 737: learn: 3.3715282 total: 14.5s remaining: 5.16s 738: learn: 3.3708568 total: 14.6s remaining: 5.16s 739: learn: 3.3699546 total: 14.6s remaining: 5.14s 740: learn: 3.3690532 total: 14.6s remaining: 5.11s 741: learn: 3.3668951 total: 14.6s remaining: 5.09s 742: learn: 3.3647964 total: 14.6s remaining: 5.06s 743: learn: 3.3618177 total: 14.7s remaining: 5.06s 744: learn: 3.3602851 total: 14.7s remaining: 5.04s 745: learn: 3.3587076 total: 14.7s remaining: 5.01s 746: learn: 3.3576963 total: 14.7s remaining: 4.99s 747: learn: 3.3573048 total: 14.7s remaining: 4.96s 748: learn: 3.3562879 total: 14.7s remaining: 4.94s 749: learn: 3.3552186 total: 14.8s remaining: 4.94s 750: learn: 3.3538874 total: 14.8s remaining: 4.91s 751: learn: 3.3524014 total: 14.8s remaining: 4.89s 752: learn: 3.3508676 total: 14.8s remaining: 4.86s 753: learn: 3.3494016 total: 14.9s remaining: 4.87s 754: learn: 3.3475248 total: 14.9s remaining: 4.84s 755: learn: 3.3467518 total: 14.9s remaining: 4.81s 756: learn: 3.3460847 total: 14.9s remaining: 4.79s 757: learn: 3.3446721 total: 14.9s remaining: 4.76s 758: learn: 3.3441657 total: 15s remaining: 4.77s 759: learn: 3.3422403 total: 15s remaining: 4.74s 760: learn: 3.3408967 total: 15s remaining: 4.71s 761: learn: 3.3392488 total: 15s remaining: 4.69s 762: learn: 3.3381024 total: 15s remaining: 4.67s 763: learn: 3.3377391 total: 15.1s remaining: 4.67s 764: learn: 3.3356044 total: 15.1s remaining: 4.64s 765: learn: 3.3349119 total: 15.1s remaining: 4.62s 766: learn: 3.3334900 total: 15.1s remaining: 4.59s 767: learn: 3.3316682 total: 15.1s remaining: 4.57s 768: learn: 3.3308483 total: 15.2s remaining: 4.57s 769: learn: 3.3295313 total: 15.2s remaining: 4.54s 770: learn: 3.3285986 total: 15.2s remaining: 4.52s 771: learn: 3.3270096 total: 15.2s remaining: 4.49s 772: learn: 3.3257082 total: 15.2s remaining: 4.47s 773: learn: 3.3249572 total: 15.3s remaining: 4.47s 774: learn: 3.3242294 total: 15.3s remaining: 4.44s 775: learn: 3.3232504 total: 15.3s remaining: 4.42s 776: learn: 3.3224960 total: 15.3s remaining: 4.39s 777: learn: 3.3210173 total: 15.3s remaining: 4.37s 778: learn: 3.3197188 total: 15.4s remaining: 4.37s 779: learn: 3.3191239 total: 15.4s remaining: 4.34s 780: learn: 3.3188810 total: 15.4s remaining: 4.32s 781: learn: 3.3175516 total: 15.4s remaining: 4.29s 782: learn: 3.3161376 total: 15.4s remaining: 4.27s 783: learn: 3.3157570 total: 15.5s remaining: 4.27s 784: learn: 3.3149482 total: 15.5s remaining: 4.25s 785: learn: 3.3147086 total: 15.5s remaining: 4.22s 786: learn: 3.3144920 total: 15.5s remaining: 4.2s 787: learn: 3.3137537 total: 15.5s remaining: 4.17s 788: learn: 3.3134988 total: 15.6s remaining: 4.17s 789: learn: 3.3122346 total: 15.6s remaining: 4.15s 790: learn: 3.3120463 total: 15.6s remaining: 4.12s 791: learn: 3.3114500 total: 15.6s remaining: 4.1s 792: learn: 3.3108517 total: 15.6s remaining: 4.07s 793: learn: 3.3093168 total: 15.7s remaining: 4.07s 794: learn: 3.3074970 total: 15.7s remaining: 4.05s 795: learn: 3.3058097 total: 15.7s remaining: 4.02s 796: learn: 3.3035428 total: 15.7s remaining: 4s 797: learn: 3.3023733 total: 15.7s remaining: 3.98s 798: learn: 3.3009091 total: 15.8s remaining: 3.97s 799: learn: 3.2996240 total: 15.8s remaining: 3.95s 800: learn: 3.2991118 total: 15.8s remaining: 3.92s 801: learn: 3.2980528 total: 15.8s remaining: 3.9s 802: learn: 3.2975762 total: 15.8s remaining: 3.88s 803: learn: 3.2947891 total: 15.9s remaining: 3.87s 804: learn: 3.2935298 total: 15.9s remaining: 3.85s 805: learn: 3.2919772 total: 15.9s remaining: 3.83s 806: learn: 3.2894361 total: 15.9s remaining: 3.8s 807: learn: 3.2879214 total: 15.9s remaining: 3.78s 808: learn: 3.2864700 total: 16s remaining: 3.77s 809: learn: 3.2847087 total: 16s remaining: 3.75s 810: learn: 3.2835180 total: 16s remaining: 3.73s 811: learn: 3.2826170 total: 16s remaining: 3.7s 812: learn: 3.2780154 total: 16s remaining: 3.68s 813: learn: 3.2776817 total: 16.1s remaining: 3.67s 814: learn: 3.2763416 total: 16.1s remaining: 3.65s 815: learn: 3.2731236 total: 16.1s remaining: 3.63s 816: learn: 3.2720981 total: 16.1s remaining: 3.6s 817: learn: 3.2714399 total: 16.1s remaining: 3.58s 818: learn: 3.2698429 total: 16.2s remaining: 3.58s 819: learn: 3.2686656 total: 16.2s remaining: 3.55s 820: learn: 3.2675157 total: 16.2s remaining: 3.53s 821: learn: 3.2663124 total: 16.2s remaining: 3.51s 822: learn: 3.2650838 total: 16.2s remaining: 3.48s 823: learn: 3.2644593 total: 16.3s remaining: 3.48s 824: learn: 3.2633756 total: 16.3s remaining: 3.45s 825: learn: 3.2621100 total: 16.3s remaining: 3.43s 826: learn: 3.2611301 total: 16.3s remaining: 3.41s 827: learn: 3.2604969 total: 16.3s remaining: 3.38s 828: learn: 3.2598528 total: 16.3s remaining: 3.36s 829: learn: 3.2585778 total: 16.4s remaining: 3.35s 830: learn: 3.2583633 total: 16.4s remaining: 3.33s 831: learn: 3.2568287 total: 16.4s remaining: 3.31s 832: learn: 3.2551263 total: 16.4s remaining: 3.28s 833: learn: 3.2536031 total: 16.5s remaining: 3.28s 834: learn: 3.2528220 total: 16.5s remaining: 3.25s 835: learn: 3.2526506 total: 16.5s remaining: 3.23s 836: learn: 3.2518332 total: 16.5s remaining: 3.21s 837: learn: 3.2502660 total: 16.5s remaining: 3.19s 838: learn: 3.2488832 total: 16.6s remaining: 3.18s 839: learn: 3.2474279 total: 16.6s remaining: 3.16s 840: learn: 3.2466913 total: 16.6s remaining: 3.13s 841: learn: 3.2452360 total: 16.6s remaining: 3.11s 842: learn: 3.2439861 total: 16.6s remaining: 3.09s 843: learn: 3.2429466 total: 16.7s remaining: 3.08s 844: learn: 3.2422359 total: 16.7s remaining: 3.06s 845: learn: 3.2411155 total: 16.7s remaining: 3.04s 846: learn: 3.2397654 total: 16.7s remaining: 3.01s 847: learn: 3.2389070 total: 16.7s remaining: 2.99s 848: learn: 3.2383056 total: 16.8s remaining: 2.98s 849: learn: 3.2374602 total: 16.8s remaining: 2.96s 850: learn: 3.2354166 total: 16.8s remaining: 2.94s 851: learn: 3.2350600 total: 16.8s remaining: 2.91s 852: learn: 3.2332189 total: 16.8s remaining: 2.89s 853: learn: 3.2329853 total: 16.9s remaining: 2.88s 854: learn: 3.2325272 total: 16.9s remaining: 2.86s 855: learn: 3.2315728 total: 16.9s remaining: 2.84s 856: learn: 3.2311266 total: 16.9s remaining: 2.81s 857: learn: 3.2294944 total: 16.9s remaining: 2.79s 858: learn: 3.2280008 total: 17s remaining: 2.78s 859: learn: 3.2275044 total: 17s remaining: 2.76s 860: learn: 3.2272417 total: 17s remaining: 2.74s 861: learn: 3.2267300 total: 17s remaining: 2.72s 862: learn: 3.2256492 total: 17s remaining: 2.69s 863: learn: 3.2232784 total: 17.1s remaining: 2.69s 864: learn: 3.2228192 total: 17.1s remaining: 2.66s 865: learn: 3.2219042 total: 17.1s remaining: 2.64s 866: learn: 3.2212497 total: 17.1s remaining: 2.62s 867: learn: 3.2206534 total: 17.1s remaining: 2.6s 868: learn: 3.2194853 total: 17.2s remaining: 2.59s 869: learn: 3.2176513 total: 17.2s remaining: 2.56s 870: learn: 3.2168033 total: 17.2s remaining: 2.54s 871: learn: 3.2157532 total: 17.2s remaining: 2.52s 872: learn: 3.2151843 total: 17.2s remaining: 2.5s 873: learn: 3.2144320 total: 17.3s remaining: 2.49s 874: learn: 3.2141366 total: 17.3s remaining: 2.46s 875: learn: 3.2136375 total: 17.3s remaining: 2.44s 876: learn: 3.2130221 total: 17.3s remaining: 2.42s 877: learn: 3.2114371 total: 17.3s remaining: 2.4s 878: learn: 3.2105926 total: 17.4s remaining: 2.39s 879: learn: 3.2095392 total: 17.4s remaining: 2.37s 880: learn: 3.2082689 total: 17.4s remaining: 2.34s 881: learn: 3.2068575 total: 17.4s remaining: 2.32s 882: learn: 3.2056287 total: 17.4s remaining: 2.3s 883: learn: 3.2038353 total: 17.4s remaining: 2.29s 884: learn: 3.2033481 total: 17.5s remaining: 2.27s 885: learn: 3.2027182 total: 17.5s remaining: 2.25s 886: learn: 3.2015936 total: 17.5s remaining: 2.22s 887: learn: 3.1999213 total: 17.5s remaining: 2.2s 888: learn: 3.1996003 total: 17.5s remaining: 2.19s 889: learn: 3.1987540 total: 17.5s remaining: 2.17s 890: learn: 3.1984901 total: 17.6s remaining: 2.15s 891: learn: 3.1979027 total: 17.6s remaining: 2.13s 892: learn: 3.1976357 total: 17.6s remaining: 2.1s 893: learn: 3.1968788 total: 17.6s remaining: 2.09s 894: learn: 3.1960936 total: 17.6s remaining: 2.07s 895: learn: 3.1953993 total: 17.6s remaining: 2.05s 896: learn: 3.1947000 total: 17.7s remaining: 2.03s 897: learn: 3.1940194 total: 17.7s remaining: 2s 898: learn: 3.1937301 total: 17.7s remaining: 1.99s 899: learn: 3.1917774 total: 17.7s remaining: 1.97s 900: learn: 3.1911578 total: 17.7s remaining: 1.95s 901: learn: 3.1908421 total: 17.7s remaining: 1.93s 902: learn: 3.1903207 total: 17.8s remaining: 1.91s 903: learn: 3.1899629 total: 17.8s remaining: 1.89s 904: learn: 3.1886532 total: 17.8s remaining: 1.87s 905: learn: 3.1881580 total: 17.8s remaining: 1.85s 906: learn: 3.1871411 total: 17.8s remaining: 1.83s 907: learn: 3.1861274 total: 17.9s remaining: 1.81s 908: learn: 3.1855236 total: 17.9s remaining: 1.79s 909: learn: 3.1839121 total: 17.9s remaining: 1.77s 910: learn: 3.1831046 total: 17.9s remaining: 1.75s 911: learn: 3.1821596 total: 17.9s remaining: 1.73s 912: learn: 3.1807786 total: 17.9s remaining: 1.71s 913: learn: 3.1796501 total: 18s remaining: 1.7s 914: learn: 3.1789270 total: 18s remaining: 1.68s 915: learn: 3.1786925 total: 18s remaining: 1.65s 916: learn: 3.1765193 total: 18s remaining: 1.63s 917: learn: 3.1763683 total: 18.1s remaining: 1.62s 918: learn: 3.1755257 total: 18.1s remaining: 1.6s 919: learn: 3.1746936 total: 18.1s remaining: 1.58s 920: learn: 3.1727213 total: 18.1s remaining: 1.55s 921: learn: 3.1711244 total: 18.1s remaining: 1.53s 922: learn: 3.1692580 total: 18.2s remaining: 1.52s 923: learn: 3.1682127 total: 18.2s remaining: 1.5s 924: learn: 3.1655832 total: 18.2s remaining: 1.48s 925: learn: 3.1647554 total: 18.2s remaining: 1.46s 926: learn: 3.1630771 total: 18.2s remaining: 1.44s 927: learn: 3.1623723 total: 18.3s remaining: 1.42s 928: learn: 3.1617142 total: 18.4s remaining: 1.4s 929: learn: 3.1601876 total: 18.4s remaining: 1.38s 930: learn: 3.1593342 total: 18.4s remaining: 1.36s 931: learn: 3.1591008 total: 18.4s remaining: 1.34s 932: learn: 3.1581037 total: 18.4s remaining: 1.32s 933: learn: 3.1568661 total: 18.4s remaining: 1.3s 934: learn: 3.1558634 total: 18.4s remaining: 1.28s 935: learn: 3.1539188 total: 18.5s remaining: 1.27s 936: learn: 3.1518152 total: 18.5s remaining: 1.25s 937: learn: 3.1506616 total: 18.5s remaining: 1.22s 938: learn: 3.1489909 total: 18.5s remaining: 1.2s 939: learn: 3.1482410 total: 18.5s remaining: 1.18s 940: learn: 3.1470818 total: 18.6s remaining: 1.17s 941: learn: 3.1461282 total: 18.6s remaining: 1.15s 942: learn: 3.1456470 total: 18.6s remaining: 1.13s 943: learn: 3.1450137 total: 18.6s remaining: 1.1s 944: learn: 3.1429732 total: 18.6s remaining: 1.08s 945: learn: 3.1421996 total: 18.7s remaining: 1.07s 946: learn: 3.1412305 total: 18.7s remaining: 1.05s 947: learn: 3.1404965 total: 18.7s remaining: 1.03s 948: learn: 3.1398505 total: 18.7s remaining: 1.01s 949: learn: 3.1379280 total: 18.7s remaining: 986ms 950: learn: 3.1368665 total: 18.8s remaining: 969ms 951: learn: 3.1357422 total: 18.8s remaining: 949ms 952: learn: 3.1350627 total: 18.8s remaining: 928ms 953: learn: 3.1333398 total: 18.8s remaining: 908ms 954: learn: 3.1317962 total: 18.8s remaining: 887ms 955: learn: 3.1313479 total: 18.9s remaining: 870ms 956: learn: 3.1309767 total: 18.9s remaining: 850ms 957: learn: 3.1300771 total: 18.9s remaining: 829ms 958: learn: 3.1290127 total: 18.9s remaining: 809ms 959: learn: 3.1286735 total: 18.9s remaining: 789ms 960: learn: 3.1272208 total: 19s remaining: 772ms 961: learn: 3.1262658 total: 19s remaining: 751ms 962: learn: 3.1243670 total: 19s remaining: 731ms 963: learn: 3.1239742 total: 19s remaining: 710ms 964: learn: 3.1217433 total: 19s remaining: 690ms 965: learn: 3.1215300 total: 19.1s remaining: 673ms 966: learn: 3.1198694 total: 19.1s remaining: 652ms 967: learn: 3.1185202 total: 19.1s remaining: 632ms 968: learn: 3.1172165 total: 19.1s remaining: 612ms 969: learn: 3.1157237 total: 19.1s remaining: 591ms 970: learn: 3.1148788 total: 19.2s remaining: 574ms 971: learn: 3.1142763 total: 19.2s remaining: 553ms 972: learn: 3.1122783 total: 19.2s remaining: 533ms 973: learn: 3.1114193 total: 19.2s remaining: 513ms 974: learn: 3.1100147 total: 19.2s remaining: 493ms 975: learn: 3.1093744 total: 19.3s remaining: 475ms 976: learn: 3.1092969 total: 19.3s remaining: 455ms 977: learn: 3.1085145 total: 19.3s remaining: 434ms 978: learn: 3.1079519 total: 19.3s remaining: 414ms 979: learn: 3.1057407 total: 19.4s remaining: 396ms 980: learn: 3.1053785 total: 19.4s remaining: 376ms 981: learn: 3.1039714 total: 19.4s remaining: 356ms 982: learn: 3.1028927 total: 19.4s remaining: 336ms 983: learn: 3.1014381 total: 19.4s remaining: 316ms 984: learn: 3.1004328 total: 19.4s remaining: 296ms 985: learn: 3.1002543 total: 19.5s remaining: 277ms 986: learn: 3.0993387 total: 19.5s remaining: 257ms 987: learn: 3.0989281 total: 19.5s remaining: 237ms 988: learn: 3.0986195 total: 19.5s remaining: 217ms 989: learn: 3.0982980 total: 19.5s remaining: 197ms 990: learn: 3.0971505 total: 19.6s remaining: 178ms 991: learn: 3.0966882 total: 19.6s remaining: 158ms 992: learn: 3.0965667 total: 19.6s remaining: 138ms 993: learn: 3.0951912 total: 19.6s remaining: 118ms 994: learn: 3.0945439 total: 19.6s remaining: 98.5ms 995: learn: 3.0938761 total: 19.7s remaining: 79.1ms 996: learn: 3.0923819 total: 19.7s remaining: 59.3ms 997: learn: 3.0921748 total: 19.7s remaining: 39.5ms 998: learn: 3.0913114 total: 19.7s remaining: 19.7ms 999: learn: 3.0900058 total: 19.7s remaining: 0us . . Grid Search offered worse results over default settings | The default config trains in less than 1/100th of the time compared to running the grid search method | . print(&#39;CatBoostRegressor MAE:&#39;, MAE(target_test, pred_2)) . CatBoostRegressor MAE: 5.221709765858424 . Model 3 - Ridge . model_3 = Ridge(random_state=54321) . param_grid = {&#39;alpha&#39; : [0.001, 0.01, 0.1, 1, 10, 100, 1000]} grid = GridSearchCV(estimator=model_3, param_grid=param_grid, scoring=&#39;neg_mean_absolute_error&#39;, cv=4, verbose=1, n_jobs=-1) grid.fit(features_train, target_train) model_3.set_params(**grid.best_params_) model_3.fit(features_train, target_train) pred_3 = model_3.predict(features_test) . Fitting 4 folds for each of 7 candidates, totalling 28 fits . [Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers. [Parallel(n_jobs=-1)]: Done 28 out of 28 | elapsed: 1.9s finished /opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal. DeprecationWarning) . . print(&#39;Ridge MAE:&#39;, MAE(target_test, pred_3)) . Ridge MAE: 5.98106353055716 . Conclusion . Linear Regression performed great compared to our sanity test model. | CatBoost trained quickly and gave us amazing results. Initially I ran a grid search for catboost but this produced worse results and slowed the model training down tremendously | Ridge was used as an alternative for Linear Regression as it performs better when some of the variables are interdependent. A quick grid search was done still arriving at very good results | All of the variables used can be derived primarily through sensors which makes automation a lot more viable | Our final model for production will be CatBoost as it performed significantly better than our other models with an MAE of 5.22 | .",
            "url": "https://nicholas-j-snyder.github.io/portfolio/fastpages/jupyter/2020/12/14/Steel-Processing.html",
            "relUrl": "/fastpages/jupyter/2020/12/14/Steel-Processing.html",
            "date": " ‚Ä¢ Dec 14, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Reviewing the Reviews",
            "content": "Problem Statement . The Film Junky Union, a new edgy community for classic movie enthusiasts, is developing a system for filtering and categorizing movie reviews. | The goal is to train a model to automatically detect negative reviews. | We will be using a dataset of IMBD movie reviews with polarity labelling to build a model for classifying positive and negative reviews. | It will need to have an F1 score of at least 0.85. | . Data Description . review: the review text | pos: the target, &#39;0&#39; for negative and &#39;1&#39; for positive | ds_part: &#39;train&#39;/&#39;test&#39; for the train/test part of dataset, correspondingly | . Solution . Prepare the Data . import math import numpy as np import pandas as pd import matplotlib import matplotlib.pyplot as plt import matplotlib.dates as mdates import seaborn as sns from tqdm.auto import tqdm from sklearn.feature_extraction.text import TfidfVectorizer import sklearn.metrics as metrics from sklearn.model_selection import RandomizedSearchCV from sklearn.model_selection import GridSearchCV from sklearn.linear_model import LogisticRegression from sklearn.dummy import DummyClassifier from lightgbm import LGBMClassifier from catboost import CatBoostClassifier import re import nltk from nltk.corpus import stopwords from nltk.tokenize import word_tokenize from nltk.stem import WordNetLemmatizer import spacy import torch import transformers from scipy.stats import randint as sp_randint from scipy.stats import uniform as sp_uniform import warnings . warnings.filterwarnings(&quot;ignore&quot;) tqdm.pandas() %matplotlib inline %config InlineBackend.figure_format = &#39;png&#39; # the next line provides graphs of better quality on HiDPI screens %config InlineBackend.figure_format = &#39;retina&#39; plt.style.use(&#39;seaborn&#39;) . STATE = 54321 . Load Data . df_reviews = pd.read_csv(&#39;C:/Users/The Ogre/datasets/imdb_reviews.tsv&#39;, sep=&#39; t&#39;, dtype={&#39;votes&#39;: &#39;Int64&#39;}) . df_reviews.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 47331 entries, 0 to 47330 Data columns (total 17 columns): # Column Non-Null Count Dtype -- -- 0 tconst 47331 non-null object 1 title_type 47331 non-null object 2 primary_title 47331 non-null object 3 original_title 47331 non-null object 4 start_year 47331 non-null int64 5 end_year 47331 non-null object 6 runtime_minutes 47331 non-null object 7 is_adult 47331 non-null int64 8 genres 47331 non-null object 9 average_rating 47329 non-null float64 10 votes 47329 non-null Int64 11 review 47331 non-null object 12 rating 47331 non-null int64 13 sp 47331 non-null object 14 pos 47331 non-null int64 15 ds_part 47331 non-null object 16 idx 47331 non-null int64 dtypes: Int64(1), float64(1), int64(5), object(10) memory usage: 6.2+ MB . . df_reviews.head() . tconst title_type primary_title original_title start_year end_year runtime_minutes is_adult genres average_rating votes review rating sp pos ds_part idx . 0 tt0068152 | movie | $ | $ | 1971 | N | 121 | 0 | Comedy,Crime,Drama | 6.3 | 2218 | The pakage implies that Warren Beatty and Gold... | 1 | neg | 0 | train | 8335 | . 1 tt0068152 | movie | $ | $ | 1971 | N | 121 | 0 | Comedy,Crime,Drama | 6.3 | 2218 | How the hell did they get this made?! Presenti... | 1 | neg | 0 | train | 8336 | . 2 tt0313150 | short | &#39;15&#39; | &#39;15&#39; | 2002 | N | 25 | 0 | Comedy,Drama,Short | 6.3 | 184 | There is no real story the film seems more lik... | 3 | neg | 0 | test | 2489 | . 3 tt0313150 | short | &#39;15&#39; | &#39;15&#39; | 2002 | N | 25 | 0 | Comedy,Drama,Short | 6.3 | 184 | Um .... a serious film about troubled teens in... | 7 | pos | 1 | test | 9280 | . 4 tt0313150 | short | &#39;15&#39; | &#39;15&#39; | 2002 | N | 25 | 0 | Comedy,Drama,Short | 6.3 | 184 | I&#39;m totally agree with GarryJohal from Singapo... | 9 | pos | 1 | test | 9281 | . . df_reviews[&#39;review&#39;].duplicated().value_counts() . False 47240 True 91 Name: review, dtype: int64 . df_reviews[df_reviews[&#39;review&#39;].duplicated()][&#39;review&#39;].value_counts().head() . Loved today&#39;s show!!! It was a variety and not solely cooking (which would have been great too). Very stimulating and captivating, always keeping the viewer peeking around the corner to see what was coming up next. She is as down to earth and as personable as you get, like one of us which made the show all the more enjoyable. Special guests, who are friends as well made for a nice surprise too. Loved the &#39;first&#39; theme and that the audience was invited to play along too. I must admit I was shocked to see her come in under her time limits on a few things, but she did it and by golly I&#39;ll be writing those recipes down. Saving time in the kitchen means more time with family. Those who haven&#39;t tuned in yet, find out what channel and the time, I assure you that you won&#39;t be disappointed. 4 Hilarious, clean, light-hearted, and quote-worthy. What else can you ask for in a film? This is my all-time, number one favorite movie. Ever since I was a little girl, I&#39;ve dreamed of owning a blue van with flames and an observation bubble. The clich√© characters in ridiculous situations are what make this film such great fun. The wonderful comedic chemistry between Stephen Furst (Harold) and Andy Tennant (Melio) make up most of my favorite parts of the movie. And who didn&#39;t love the hopeless awkwardness of Flynch? Don&#39;t forget the airport antics of Leon&#39;s cronies, dressed up as Hari Krishnas: dancing, chanting and playing the tambourine--unbeatable! The clues are genius, the locations are classic, and the plot is timeless. A word to the wise, if you didn&#39;t watch this film when you were little, it probably won&#39;t win a place in your heart today. But nevertheless give it a chance, you may find that &#34;It doesn&#39;t matter what you say, it doesn&#39;t matter what you do, you&#39;ve gotta play.&#34; 3 Robert Jordan is a television star. Robert Jordan likes things orderly, on time and properly executed. In his world children are to be seen, not heard. So why would Mr. Jordan want to become the master of a rambunctious band of Boy Scouts? Ratings. His staff figures that if learns how to interact with the youth, they will be more inclined to watch his show. Of course watching Jordan cope comprises most of the fun. Like Mr. Belvedere and Mr. Belvedere Goes to College this one is sure to please. ANYONE INTERESTED IN OBTAINING A COPY OF THIS FILM PLEASE WRITE TO ME AT: IAMASEAL2@YAHOO.COM 2 Nickelodeon has gone down the toilet. They have kids saying things like &#34;Oh my God!&#34; and &#34;We&#39;re screwed&#34; This show promotes hate for people who aren&#39;t good looking, or aren&#39;t in the in crowd. It say that sexual promiscuity is alright, by having girls slobbering over shirtless boys. Not to mention the overweight boy who takes off his shirt. The main characters basically shun anyone out of the ordinary. Carly&#39;s friend Sam, who may be a lesbian, beats the snot out of anybody that crosses her path, which says it&#39;s alright to be a b**ch. This show has so much negativity in it that nobody should watch it! I give it a 0 out of 10!!! 2 I see that C. Thomas Howell has appeared in many movies since his heyday in the 80s as an accomplished young actor. I bought this DVD because it was cheap and in part for the internet-related plot and to see how much older C. Thomas Howell is; I do not recall seeing him in any movies since the 1980s. In just a few words: what a very big disappointment. I give some low budget movies a chance, but this one started out lame. Within the first 15 minutes of the movie, this elusive woman is chatting with an Asian guy in a chatroom. They basically stimulate themselves to their own chat, she then insists on meeting the participant in person. She meets him, has sex, ties him up and then murders him in cold blood. The plot then deteriorates further. The plot is thin and flimsy and the acting is very stiff. Do not bother renting it much less purchasing it, even if it is in the $1 DVD bin. I plan to take my copy of the DVD to Goodwill. I am truly amazed that any of the prior reviewers here gave this movie a bad rating. 2 Name: review, dtype: int64 . df_reviews.drop_duplicates(&#39;review&#39;, inplace=True) df_reviews[&#39;review&#39;].duplicated().value_counts() . False 47240 Name: review, dtype: int64 . df_reviews[&#39;ds_part&#39;].value_counts() . train 23757 test 23483 Name: ds_part, dtype: int64 . Dropped all reviews that were word for word duplicates | These reviews are likely errors where people accidentally posted multiple times | . EDA . Let&#39;s check the number of movies and reviews over years. . fig, axs = plt.subplots(2, 1, figsize=(16, 8)) ax = axs[0] dft1 = df_reviews[[&#39;tconst&#39;, &#39;start_year&#39;]].drop_duplicates() [&#39;start_year&#39;].value_counts().sort_index() dft1 = dft1.reindex(index=np.arange(dft1.index.min(), max(dft1.index.max(), 2021))).fillna(0) dft1.plot(kind=&#39;bar&#39;, ax=ax) ax.set_title(&#39;Number of Movies Over Years&#39;) ax = axs[1] dft2 = df_reviews.groupby([&#39;start_year&#39;, &#39;pos&#39;])[&#39;pos&#39;].count().unstack() dft2 = dft2.reindex(index=np.arange(dft2.index.min(), max(dft2.index.max(), 2021))).fillna(0) dft2.plot(kind=&#39;bar&#39;, stacked=True, label=&#39;#reviews (neg, pos)&#39;, ax=ax) dft2 = df_reviews[&#39;start_year&#39;].value_counts().sort_index() dft2 = dft2.reindex(index=np.arange(dft2.index.min(), max(dft2.index.max(), 2021))).fillna(0) dft3 = (dft2/dft1).fillna(0) axt = ax.twinx() dft3.reset_index(drop=True).rolling(5).mean().plot(color=&#39;orange&#39;, label=&#39;reviews per movie (avg over 5 years)&#39;, ax=axt) lines, labels = axt.get_legend_handles_labels() ax.legend(lines, labels, loc=&#39;upper left&#39;) ax.set_title(&#39;Number of Reviews Over Years&#39;) fig.tight_layout() . . Let&#39;s check the distribution of number of reviews per movie with the exact counting and KDE (just to learn how it may differ from the exact counting) . fig, axs = plt.subplots(1, 2, figsize=(16, 5)) ax = axs[0] dft = df_reviews.groupby(&#39;tconst&#39;)[&#39;review&#39;].count() .value_counts() .sort_index() dft.plot.bar(ax=ax) ax.set_title(&#39;Bar Plot of #Reviews Per Movie&#39;) ax = axs[1] dft = df_reviews.groupby(&#39;tconst&#39;)[&#39;review&#39;].count() sns.kdeplot(dft, ax=ax) ax.set_title(&#39;KDE Plot of #Reviews Per Movie&#39;) fig.tight_layout() . . Most movies recieve only 1-5 reviews, with less and less recieving higher numbers | 30 appears to be the maximum amount of reviews imported into the dataset per movie, causing what looks to be a major outlier | . df_reviews[&#39;pos&#39;].value_counts() . 0 23680 1 23560 Name: pos, dtype: int64 . No significant class imbalance in the dataset as a whole | . fig, axs = plt.subplots(1, 2, figsize=(12, 4)) ax = axs[0] dft = df_reviews.query(&#39;ds_part == &quot;train&quot;&#39;)[&#39;rating&#39;].value_counts().sort_index() dft = dft.reindex(index=np.arange(min(dft.index.min(), 1), max(dft.index.max(), 11))).fillna(0) dft.plot.bar(ax=ax) ax.set_ylim([0, 5000]) ax.set_title(&#39;The train set: distribution of ratings&#39;) ax = axs[1] dft = df_reviews.query(&#39;ds_part == &quot;test&quot;&#39;)[&#39;rating&#39;].value_counts().sort_index() dft = dft.reindex(index=np.arange(min(dft.index.min(), 1), max(dft.index.max(), 11))).fillna(0) dft.plot.bar(ax=ax) ax.set_ylim([0, 5000]) ax.set_title(&#39;The test set: distribution of ratings&#39;) fig.tight_layout() . . Distributions appear to be roughly the same | . Distribution of negative and positive reviews over the years for two parts of the dataset . fig, axs = plt.subplots(2, 2, figsize=(16, 8), gridspec_kw=dict(width_ratios=(2, 1), height_ratios=(1, 1))) ax = axs[0][0] dft = df_reviews.query(&#39;ds_part == &quot;train&quot;&#39;).groupby([&#39;start_year&#39;, &#39;pos&#39;])[&#39;pos&#39;].count().unstack() dft.index = dft.index.astype(&#39;int&#39;) dft = dft.reindex(index=np.arange(dft.index.min(), max(dft.index.max(), 2020))).fillna(0) dft.plot(kind=&#39;bar&#39;, stacked=True, ax=ax) ax.set_title(&#39;The train set: number of reviews of different polarities per year&#39;) ax = axs[0][1] dft = df_reviews.query(&#39;ds_part == &quot;train&quot;&#39;).groupby([&#39;tconst&#39;, &#39;pos&#39;])[&#39;pos&#39;].count().unstack() sns.kdeplot(dft[0], color=&#39;blue&#39;, label=&#39;negative&#39;, kernel=&#39;epa&#39;, ax=ax) sns.kdeplot(dft[1], color=&#39;green&#39;, label=&#39;positive&#39;, kernel=&#39;epa&#39;, ax=ax) ax.legend() ax.set_title(&#39;The train set: distribution of different polarities per movie&#39;) ax = axs[1][0] dft = df_reviews.query(&#39;ds_part == &quot;test&quot;&#39;).groupby([&#39;start_year&#39;, &#39;pos&#39;])[&#39;pos&#39;].count().unstack() dft.index = dft.index.astype(&#39;int&#39;) dft = dft.reindex(index=np.arange(dft.index.min(), max(dft.index.max(), 2020))).fillna(0) dft.plot(kind=&#39;bar&#39;, stacked=True, ax=ax) ax.set_title(&#39;The test set: number of reviews of different polarities per year&#39;) ax = axs[1][1] dft = df_reviews.query(&#39;ds_part == &quot;test&quot;&#39;).groupby([&#39;tconst&#39;, &#39;pos&#39;])[&#39;pos&#39;].count().unstack() sns.kdeplot(dft[0], color=&#39;blue&#39;, label=&#39;negative&#39;, kernel=&#39;epa&#39;, ax=ax) sns.kdeplot(dft[1], color=&#39;green&#39;, label=&#39;positive&#39;, kernel=&#39;epa&#39;, ax=ax) ax.legend() ax.set_title(&#39;The test set: distribution of different polarities per movie&#39;) fig.tight_layout() . . Distributions still have an overall similar shape per movie and over the years for the 2 datasets | . Evaluation Procedure . Composing an evaluation routine which can be used for all models in this project | . import sklearn.metrics as metrics def evaluate_model(model, train_features, train_target, test_features, test_target): eval_stats = {} fig, axs = plt.subplots(1, 3, figsize=(20, 6)) for type, features, target in ((&#39;train&#39;, train_features, train_target), (&#39;test&#39;, test_features, test_target)): eval_stats[type] = {} pred_target = model.predict(features) pred_proba = model.predict_proba(features)[:, 1] # F1 f1_thresholds = np.arange(0, 1.01, 0.05) f1_scores = [metrics.f1_score(target, pred_proba&gt;=threshold) for threshold in f1_thresholds] # ROC fpr, tpr, roc_thresholds = metrics.roc_curve(target, pred_proba) roc_auc = metrics.roc_auc_score(target, pred_proba) eval_stats[type][&#39;ROC AUC&#39;] = roc_auc # PRC precision, recall, pr_thresholds = metrics.precision_recall_curve(target, pred_proba) aps = metrics.average_precision_score(target, pred_proba) eval_stats[type][&#39;APS&#39;] = aps if type == &#39;train&#39;: color = &#39;blue&#39; else: color = &#39;green&#39; # F1 Score ax = axs[0] max_f1_score_idx = np.argmax(f1_scores) ax.plot(f1_thresholds, f1_scores, color=color, label=f&#39;{type}, max={f1_scores[max_f1_score_idx]:.2f} @ {f1_thresholds[max_f1_score_idx]:.2f}&#39;) # setting crosses for some thresholds for threshold in (0.2, 0.4, 0.5, 0.6, 0.8): closest_value_idx = np.argmin(np.abs(f1_thresholds-threshold)) marker_color = &#39;orange&#39; if threshold != 0.5 else &#39;red&#39; ax.plot(f1_thresholds[closest_value_idx], f1_scores[closest_value_idx], color=marker_color, marker=&#39;X&#39;, markersize=7) ax.set_xlim([-0.02, 1.02]) ax.set_ylim([-0.02, 1.02]) ax.set_xlabel(&#39;threshold&#39;) ax.set_ylabel(&#39;F1&#39;) ax.legend(loc=&#39;lower center&#39;) ax.set_title(f&#39;F1 Score&#39;) # ROC ax = axs[1] ax.plot(fpr, tpr, color=color, label=f&#39;{type}, ROC AUC={roc_auc:.2f}&#39;) # setting crosses for some thresholds for threshold in (0.2, 0.4, 0.5, 0.6, 0.8): closest_value_idx = np.argmin(np.abs(roc_thresholds-threshold)) marker_color = &#39;orange&#39; if threshold != 0.5 else &#39;red&#39; ax.plot(fpr[closest_value_idx], tpr[closest_value_idx], color=marker_color, marker=&#39;X&#39;, markersize=7) ax.plot([0, 1], [0, 1], color=&#39;grey&#39;, linestyle=&#39;--&#39;) ax.set_xlim([-0.02, 1.02]) ax.set_ylim([-0.02, 1.02]) ax.set_xlabel(&#39;FPR&#39;) ax.set_ylabel(&#39;TPR&#39;) ax.legend(loc=&#39;lower center&#39;) ax.set_title(f&#39;ROC Curve&#39;) # PRC ax = axs[2] ax.plot(recall, precision, color=color, label=f&#39;{type}, AP={aps:.2f}&#39;) # setting crosses for some thresholds for threshold in (0.2, 0.4, 0.5, 0.6, 0.8): closest_value_idx = np.argmin(np.abs(pr_thresholds-threshold)) marker_color = &#39;orange&#39; if threshold != 0.5 else &#39;red&#39; ax.plot(recall[closest_value_idx], precision[closest_value_idx], color=marker_color, marker=&#39;X&#39;, markersize=7) ax.set_xlim([-0.02, 1.02]) ax.set_ylim([-0.02, 1.02]) ax.set_xlabel(&#39;recall&#39;) ax.set_ylabel(&#39;precision&#39;) ax.legend(loc=&#39;lower center&#39;) ax.set_title(f&#39;PRC&#39;) eval_stats[type][&#39;Accuracy&#39;] = metrics.accuracy_score(target, pred_target) eval_stats[type][&#39;F1&#39;] = metrics.f1_score(target, pred_target) df_eval_stats = pd.DataFrame(eval_stats) df_eval_stats = df_eval_stats.round(2) df_eval_stats = df_eval_stats.reindex(index=(&#39;Accuracy&#39;, &#39;F1&#39;, &#39;APS&#39;, &#39;ROC AUC&#39;)) print(df_eval_stats) return . Normalization . We assume all models below accepts texts in lowercase and without any digits, punctuations marks etc. . def clear_text(text): text = text.lower() text = re.sub(r&quot;[^a-z&#39;]&quot;, &quot; &quot;, text) return &quot; &quot;.join(text.split()) . df_reviews[&#39;review_norm&#39;] = df_reviews[&#39;review&#39;].progress_apply(clear_text) . 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47240/47240 [00:03&lt;00:00, 12399.70it/s] . df_reviews[&#39;review_norm&#39;].head() . 0 the pakage implies that warren beatty and gold... 1 how the hell did they get this made presenting... 2 there is no real story the film seems more lik... 3 um a serious film about troubled teens in sing... 4 i&#39;m totally agree with garryjohal from singapo... Name: review_norm, dtype: object . Data normalization successful | . Train / Test Split . Luckily, the whole dataset is already divided into train/test one parts. The corresponding flag is &#39;ds_part&#39;. . df_reviews_train = df_reviews.query(&#39;ds_part == &quot;train&quot;&#39;).copy() df_reviews_test = df_reviews.query(&#39;ds_part == &quot;test&quot;&#39;).copy() train_target = df_reviews_train[&#39;pos&#39;] test_target = df_reviews_test[&#39;pos&#39;] print(df_reviews_train.shape) print(df_reviews_test.shape) . (23757, 18) (23483, 18) . Model 0 - Constant . model_0 = DummyClassifier(random_state=STATE) . model_0.fit(df_reviews_train, train_target) . DummyClassifier(random_state=54321) . evaluate_model(model_0, df_reviews_train, train_target, df_reviews_test, test_target) . train test Accuracy 0.5 0.50 F1 0.5 0.49 APS 0.5 0.50 ROC AUC 0.5 0.50 . Sanity Test model created | . Model 1 - TF-IDF &amp; LR . corpus = df_reviews_train[&#39;review_norm&#39;] tfidf_vectorizer_1 = TfidfVectorizer() train_features_1 = tfidf_vectorizer_1.fit_transform(corpus) . corpus = df_reviews_test[&#39;review_norm&#39;] test_features_1 = tfidf_vectorizer_1.transform(corpus) . model_1 = LogisticRegression(random_state=STATE, n_jobs=-1) model_1.fit(train_features_1, train_target) . LogisticRegression(n_jobs=-1, random_state=54321) . evaluate_model(model_1, train_features_1, train_target, test_features_1, test_target) . train test Accuracy 0.93 0.88 F1 0.93 0.88 APS 0.98 0.95 ROC AUC 0.98 0.95 . Very strong start using TF-IDF and LR | . Model 2 - NTLK, TF-IDF &amp; LR . def text_preprocessing_2(text): lemmatizer = WordNetLemmatizer() tokens = word_tokenize(text) lemmas = [lemmatizer.lemmatize(token) for token in tokens] return &quot; &quot;.join(lemmas) . stop_words = set(stopwords.words(&#39;english&#39;)) corpus = df_reviews_train[&#39;review_norm&#39;].progress_apply(text_preprocessing_2) tfidf_vectorizer_2 = TfidfVectorizer(stop_words=stop_words) train_features_2 = tfidf_vectorizer_2.fit_transform(corpus) . 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23757/23757 [00:38&lt;00:00, 620.25it/s] . corpus = df_reviews_test[&#39;review_norm&#39;].progress_apply(text_preprocessing_2) test_features_2 = tfidf_vectorizer_2.transform(corpus) . 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23483/23483 [00:35&lt;00:00, 656.53it/s] . model_2 = LogisticRegression(random_state=STATE, n_jobs=-1) model_2.fit(train_features_2, train_target) . LogisticRegression(n_jobs=-1, random_state=54321) . evaluate_model(model_2, train_features_2, train_target, test_features_2, test_target) . train test Accuracy 0.94 0.88 F1 0.94 0.88 APS 0.98 0.95 ROC AUC 0.98 0.95 . No significant changes in results with NTLK lemmatization | . Model 3 - spaCy, TF-IDF &amp; LR . nlp = spacy.load(&#39;en_core_web_sm&#39;, disable=[&#39;parser&#39;, &#39;ner&#39;]) . def text_preprocessing_3(text): doc = nlp(text) #tokens = [token.lemma_ for token in doc if not token.is_stop] tokens = [token.lemma_ for token in doc] return &#39; &#39;.join(tokens) . corpus = df_reviews_train[&#39;review_norm&#39;].progress_apply(text_preprocessing_3) tfidf_vectorizer_3 = TfidfVectorizer() train_features_3 = tfidf_vectorizer_3.fit_transform(corpus) . 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23757/23757 [03:02&lt;00:00, 130.51it/s] . corpus = df_reviews_test[&#39;review_norm&#39;].progress_apply(text_preprocessing_3) test_features_3 = tfidf_vectorizer_3.transform(corpus) . 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23483/23483 [02:55&lt;00:00, 134.12it/s] . model_3 = LogisticRegression(random_state=STATE, n_jobs=-1) model_3.fit(train_features_3, train_target) . LogisticRegression(n_jobs=-1, random_state=54321) . evaluate_model(model_3, train_features_3, train_target, test_features_3, test_target) . train test Accuracy 0.92 0.88 F1 0.92 0.88 APS 0.98 0.95 ROC AUC 0.98 0.95 . Great results so far from our basic Logistic Regression models | . Model 4 - spaCy, TF-IDF &amp; LGBMClassifier . model_4 = LGBMClassifier(max_depth=-1, random_state=STATE, n_jobs=-1, silent=True, n_estimators=5000) . n_HP_points_to_test = 100 fit_params={&quot;early_stopping_rounds&quot;:30, &quot;eval_metric&quot; : &#39;f1&#39;, &quot;eval_set&quot; : [(test_features_3,test_target)], &#39;eval_names&#39;: [&#39;valid&#39;], #&#39;callbacks&#39;: [lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_099)], &#39;verbose&#39;: 100, &#39;categorical_feature&#39;: &#39;auto&#39;} param_test ={&#39;num_leaves&#39;: sp_randint(6, 50), &#39;min_child_samples&#39;: sp_randint(100, 500), &#39;min_child_weight&#39;: [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4], &#39;subsample&#39;: sp_uniform(loc=0.2, scale=0.8), &#39;colsample_bytree&#39;: sp_uniform(loc=0.4, scale=0.6), &#39;reg_alpha&#39;: [0, 1e-1, 1, 2, 5, 7, 10, 50, 100], &#39;reg_lambda&#39;: [0, 1e-1, 1, 5, 10, 20, 50, 100]} gs = RandomizedSearchCV( estimator=model_4, param_distributions=param_test, n_iter=n_HP_points_to_test, scoring=&#39;f1&#39;, cv=3, refit=True, random_state=STATE, verbose=True, n_jobs=-1) gs.fit(train_features_3, train_target, **fit_params) print(&#39;Best score reached: {} with params: {} &#39;.format(gs.best_score_, gs.best_params_)) . Fitting 3 folds for each of 100 candidates, totalling 300 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers. [Parallel(n_jobs=-1)]: Done 2 tasks | elapsed: 4.3s [Parallel(n_jobs=-1)]: Done 152 tasks | elapsed: 12.4min [Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 26.0min finished Training until validation scores don&#39;t improve for 30 rounds [100] valid&#39;s binary_logloss: 0.40199 [200] valid&#39;s binary_logloss: 0.353794 [300] valid&#39;s binary_logloss: 0.331571 [400] valid&#39;s binary_logloss: 0.318816 [500] valid&#39;s binary_logloss: 0.310229 [600] valid&#39;s binary_logloss: 0.305295 [700] valid&#39;s binary_logloss: 0.301388 [800] valid&#39;s binary_logloss: 0.298349 [900] valid&#39;s binary_logloss: 0.296801 [1000] valid&#39;s binary_logloss: 0.295235 [1100] valid&#39;s binary_logloss: 0.294407 Early stopping, best iteration is: [1075] valid&#39;s binary_logloss: 0.294294 Best score reached: 0.8622412032998058 with params: {&#39;colsample_bytree&#39;: 0.7537945420199035, &#39;min_child_samples&#39;: 136, &#39;min_child_weight&#39;: 0.001, &#39;num_leaves&#39;: 7, &#39;reg_alpha&#39;: 0.1, &#39;reg_lambda&#39;: 0.1, &#39;subsample&#39;: 0.6758840778108695} . . model_4.set_params(**gs.best_params_) model_4.fit(train_features_3, train_target) . LGBMClassifier(colsample_bytree=0.7537945420199035, min_child_samples=136, n_estimators=5000, num_leaves=7, random_state=54321, reg_alpha=0.1, reg_lambda=0.1, subsample=0.6758840778108695) . Used features from previous spacy implementation as only the model is changing | . evaluate_model(model_4, train_features_3, train_target, test_features_3, test_target) . train test Accuracy 1.0 0.87 F1 1.0 0.87 APS 1.0 0.94 ROC AUC 1.0 0.95 . LGBM returned similar results to our other models | . Clearing RAM . del train_features_1 del train_features_2 del train_features_3 del test_features_1 del test_features_2 del test_features_3 . Model 9 - BERT . tokenizer = transformers.BertTokenizer.from_pretrained(&#39;bert-base-uncased&#39;) config = transformers.BertConfig.from_pretrained(&#39;bert-base-uncased&#39;) model = transformers.BertModel.from_pretrained(&#39;bert-base-uncased&#39;) . def BERT_text_to_embeddings(texts, max_length=512, batch_size=25, force_device=None, disable_progress_bar=False): ids_list = [] attention_mask_list = [] # text to padded ids of tokens along with their attention masks for input_text in texts: ids = tokenizer.encode(input_text.lower(), add_special_tokens=True, truncation=True, max_length=max_length) padded = np.array(ids + [0]*(max_length - len(ids))) attention_mask = np.where(padded != 0, 1, 0) ids_list.append(padded) attention_mask_list.append(attention_mask) if force_device is not None: device = torch.device(force_device) else: device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) model.to(device) if not disable_progress_bar: print(f&#39;Using the {device} device.&#39;) # gettings embeddings in batches embeddings = [] for i in tqdm(range(math.ceil(len(ids_list)/batch_size)), disable=disable_progress_bar): ids_batch = torch.LongTensor(ids_list[batch_size*i:batch_size*(i+1)]).to(device) attention_mask_batch = torch.LongTensor(attention_mask_list[batch_size*i:batch_size*(i+1)]).to(device) with torch.no_grad(): model.eval() batch_embeddings = model(input_ids=ids_batch, attention_mask=attention_mask_batch) embeddings.append(batch_embeddings[0][:,0,:].detach().cpu().numpy()) return np.concatenate(embeddings) . train_features_9 = BERT_text_to_embeddings(df_reviews_train[&#39;review_norm&#39;]) test_features_9 = BERT_text_to_embeddings(df_reviews_test[&#39;review_norm&#39;]) . display(train_features_9.shape) display(test_features_9.shape) display(train_features_9.head()) test_features_9.head() . (23757, 768) . (23483, 768) . 2 3 4 5 6 7 8 9 10 11 ... 760 761 762 763 764 765 766 767 768 769 . 0 0.190339 | 0.089573 | 0.223359 | 0.149401 | -0.086628 | -0.320568 | -0.334701 | 0.861027 | 0.177856 | -0.072855 | ... | -0.418221 | -0.656052 | -0.048138 | -0.024805 | 0.230614 | 0.643616 | 0.229146 | -0.413782 | 0.665755 | 0.122339 | . 1 -0.099164 | 0.485257 | 0.052601 | 0.110193 | -0.361833 | -0.282430 | 0.387531 | 0.437034 | -0.173943 | 0.199071 | ... | -0.346951 | -0.504075 | -0.381644 | -0.027139 | -0.145117 | 0.728060 | -0.124074 | -0.229344 | 0.476974 | 0.161533 | . 2 0.032127 | 0.056152 | -0.088463 | 0.058654 | -0.107764 | -0.689901 | -0.119534 | 0.435741 | 0.169011 | 0.000699 | ... | -0.108317 | -0.366573 | 0.098580 | -0.225140 | -0.457189 | 0.698643 | -0.301066 | -0.024077 | 0.624291 | 0.418772 | . 3 -0.582068 | -0.066873 | -0.318083 | 0.162336 | -0.350301 | -0.278575 | -0.005970 | 0.032876 | -0.066221 | -0.113282 | ... | -0.261830 | -1.028325 | -0.163137 | -0.036852 | 0.171570 | 0.843479 | -0.019227 | -0.407386 | 0.681669 | 0.240410 | . 4 0.096805 | 0.012247 | -0.121642 | 0.158698 | -0.661804 | -0.657599 | -0.076837 | 0.384795 | -0.290214 | -0.204065 | ... | -0.247690 | -0.384748 | 0.350517 | -0.353625 | -0.183866 | 0.858838 | -0.086312 | -0.702053 | 0.332977 | 0.339924 | . 5 rows √ó 768 columns . 2 3 4 5 6 7 8 9 10 11 ... 760 761 762 763 764 765 766 767 768 769 . 0 -0.119205 | -0.410492 | 0.285639 | -0.081137 | -0.621164 | -0.426441 | -0.075280 | 1.080920 | -0.220635 | 0.260518 | ... | -0.361808 | -0.482571 | -0.046029 | -0.090273 | -0.044551 | 0.357637 | -0.153269 | -0.950137 | 0.511310 | 0.197022 | . 1 -0.224344 | -0.165587 | 0.451822 | -0.011807 | -0.260766 | -0.657552 | 0.166115 | 0.720476 | -0.114380 | -0.210010 | ... | 0.187561 | -0.503779 | 0.206527 | -0.226220 | -0.243826 | 0.590033 | -0.309501 | -0.648067 | 0.759704 | -0.335718 | . 2 -0.331963 | -0.134063 | 0.410920 | -0.198605 | -0.194172 | -0.553976 | 0.278082 | 0.812281 | -0.096778 | 0.164005 | ... | 0.147637 | -0.543631 | 0.066374 | -0.279855 | -0.368114 | 1.056353 | -0.057480 | -0.804741 | 0.671496 | -0.274263 | . 3 -0.300281 | -0.397958 | -0.173543 | 0.048574 | -0.180938 | -0.632225 | -0.046877 | 0.653703 | 0.001793 | -0.214381 | ... | 0.153527 | -0.399642 | 0.032174 | -0.204962 | -0.250591 | 0.929617 | -0.036775 | -0.558343 | 0.704407 | -0.367202 | . 4 -0.097485 | 0.020588 | 0.110709 | -0.018498 | -0.378970 | -0.381272 | 0.061755 | 0.614321 | 0.001056 | 0.048258 | ... | -0.000166 | -0.662073 | 0.118275 | -0.099041 | -0.088258 | 0.869683 | 0.025520 | -0.732207 | 0.713102 | -0.070752 | . 5 rows √ó 768 columns . . model_9 = LogisticRegression(random_state=STATE, n_jobs=-1) . model_9.fit(train_features_9, train_target) . LogisticRegression(n_jobs=-1, random_state=54321) . evaluate_model(model_9, train_features_9, train_target, test_features_9, test_target) . train test Accuracy 0.88 0.86 F1 0.88 0.86 APS 0.95 0.94 ROC AUC 0.95 0.94 . Our BERT model has similar results to our 4 other models | . #pd.DataFrame(test_features_9).to_csv(&#39;C:/Users/The Ogre/datasets/test_features_9.csv&#39;, index=False) #np.save(&#39;C:/Users/The Ogre/datasets/train_features_9.npy&#39;, train_features_9) #np.save(&#39;C:/Users/The Ogre/datasets/test_features_9.npy&#39;, test_features_9) . Saving our embeddings so that we don&#39;t have to run hours of calculations again | . My Reviews . my_reviews = pd.DataFrame([ &#39;I simply did not like it, not my kind of movie.&#39;, &#39;Well, I was bored and fell asleep in the middle of the movie.&#39;, &#39;I was really fascinated with the movie&#39;, &#39;Even the actors looked really old and disinterested, and they got paid to be in the movie. What a soulless cash grab.&#39;, &#39;I didn &#39;t expect the reboot to be so good! Writers really cared about the source material&#39;, &#39;The movie had its upsides and downsides, but I feel like overall it &#39;s a decent flick. I could see myself going to see it again.&#39;, &#39;What a rotten attempt at a comedy. Not a single joke lands, everyone acts annoying and loud, even kids won &#39;t like this!&#39;, &#39;Launching on Netflix was a brave move &amp; I really appreciate being able to binge on episode after episode, of this exciting intelligent new drama.&#39; ], columns=[&#39;review&#39;]) my_reviews[&#39;review_norm&#39;] = my_reviews[&#39;review&#39;].progress_apply(clear_text) my_reviews . 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:00&lt;00:00, 15978.30it/s] . review review_norm . 0 I simply did not like it, not my kind of movie. | i simply did not like it not my kind of movie | . 1 Well, I was bored and fell asleep in the middl... | well i was bored and fell asleep in the middle... | . 2 I was really fascinated with the movie | i was really fascinated with the movie | . 3 Even the actors looked really old and disinter... | even the actors looked really old and disinter... | . 4 I didn&#39;t expect the reboot to be so good! Writ... | i didn&#39;t expect the reboot to be so good write... | . 5 The movie had its upsides and downsides, but I... | the movie had its upsides and downsides but i ... | . 6 What a rotten attempt at a comedy. Not a singl... | what a rotten attempt at a comedy not a single... | . 7 Launching on Netflix was a brave move &amp; I real... | launching on netflix was a brave move i really... | . Model Evaluation . Model 1 . texts = my_reviews[&#39;review_norm&#39;] my_reviews_pred_prob = model_1.predict_proba(tfidf_vectorizer_1.transform(texts))[:, 1] for i, review in enumerate(texts.str.slice(0, 100)): print(f&#39;{my_reviews_pred_prob[i]:.2f}: {review}&#39;) . 0.15: i simply did not like it not my kind of movie 0.24: well i was bored and fell asleep in the middle of the movie 0.47: i was really fascinated with the movie 0.15: even the actors looked really old and disinterested and they got paid to be in the movie what a soul 0.22: i didn&#39;t expect the reboot to be so good writers really cared about the source material 0.63: the movie had its upsides and downsides but i feel like overall it&#39;s a decent flick i could see myse 0.04: what a rotten attempt at a comedy not a single joke lands everyone acts annoying and loud even kids 0.73: launching on netflix was a brave move i really appreciate being able to binge on episode after episo . Model 2 . texts = my_reviews[&#39;review_norm&#39;] my_reviews_pred_prob = model_2.predict_proba(tfidf_vectorizer_2.transform(texts.apply(lambda x: text_preprocessing_2(x))))[:, 1] for i, review in enumerate(texts.str.slice(0, 100)): print(f&#39;{my_reviews_pred_prob[i]:.2f}: {review}&#39;) . 0.16: i simply did not like it not my kind of movie 0.15: well i was bored and fell asleep in the middle of the movie 0.46: i was really fascinated with the movie 0.12: even the actors looked really old and disinterested and they got paid to be in the movie what a soul 0.30: i didn&#39;t expect the reboot to be so good writers really cared about the source material 0.53: the movie had its upsides and downsides but i feel like overall it&#39;s a decent flick i could see myse 0.04: what a rotten attempt at a comedy not a single joke lands everyone acts annoying and loud even kids 0.87: launching on netflix was a brave move i really appreciate being able to binge on episode after episo . Model 3 . texts = my_reviews[&#39;review_norm&#39;] my_reviews_pred_prob = model_3.predict_proba(tfidf_vectorizer_3.transform(texts.apply(lambda x: text_preprocessing_3(x))))[:, 1] for i, review in enumerate(texts.str.slice(0, 100)): print(f&#39;{my_reviews_pred_prob[i]:.2f}: {review}&#39;) . 0.13: i simply did not like it not my kind of movie 0.18: well i was bored and fell asleep in the middle of the movie 0.48: i was really fascinated with the movie 0.20: even the actors looked really old and disinterested and they got paid to be in the movie what a soul 0.19: i didn&#39;t expect the reboot to be so good writers really cared about the source material 0.72: the movie had its upsides and downsides but i feel like overall it&#39;s a decent flick i could see myse 0.03: what a rotten attempt at a comedy not a single joke lands everyone acts annoying and loud even kids 0.87: launching on netflix was a brave move i really appreciate being able to binge on episode after episo . Model 4 . texts = my_reviews[&#39;review_norm&#39;] tfidf_vectorizer_4 = tfidf_vectorizer_3 my_reviews_pred_prob = model_4.predict_proba(tfidf_vectorizer_4.transform(texts.apply(lambda x: text_preprocessing_3(x))))[:, 1] for i, review in enumerate(texts.str.slice(0, 100)): print(f&#39;{my_reviews_pred_prob[i]:.2f}: {review}&#39;) . 0.45: i simply did not like it not my kind of movie 0.34: well i was bored and fell asleep in the middle of the movie 0.73: i was really fascinated with the movie 0.26: even the actors looked really old and disinterested and they got paid to be in the movie what a soul 0.18: i didn&#39;t expect the reboot to be so good writers really cared about the source material 0.93: the movie had its upsides and downsides but i feel like overall it&#39;s a decent flick i could see myse 0.02: what a rotten attempt at a comedy not a single joke lands everyone acts annoying and loud even kids 0.95: launching on netflix was a brave move i really appreciate being able to binge on episode after episo . Model 9 . texts = my_reviews[&#39;review_norm&#39;] my_reviews_features_9 = BERT_text_to_embeddings(texts, disable_progress_bar=True) my_reviews_pred_prob = model_9.predict_proba(my_reviews_features_9)[:, 1] for i, review in enumerate(texts.str.slice(0, 100)): print(f&#39;{my_reviews_pred_prob[i]:.2f}: {review}&#39;) . 0.14: i simply did not like it not my kind of movie 0.01: well i was bored and fell asleep in the middle of the movie 0.99: i was really fascinated with the movie 0.00: even the actors looked really old and disinterested and they got paid to be in the movie what a soul 0.21: i didn&#39;t expect the reboot to be so good writers really cared about the source material 0.97: the movie had its upsides and downsides but i feel like overall it&#39;s a decent flick i could see myse 0.07: what a rotten attempt at a comedy not a single joke lands everyone acts annoying and loud even kids 0.95: launching on netflix was a brave move i really appreciate being able to binge on episode after episo . Final Conclusion . The results from our models are quite fascinating | All the models appeared to lean in a similar direction with their predictions | Some models like the BERT model gave predictions that were much closer to 1 and 0 which may imply a better understanding of word correlation | All the models struggled to identify the pleasantly surprised review (review #5). Some more research is needed to see if we can eliminate that somehow | Every model we made passes the f1 score threshold but overall our simpler logistic regression models performed the best and were quick | .",
            "url": "https://nicholas-j-snyder.github.io/portfolio/fastpages/jupyter/2020/10/25/Reviewing-the-Reviews.html",
            "relUrl": "/fastpages/jupyter/2020/10/25/Reviewing-the-Reviews.html",
            "date": " ‚Ä¢ Oct 25, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Gold Recovery",
            "content": "Problem Statement . Prepare a prototype of a machine learning model for Zyfra. The company develops efficiency solutions for heavy industry. | The model should predict the amount of gold recovered from gold ore. You have the data on extraction and purification. | The model will help to optimize the production and eliminate unprofitable parameters. | . Data description . Technological process . Rougher feed ‚Äî raw material | Rougher additions (or reagent additions) ‚Äî flotation reagents: Xanthate, Sulphate, Depressant Xanthate ‚Äî promoter or flotation activator; | Sulphate ‚Äî sodium sulphide for this particular process; | Depressant ‚Äî sodium silicate. | . | Rougher process ‚Äî flotation | Rougher tails ‚Äî product residues | Float banks ‚Äî flotation unit | Cleaner process ‚Äî purification | Rougher Au ‚Äî rougher gold concentrate | Final Au ‚Äî final gold concentrate ## Parameters of stages | air amount ‚Äî volume of air | fluid levels | feed size ‚Äî feed particle size | feed rate | . Feature naming . Here&#39;s how we named the features: [stage].[parameter_type].[parameter_name] Example: rougher.input.feed_ag . Possible values for [stage]: rougher ‚Äî flotation | primary_cleaner ‚Äî primary purification | secondary_cleaner ‚Äî secondary purification | final ‚Äî final characteristics | . | Possible values for [parameter_type]: input ‚Äî raw material parameters | output ‚Äî product parameters | state ‚Äî parameters characterizing the current state of the stage | calculation ‚Äî calculation characteristics | . | . Solution . 1. Prepare the data . import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.ensemble import RandomForestRegressor from sklearn.multioutput import MultiOutputRegressor from sklearn.linear_model import Ridge from sklearn.dummy import DummyRegressor from sklearn.model_selection import train_test_split from sklearn.model_selection import cross_validate from sklearn.metrics import make_scorer from sklearn.metrics import mean_absolute_error as MAE . . 1.1. Open the files and look into the data. . train = pd.read_csv(&#39;/datasets/gold_recovery_train.csv&#39;) test = pd.read_csv(&#39;/datasets/gold_recovery_test.csv&#39;) full = pd.read_csv(&#39;/datasets/gold_recovery_full.csv&#39;) . display(train.head()) display(test.head()) full.head() . date final.output.concentrate_ag final.output.concentrate_pb final.output.concentrate_sol final.output.concentrate_au final.output.recovery final.output.tail_ag final.output.tail_pb final.output.tail_sol final.output.tail_au ... secondary_cleaner.state.floatbank4_a_air secondary_cleaner.state.floatbank4_a_level secondary_cleaner.state.floatbank4_b_air secondary_cleaner.state.floatbank4_b_level secondary_cleaner.state.floatbank5_a_air secondary_cleaner.state.floatbank5_a_level secondary_cleaner.state.floatbank5_b_air secondary_cleaner.state.floatbank5_b_level secondary_cleaner.state.floatbank6_a_air secondary_cleaner.state.floatbank6_a_level . 0 | 2016-01-15 00:00:00 | 6.055403 | 9.889648 | 5.507324 | 42.192020 | 70.541216 | 10.411962 | 0.895447 | 16.904297 | 2.143149 | ... | 14.016835 | -502.488007 | 12.099931 | -504.715942 | 9.925633 | -498.310211 | 8.079666 | -500.470978 | 14.151341 | -605.841980 | . 1 | 2016-01-15 01:00:00 | 6.029369 | 9.968944 | 5.257781 | 42.701629 | 69.266198 | 10.462676 | 0.927452 | 16.634514 | 2.224930 | ... | 13.992281 | -505.503262 | 11.950531 | -501.331529 | 10.039245 | -500.169983 | 7.984757 | -500.582168 | 13.998353 | -599.787184 | . 2 | 2016-01-15 02:00:00 | 6.055926 | 10.213995 | 5.383759 | 42.657501 | 68.116445 | 10.507046 | 0.953716 | 16.208849 | 2.257889 | ... | 14.015015 | -502.520901 | 11.912783 | -501.133383 | 10.070913 | -500.129135 | 8.013877 | -500.517572 | 14.028663 | -601.427363 | . 3 | 2016-01-15 03:00:00 | 6.047977 | 9.977019 | 4.858634 | 42.689819 | 68.347543 | 10.422762 | 0.883763 | 16.532835 | 2.146849 | ... | 14.036510 | -500.857308 | 11.999550 | -501.193686 | 9.970366 | -499.201640 | 7.977324 | -500.255908 | 14.005551 | -599.996129 | . 4 | 2016-01-15 04:00:00 | 6.148599 | 10.142511 | 4.939416 | 42.774141 | 66.927016 | 10.360302 | 0.792826 | 16.525686 | 2.055292 | ... | 14.027298 | -499.838632 | 11.953070 | -501.053894 | 9.925709 | -501.686727 | 7.894242 | -500.356035 | 13.996647 | -601.496691 | . 5 rows √ó 87 columns . date primary_cleaner.input.sulfate primary_cleaner.input.depressant primary_cleaner.input.feed_size primary_cleaner.input.xanthate primary_cleaner.state.floatbank8_a_air primary_cleaner.state.floatbank8_a_level primary_cleaner.state.floatbank8_b_air primary_cleaner.state.floatbank8_b_level primary_cleaner.state.floatbank8_c_air ... secondary_cleaner.state.floatbank4_a_air secondary_cleaner.state.floatbank4_a_level secondary_cleaner.state.floatbank4_b_air secondary_cleaner.state.floatbank4_b_level secondary_cleaner.state.floatbank5_a_air secondary_cleaner.state.floatbank5_a_level secondary_cleaner.state.floatbank5_b_air secondary_cleaner.state.floatbank5_b_level secondary_cleaner.state.floatbank6_a_air secondary_cleaner.state.floatbank6_a_level . 0 | 2016-09-01 00:59:59 | 210.800909 | 14.993118 | 8.080000 | 1.005021 | 1398.981301 | -500.225577 | 1399.144926 | -499.919735 | 1400.102998 | ... | 12.023554 | -497.795834 | 8.016656 | -501.289139 | 7.946562 | -432.317850 | 4.872511 | -500.037437 | 26.705889 | -499.709414 | . 1 | 2016-09-01 01:59:59 | 215.392455 | 14.987471 | 8.080000 | 0.990469 | 1398.777912 | -500.057435 | 1398.055362 | -499.778182 | 1396.151033 | ... | 12.058140 | -498.695773 | 8.130979 | -499.634209 | 7.958270 | -525.839648 | 4.878850 | -500.162375 | 25.019940 | -499.819438 | . 2 | 2016-09-01 02:59:59 | 215.259946 | 12.884934 | 7.786667 | 0.996043 | 1398.493666 | -500.868360 | 1398.860436 | -499.764529 | 1398.075709 | ... | 11.962366 | -498.767484 | 8.096893 | -500.827423 | 8.071056 | -500.801673 | 4.905125 | -499.828510 | 24.994862 | -500.622559 | . 3 | 2016-09-01 03:59:59 | 215.336236 | 12.006805 | 7.640000 | 0.863514 | 1399.618111 | -498.863574 | 1397.440120 | -499.211024 | 1400.129303 | ... | 12.033091 | -498.350935 | 8.074946 | -499.474407 | 7.897085 | -500.868509 | 4.931400 | -499.963623 | 24.948919 | -498.709987 | . 4 | 2016-09-01 04:59:59 | 199.099327 | 10.682530 | 7.530000 | 0.805575 | 1401.268123 | -500.808305 | 1398.128818 | -499.504543 | 1402.172226 | ... | 12.025367 | -500.786497 | 8.054678 | -500.397500 | 8.107890 | -509.526725 | 4.957674 | -500.360026 | 25.003331 | -500.856333 | . 5 rows √ó 53 columns . date final.output.concentrate_ag final.output.concentrate_pb final.output.concentrate_sol final.output.concentrate_au final.output.recovery final.output.tail_ag final.output.tail_pb final.output.tail_sol final.output.tail_au ... secondary_cleaner.state.floatbank4_a_air secondary_cleaner.state.floatbank4_a_level secondary_cleaner.state.floatbank4_b_air secondary_cleaner.state.floatbank4_b_level secondary_cleaner.state.floatbank5_a_air secondary_cleaner.state.floatbank5_a_level secondary_cleaner.state.floatbank5_b_air secondary_cleaner.state.floatbank5_b_level secondary_cleaner.state.floatbank6_a_air secondary_cleaner.state.floatbank6_a_level . 0 | 2016-01-15 00:00:00 | 6.055403 | 9.889648 | 5.507324 | 42.192020 | 70.541216 | 10.411962 | 0.895447 | 16.904297 | 2.143149 | ... | 14.016835 | -502.488007 | 12.099931 | -504.715942 | 9.925633 | -498.310211 | 8.079666 | -500.470978 | 14.151341 | -605.841980 | . 1 | 2016-01-15 01:00:00 | 6.029369 | 9.968944 | 5.257781 | 42.701629 | 69.266198 | 10.462676 | 0.927452 | 16.634514 | 2.224930 | ... | 13.992281 | -505.503262 | 11.950531 | -501.331529 | 10.039245 | -500.169983 | 7.984757 | -500.582168 | 13.998353 | -599.787184 | . 2 | 2016-01-15 02:00:00 | 6.055926 | 10.213995 | 5.383759 | 42.657501 | 68.116445 | 10.507046 | 0.953716 | 16.208849 | 2.257889 | ... | 14.015015 | -502.520901 | 11.912783 | -501.133383 | 10.070913 | -500.129135 | 8.013877 | -500.517572 | 14.028663 | -601.427363 | . 3 | 2016-01-15 03:00:00 | 6.047977 | 9.977019 | 4.858634 | 42.689819 | 68.347543 | 10.422762 | 0.883763 | 16.532835 | 2.146849 | ... | 14.036510 | -500.857308 | 11.999550 | -501.193686 | 9.970366 | -499.201640 | 7.977324 | -500.255908 | 14.005551 | -599.996129 | . 4 | 2016-01-15 04:00:00 | 6.148599 | 10.142511 | 4.939416 | 42.774141 | 66.927016 | 10.360302 | 0.792826 | 16.525686 | 2.055292 | ... | 14.027298 | -499.838632 | 11.953070 | -501.053894 | 9.925709 | -501.686727 | 7.894242 | -500.356035 | 13.996647 | -601.496691 | . 5 rows √ó 87 columns . . train.info() print() test.info() print() full.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16860 entries, 0 to 16859 Data columns (total 87 columns): date 16860 non-null object final.output.concentrate_ag 16788 non-null float64 final.output.concentrate_pb 16788 non-null float64 final.output.concentrate_sol 16490 non-null float64 final.output.concentrate_au 16789 non-null float64 final.output.recovery 15339 non-null float64 final.output.tail_ag 16794 non-null float64 final.output.tail_pb 16677 non-null float64 final.output.tail_sol 16715 non-null float64 final.output.tail_au 16794 non-null float64 primary_cleaner.input.sulfate 15553 non-null float64 primary_cleaner.input.depressant 15598 non-null float64 primary_cleaner.input.feed_size 16860 non-null float64 primary_cleaner.input.xanthate 15875 non-null float64 primary_cleaner.output.concentrate_ag 16778 non-null float64 primary_cleaner.output.concentrate_pb 16502 non-null float64 primary_cleaner.output.concentrate_sol 16224 non-null float64 primary_cleaner.output.concentrate_au 16778 non-null float64 primary_cleaner.output.tail_ag 16777 non-null float64 primary_cleaner.output.tail_pb 16761 non-null float64 primary_cleaner.output.tail_sol 16579 non-null float64 primary_cleaner.output.tail_au 16777 non-null float64 primary_cleaner.state.floatbank8_a_air 16820 non-null float64 primary_cleaner.state.floatbank8_a_level 16827 non-null float64 primary_cleaner.state.floatbank8_b_air 16820 non-null float64 primary_cleaner.state.floatbank8_b_level 16833 non-null float64 primary_cleaner.state.floatbank8_c_air 16822 non-null float64 primary_cleaner.state.floatbank8_c_level 16833 non-null float64 primary_cleaner.state.floatbank8_d_air 16821 non-null float64 primary_cleaner.state.floatbank8_d_level 16833 non-null float64 rougher.calculation.sulfate_to_au_concentrate 16833 non-null float64 rougher.calculation.floatbank10_sulfate_to_au_feed 16833 non-null float64 rougher.calculation.floatbank11_sulfate_to_au_feed 16833 non-null float64 rougher.calculation.au_pb_ratio 15618 non-null float64 rougher.input.feed_ag 16778 non-null float64 rougher.input.feed_pb 16632 non-null float64 rougher.input.feed_rate 16347 non-null float64 rougher.input.feed_size 16443 non-null float64 rougher.input.feed_sol 16568 non-null float64 rougher.input.feed_au 16777 non-null float64 rougher.input.floatbank10_sulfate 15816 non-null float64 rougher.input.floatbank10_xanthate 16514 non-null float64 rougher.input.floatbank11_sulfate 16237 non-null float64 rougher.input.floatbank11_xanthate 14956 non-null float64 rougher.output.concentrate_ag 16778 non-null float64 rougher.output.concentrate_pb 16778 non-null float64 rougher.output.concentrate_sol 16698 non-null float64 rougher.output.concentrate_au 16778 non-null float64 rougher.output.recovery 14287 non-null float64 rougher.output.tail_ag 14610 non-null float64 rougher.output.tail_pb 16778 non-null float64 rougher.output.tail_sol 14611 non-null float64 rougher.output.tail_au 14611 non-null float64 rougher.state.floatbank10_a_air 16807 non-null float64 rougher.state.floatbank10_a_level 16807 non-null float64 rougher.state.floatbank10_b_air 16807 non-null float64 rougher.state.floatbank10_b_level 16807 non-null float64 rougher.state.floatbank10_c_air 16807 non-null float64 rougher.state.floatbank10_c_level 16814 non-null float64 rougher.state.floatbank10_d_air 16802 non-null float64 rougher.state.floatbank10_d_level 16809 non-null float64 rougher.state.floatbank10_e_air 16257 non-null float64 rougher.state.floatbank10_e_level 16809 non-null float64 rougher.state.floatbank10_f_air 16802 non-null float64 rougher.state.floatbank10_f_level 16802 non-null float64 secondary_cleaner.output.tail_ag 16776 non-null float64 secondary_cleaner.output.tail_pb 16764 non-null float64 secondary_cleaner.output.tail_sol 14874 non-null float64 secondary_cleaner.output.tail_au 16778 non-null float64 secondary_cleaner.state.floatbank2_a_air 16497 non-null float64 secondary_cleaner.state.floatbank2_a_level 16751 non-null float64 secondary_cleaner.state.floatbank2_b_air 16705 non-null float64 secondary_cleaner.state.floatbank2_b_level 16748 non-null float64 secondary_cleaner.state.floatbank3_a_air 16763 non-null float64 secondary_cleaner.state.floatbank3_a_level 16747 non-null float64 secondary_cleaner.state.floatbank3_b_air 16752 non-null float64 secondary_cleaner.state.floatbank3_b_level 16750 non-null float64 secondary_cleaner.state.floatbank4_a_air 16731 non-null float64 secondary_cleaner.state.floatbank4_a_level 16747 non-null float64 secondary_cleaner.state.floatbank4_b_air 16768 non-null float64 secondary_cleaner.state.floatbank4_b_level 16767 non-null float64 secondary_cleaner.state.floatbank5_a_air 16775 non-null float64 secondary_cleaner.state.floatbank5_a_level 16775 non-null float64 secondary_cleaner.state.floatbank5_b_air 16775 non-null float64 secondary_cleaner.state.floatbank5_b_level 16776 non-null float64 secondary_cleaner.state.floatbank6_a_air 16757 non-null float64 secondary_cleaner.state.floatbank6_a_level 16775 non-null float64 dtypes: float64(86), object(1) memory usage: 11.2+ MB &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 5856 entries, 0 to 5855 Data columns (total 53 columns): date 5856 non-null object primary_cleaner.input.sulfate 5554 non-null float64 primary_cleaner.input.depressant 5572 non-null float64 primary_cleaner.input.feed_size 5856 non-null float64 primary_cleaner.input.xanthate 5690 non-null float64 primary_cleaner.state.floatbank8_a_air 5840 non-null float64 primary_cleaner.state.floatbank8_a_level 5840 non-null float64 primary_cleaner.state.floatbank8_b_air 5840 non-null float64 primary_cleaner.state.floatbank8_b_level 5840 non-null float64 primary_cleaner.state.floatbank8_c_air 5840 non-null float64 primary_cleaner.state.floatbank8_c_level 5840 non-null float64 primary_cleaner.state.floatbank8_d_air 5840 non-null float64 primary_cleaner.state.floatbank8_d_level 5840 non-null float64 rougher.input.feed_ag 5840 non-null float64 rougher.input.feed_pb 5840 non-null float64 rougher.input.feed_rate 5816 non-null float64 rougher.input.feed_size 5834 non-null float64 rougher.input.feed_sol 5789 non-null float64 rougher.input.feed_au 5840 non-null float64 rougher.input.floatbank10_sulfate 5599 non-null float64 rougher.input.floatbank10_xanthate 5733 non-null float64 rougher.input.floatbank11_sulfate 5801 non-null float64 rougher.input.floatbank11_xanthate 5503 non-null float64 rougher.state.floatbank10_a_air 5839 non-null float64 rougher.state.floatbank10_a_level 5840 non-null float64 rougher.state.floatbank10_b_air 5839 non-null float64 rougher.state.floatbank10_b_level 5840 non-null float64 rougher.state.floatbank10_c_air 5839 non-null float64 rougher.state.floatbank10_c_level 5840 non-null float64 rougher.state.floatbank10_d_air 5839 non-null float64 rougher.state.floatbank10_d_level 5840 non-null float64 rougher.state.floatbank10_e_air 5839 non-null float64 rougher.state.floatbank10_e_level 5840 non-null float64 rougher.state.floatbank10_f_air 5839 non-null float64 rougher.state.floatbank10_f_level 5840 non-null float64 secondary_cleaner.state.floatbank2_a_air 5836 non-null float64 secondary_cleaner.state.floatbank2_a_level 5840 non-null float64 secondary_cleaner.state.floatbank2_b_air 5833 non-null float64 secondary_cleaner.state.floatbank2_b_level 5840 non-null float64 secondary_cleaner.state.floatbank3_a_air 5822 non-null float64 secondary_cleaner.state.floatbank3_a_level 5840 non-null float64 secondary_cleaner.state.floatbank3_b_air 5840 non-null float64 secondary_cleaner.state.floatbank3_b_level 5840 non-null float64 secondary_cleaner.state.floatbank4_a_air 5840 non-null float64 secondary_cleaner.state.floatbank4_a_level 5840 non-null float64 secondary_cleaner.state.floatbank4_b_air 5840 non-null float64 secondary_cleaner.state.floatbank4_b_level 5840 non-null float64 secondary_cleaner.state.floatbank5_a_air 5840 non-null float64 secondary_cleaner.state.floatbank5_a_level 5840 non-null float64 secondary_cleaner.state.floatbank5_b_air 5840 non-null float64 secondary_cleaner.state.floatbank5_b_level 5840 non-null float64 secondary_cleaner.state.floatbank6_a_air 5840 non-null float64 secondary_cleaner.state.floatbank6_a_level 5840 non-null float64 dtypes: float64(52), object(1) memory usage: 2.4+ MB &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 22716 entries, 0 to 22715 Data columns (total 87 columns): date 22716 non-null object final.output.concentrate_ag 22627 non-null float64 final.output.concentrate_pb 22629 non-null float64 final.output.concentrate_sol 22331 non-null float64 final.output.concentrate_au 22630 non-null float64 final.output.recovery 20753 non-null float64 final.output.tail_ag 22633 non-null float64 final.output.tail_pb 22516 non-null float64 final.output.tail_sol 22445 non-null float64 final.output.tail_au 22635 non-null float64 primary_cleaner.input.sulfate 21107 non-null float64 primary_cleaner.input.depressant 21170 non-null float64 primary_cleaner.input.feed_size 22716 non-null float64 primary_cleaner.input.xanthate 21565 non-null float64 primary_cleaner.output.concentrate_ag 22618 non-null float64 primary_cleaner.output.concentrate_pb 22268 non-null float64 primary_cleaner.output.concentrate_sol 21918 non-null float64 primary_cleaner.output.concentrate_au 22618 non-null float64 primary_cleaner.output.tail_ag 22614 non-null float64 primary_cleaner.output.tail_pb 22594 non-null float64 primary_cleaner.output.tail_sol 22365 non-null float64 primary_cleaner.output.tail_au 22617 non-null float64 primary_cleaner.state.floatbank8_a_air 22660 non-null float64 primary_cleaner.state.floatbank8_a_level 22667 non-null float64 primary_cleaner.state.floatbank8_b_air 22660 non-null float64 primary_cleaner.state.floatbank8_b_level 22673 non-null float64 primary_cleaner.state.floatbank8_c_air 22662 non-null float64 primary_cleaner.state.floatbank8_c_level 22673 non-null float64 primary_cleaner.state.floatbank8_d_air 22661 non-null float64 primary_cleaner.state.floatbank8_d_level 22673 non-null float64 rougher.calculation.sulfate_to_au_concentrate 22672 non-null float64 rougher.calculation.floatbank10_sulfate_to_au_feed 22672 non-null float64 rougher.calculation.floatbank11_sulfate_to_au_feed 22672 non-null float64 rougher.calculation.au_pb_ratio 21089 non-null float64 rougher.input.feed_ag 22618 non-null float64 rougher.input.feed_pb 22472 non-null float64 rougher.input.feed_rate 22163 non-null float64 rougher.input.feed_size 22277 non-null float64 rougher.input.feed_sol 22357 non-null float64 rougher.input.feed_au 22617 non-null float64 rougher.input.floatbank10_sulfate 21415 non-null float64 rougher.input.floatbank10_xanthate 22247 non-null float64 rougher.input.floatbank11_sulfate 22038 non-null float64 rougher.input.floatbank11_xanthate 20459 non-null float64 rougher.output.concentrate_ag 22618 non-null float64 rougher.output.concentrate_pb 22618 non-null float64 rougher.output.concentrate_sol 22526 non-null float64 rougher.output.concentrate_au 22618 non-null float64 rougher.output.recovery 19597 non-null float64 rougher.output.tail_ag 19979 non-null float64 rougher.output.tail_pb 22618 non-null float64 rougher.output.tail_sol 19980 non-null float64 rougher.output.tail_au 19980 non-null float64 rougher.state.floatbank10_a_air 22646 non-null float64 rougher.state.floatbank10_a_level 22647 non-null float64 rougher.state.floatbank10_b_air 22646 non-null float64 rougher.state.floatbank10_b_level 22647 non-null float64 rougher.state.floatbank10_c_air 22646 non-null float64 rougher.state.floatbank10_c_level 22654 non-null float64 rougher.state.floatbank10_d_air 22641 non-null float64 rougher.state.floatbank10_d_level 22649 non-null float64 rougher.state.floatbank10_e_air 22096 non-null float64 rougher.state.floatbank10_e_level 22649 non-null float64 rougher.state.floatbank10_f_air 22641 non-null float64 rougher.state.floatbank10_f_level 22642 non-null float64 secondary_cleaner.output.tail_ag 22616 non-null float64 secondary_cleaner.output.tail_pb 22600 non-null float64 secondary_cleaner.output.tail_sol 20501 non-null float64 secondary_cleaner.output.tail_au 22618 non-null float64 secondary_cleaner.state.floatbank2_a_air 22333 non-null float64 secondary_cleaner.state.floatbank2_a_level 22591 non-null float64 secondary_cleaner.state.floatbank2_b_air 22538 non-null float64 secondary_cleaner.state.floatbank2_b_level 22588 non-null float64 secondary_cleaner.state.floatbank3_a_air 22585 non-null float64 secondary_cleaner.state.floatbank3_a_level 22587 non-null float64 secondary_cleaner.state.floatbank3_b_air 22592 non-null float64 secondary_cleaner.state.floatbank3_b_level 22590 non-null float64 secondary_cleaner.state.floatbank4_a_air 22571 non-null float64 secondary_cleaner.state.floatbank4_a_level 22587 non-null float64 secondary_cleaner.state.floatbank4_b_air 22608 non-null float64 secondary_cleaner.state.floatbank4_b_level 22607 non-null float64 secondary_cleaner.state.floatbank5_a_air 22615 non-null float64 secondary_cleaner.state.floatbank5_a_level 22615 non-null float64 secondary_cleaner.state.floatbank5_b_air 22615 non-null float64 secondary_cleaner.state.floatbank5_b_level 22616 non-null float64 secondary_cleaner.state.floatbank6_a_air 22597 non-null float64 secondary_cleaner.state.floatbank6_a_level 22615 non-null float64 dtypes: float64(86), object(1) memory usage: 15.1+ MB . . train.dropna(inplace=True) train.reset_index(drop=True, inplace=True) train.tail() . date final.output.concentrate_ag final.output.concentrate_pb final.output.concentrate_sol final.output.concentrate_au final.output.recovery final.output.tail_ag final.output.tail_pb final.output.tail_sol final.output.tail_au ... secondary_cleaner.state.floatbank4_a_air secondary_cleaner.state.floatbank4_a_level secondary_cleaner.state.floatbank4_b_air secondary_cleaner.state.floatbank4_b_level secondary_cleaner.state.floatbank5_a_air secondary_cleaner.state.floatbank5_a_level secondary_cleaner.state.floatbank5_b_air secondary_cleaner.state.floatbank5_b_level secondary_cleaner.state.floatbank6_a_air secondary_cleaner.state.floatbank6_a_level . 11012 | 2018-08-18 06:59:59 | 3.224920 | 11.356233 | 6.803482 | 46.713954 | 73.755150 | 8.769645 | 3.141541 | 10.403181 | 1.529220 | ... | 23.031497 | -501.167942 | 20.007571 | -499.740028 | 18.006038 | -499.834374 | 13.001114 | -500.155694 | 20.007840 | -501.296428 | . 11013 | 2018-08-18 07:59:59 | 3.195978 | 11.349355 | 6.862249 | 46.866780 | 69.049291 | 8.897321 | 3.130493 | 10.549470 | 1.612542 | ... | 22.960095 | -501.612783 | 20.035660 | -500.251357 | 17.998535 | -500.395178 | 12.954048 | -499.895163 | 19.968498 | -501.041608 | . 11014 | 2018-08-18 08:59:59 | 3.109998 | 11.434366 | 6.886013 | 46.795691 | 67.002189 | 8.529606 | 2.911418 | 11.115147 | 1.596616 | ... | 23.015718 | -501.711599 | 19.951231 | -499.857027 | 18.019543 | -500.451156 | 13.023431 | -499.914391 | 19.990885 | -501.518452 | . 11015 | 2018-08-18 09:59:59 | 3.367241 | 11.625587 | 6.799433 | 46.408188 | 65.523246 | 8.777171 | 2.819214 | 10.463847 | 1.602879 | ... | 23.024963 | -501.153409 | 20.054122 | -500.314711 | 17.979515 | -499.272871 | 12.992404 | -499.976268 | 20.013986 | -500.625471 | . 11016 | 2018-08-18 10:59:59 | 3.598375 | 11.737832 | 6.717509 | 46.299438 | 70.281454 | 8.406690 | 2.517518 | 10.652193 | 1.389434 | ... | 23.018622 | -500.492702 | 20.020205 | -500.220296 | 17.963512 | -499.939490 | 12.990306 | -500.080993 | 19.990336 | -499.191575 | . 5 rows √ó 87 columns . The data has tons of Null values, to avoid trying to sort through all the holes we dropped them | After trimming down the data set there is still 11017 entries | If we struggle to find a good model we will revisit this method of approach | . 1.2. Check that recovery is calculated correctly. . Using the training set, calculate recovery for the rougher.output.recovery feature. | Find the MAE between your calculations and the feature values. | Provide findings. | . C = train[&#39;rougher.output.concentrate_au&#39;] F = train[&#39;rougher.input.feed_au&#39;] T = train[&#39;rougher.output.tail_au&#39;] train[&#39;rougher.calculation.recovery&#39;] = C * (F - T) / (F * (C - T)) * 100 MAE(train[&#39;rougher.output.recovery&#39;], train[&#39;rougher.calculation.recovery&#39;]) . 9.555596961987514e-15 . This is an incredibly small number, we&#39;re safe to proceed with testing | Tried following suit with labeling standards | . 1.3. Analyze the features not available in the test set. . What are these parameters? | What is their type? | . columns_full = set(full.columns) columns_test = set(test.columns) removable_features = list(columns_full.difference(columns_test)) . removable_features.remove(&#39;final.output.recovery&#39;) removable_features.remove(&#39;rougher.output.recovery&#39;) removable_features . . [&#39;primary_cleaner.output.tail_ag&#39;, &#39;final.output.concentrate_ag&#39;, &#39;secondary_cleaner.output.tail_ag&#39;, &#39;final.output.tail_au&#39;, &#39;secondary_cleaner.output.tail_sol&#39;, &#39;final.output.tail_sol&#39;, &#39;secondary_cleaner.output.tail_au&#39;, &#39;primary_cleaner.output.concentrate_sol&#39;, &#39;final.output.concentrate_sol&#39;, &#39;primary_cleaner.output.concentrate_au&#39;, &#39;rougher.calculation.sulfate_to_au_concentrate&#39;, &#39;rougher.output.tail_sol&#39;, &#39;rougher.output.concentrate_au&#39;, &#39;rougher.calculation.floatbank11_sulfate_to_au_feed&#39;, &#39;rougher.output.tail_au&#39;, &#39;primary_cleaner.output.tail_sol&#39;, &#39;final.output.tail_ag&#39;, &#39;rougher.calculation.au_pb_ratio&#39;, &#39;primary_cleaner.output.concentrate_ag&#39;, &#39;secondary_cleaner.output.tail_pb&#39;, &#39;final.output.concentrate_pb&#39;, &#39;primary_cleaner.output.tail_pb&#39;, &#39;primary_cleaner.output.concentrate_pb&#39;, &#39;rougher.calculation.floatbank10_sulfate_to_au_feed&#39;, &#39;final.output.concentrate_au&#39;, &#39;rougher.output.concentrate_sol&#39;, &#39;final.output.tail_pb&#39;, &#39;rougher.output.concentrate_ag&#39;, &#39;rougher.output.concentrate_pb&#39;, &#39;rougher.output.tail_pb&#39;, &#39;rougher.output.tail_ag&#39;, &#39;primary_cleaner.output.tail_au&#39;] . All the missing columns from the test set appear to be output values or calculations | We have no Target columns in the test dataset making it useless for testing. | . 1.4. Perform data preprocessing. . STATE = 12345 . full.dropna(inplace=True) full.reset_index(drop=True, inplace=True) full.shape . (16094, 87) . train, test = train_test_split(full, test_size=0.25, random_state=STATE) . With missing Target columns in the test set I decided to trash the train and test data and resplit the full data set | If this method wasn&#39;t acceptable to the owner or project supervisor I would manually calculate the target values as I did earlier in this project. | Roughly 6000 data entries were trashed due to Null values, if this causes issues down the line we will revisit scrapping these points | . 2. Analyze the data . 2.1. Take note of how the concentrations of metals (Au, Ag, Pb) change depending on the purification stage. . ORES = [&#39;au&#39;, &#39;ag&#39;, &#39;pb&#39;] STAGES = [&#39;rougher.output.concentrate_&#39;, &#39;primary_cleaner.output.concentrate_&#39;, &#39;final.output.concentrate_&#39;] . for ore in ORES: plt.figure(figsize=(12,6)) for stage in STAGES: plt.hist(full[stage + ore], label=(stage + ore), bins = 50, alpha=0.5) plt.xlabel(ore.capitalize() + &#39; Concentration&#39;) plt.title(ore.capitalize() + &#39; Concentrations&#39;) plt.legend() plt.grid() plt.show() . . Gold(Au) concentrations increase with each stage | Silver(Ag) gets filtered out and goes down with each stage | Lead(Pb) stays roughly the same throughout the process | There appears to be some outliers where ore == 0 | . 2.2. Compare the feed particle size distributions in the training set and in the test set. If the distributions vary significantly, the model evaluation will be incorrect. . plt.figure(figsize=(12,6)) plt.hist(train[&#39;rougher.input.feed_size&#39;], label=&#39;Training Set&#39;, bins=75, alpha=0.5, density=True) plt.hist(test[&#39;rougher.input.feed_size&#39;], label=&#39;Test Set&#39;, bins=75, alpha=0.5, density=True) plt.xlabel(&#39;Feed Size&#39;) plt.xlim((0, 150)) plt.title(&#39;Feed Size Distributions&#39;) plt.legend() plt.grid() plt.show() . . The feed size distributions appear to be very similar making things safe to proceed | There are some major outliers in the dataset extending into the 300+ range, an xlim was placed to get a better view of the bulk of the data | . for ore in ORES: plt.figure(figsize=(12,6)) plt.hist(train[&#39;rougher.input.feed_&#39; + ore], label=&#39;Training Set&#39;, bins = 50, alpha=0.25, density=True) plt.hist(test[&#39;rougher.input.feed_&#39; + ore], label= &#39;Test Set&#39;, bins = 50, alpha=0.50, density=True) plt.xlabel(ore.capitalize() + &#39; Feed Size&#39;) plt.title(ore.capitalize() + &#39; Feed Size Distributions&#39;) plt.legend() plt.grid() plt.show() . . Individual distributions for Gold(Au), Silver(Ag), and Lead(Pb) appear to be similar as well | . 2.3. Consider the total concentrations of all substances at different stages: raw feed, rougher concentrate, and final concentrate. . Do we notice any abnormal values in the total distribution? | If we do, is it worth removing such values from both samples? | . print(&#39;Train Size Before:&#39;, train.shape[0]) print(&#39;Test Size Before:&#39;, test.shape[0]) train = train[train[&#39;rougher.input.feed_size&#39;] &lt;= 120] test = test[test[&#39;rougher.input.feed_size&#39;] &lt;= 120] for ore in ORES: for stage in STAGES: [stage + ore] train = train[train[stage + ore] &gt; 0] test = test[test[stage + ore] &gt; 0] print(&#39;Train Size After:&#39;, train.shape[0]) print(&#39;Test Size After:&#39;, test.shape[0]) . Train Size Before: 12070 Test Size Before: 4024 Train Size After: 11707 Test Size After: 3884 . 3. Build the model . train = train.drop(removable_features + [&#39;date&#39;], axis=1) test = test.drop(removable_features + [&#39;date&#39;], axis=1) print(train.shape) test.shape . (11707, 54) . (3884, 54) . Removed columns that were not initially available in the test set (excluding target columns) | . features_train = train.drop([&#39;rougher.output.recovery&#39;, &#39;final.output.recovery&#39;], axis=1) targets_train = train[[&#39;rougher.output.recovery&#39;, &#39;final.output.recovery&#39;]] features_test = test.drop([&#39;rougher.output.recovery&#39;, &#39;final.output.recovery&#39;], axis=1) targets_test = test[[&#39;rougher.output.recovery&#39;, &#39;final.output.recovery&#39;]] . Datasets prepared for model training | . 3.1. Write a function to calculate the final sMAPE value. . def smape(target, predictions): return np.sum(np.abs(target - predictions) / ((np.abs(target) + np.abs(predictions)) / 2)) / len(target) * 100 def final_smape(targets, predictions): return (.25 * smape(targets[0], predictions[0])) + (.75 * smape(targets[1], predictions[1])) . Broke the smape function into 2 parts, so that smape can still be called on a single target | . 3.2. Train different models. . Evaluate them using cross-validation. | Pick the best model and test it using the test sample. Provide findings. | . MultiOutputRegressor - Ridge . model = MultiOutputRegressor(Ridge(random_state=STATE)) scores = cross_validate( model, features_train, targets_train.values, cv=10, scoring=make_scorer(final_smape, greater_is_better=False), n_jobs=-1 ) . scores[&#39;test_score&#39;] = np.abs(scores[&#39;test_score&#39;]) print(&#39;All Scores:&#39;, scores[&#39;test_score&#39;]) print(&#39;Min Score:&#39;, scores[&#39;test_score&#39;].min()) print(&#39;Max Score:&#39;, scores[&#39;test_score&#39;].max()) print(&#39;Mean Score&#39;, scores[&#39;test_score&#39;].mean()) . All Scores: [3.28774945 5.78103106 3.30564647 1.78031086 8.5693093 4.72647352 3.0575838 2.57316625 2.14893175 4.33952347] Min Score: 1.7803108614465526 Max Score: 8.56930929520799 Mean Score 3.9569725918259193 . We have a set of decent scores, time to train model 2 and compare | . MultiOutput - RandomForestRegressor . model = MultiOutputRegressor(RandomForestRegressor(n_estimators=100, max_depth=20, random_state=STATE)) . scores = cross_validate( model, features_train, targets_train.values, cv=10, scoring=make_scorer(final_smape, greater_is_better=False), n_jobs=-1 ) . scores[&#39;test_score&#39;] = np.abs(scores[&#39;test_score&#39;]) print(&#39;All Scores:&#39;, scores[&#39;test_score&#39;]) print(&#39;Min Score:&#39;, scores[&#39;test_score&#39;].min()) print(&#39;Max Score:&#39;, scores[&#39;test_score&#39;].max()) print(&#39;Mean Score&#39;, scores[&#39;test_score&#39;].mean()) . All Scores: [3.4162687 6.70766298 1.83670086 2.70828718 6.03572728 4.60284131 2.83019246 0.74377943 1.02469512 0.81966925] Min Score: 0.743779429586609 Max Score: 6.707662976360825 Mean Score 3.0725824553865158 . This is computationally expensive and so cross_validation was done less times | Overall though these scores appear to be much better than our previous model | . Sanity Test . dummy_model = DummyRegressor(strategy=&#39;mean&#39;) dummy_model.fit(features_train, targets_train.values) dummy_pred = dummy_model.predict(features_test) . print(f&quot;Baseline using the mean: {final_smape(targets_test.values, dummy_pred):.2f}% sMAPE&quot;) . Baseline using the mean: 7.31% sMAPE . model = MultiOutputRegressor(RandomForestRegressor(n_estimators=100, max_depth=20, random_state=STATE)) model.fit(features_train, targets_train.values) predictions = model.predict(features_test) . print(f&quot;Our model yields: {final_smape(targets_test.values, predictions):.2f}% sMAPE&quot;) . Our model yields: 3.77% sMAPE . Final Conclusion . We were able to achieve a 3.54% improvement over the sanity test model. | With more time we may be able to bring this number down even more by pruning out less data points | The model chosen may not appeal to the company due to its overall computational cost | Looking into more advanced models and estimators may also be of benefit to the company | .",
            "url": "https://nicholas-j-snyder.github.io/portfolio/machine%20learning/python/eda/multi-target/regression/2020/08/09/Gold-Recovery.html",
            "relUrl": "/machine%20learning/python/eda/multi-target/regression/2020/08/09/Gold-Recovery.html",
            "date": " ‚Ä¢ Aug 9, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Video Game Sales",
            "content": "Problem Statement . You work for the online store Ice, which sells video games all over the world. | User and expert reviews, genres, platforms (e.g. Xbox or PlayStation), and historical data on game sales are available from open sources. | We need to identify patterns that determine whether a game succeeds or not. This will allow us to spot potential big winners and plan advertising campaigns. | . In front of us is data going back to 2016. Let‚Äôs imagine that it‚Äôs December 2016 and we‚Äôre planning a campaign for 2017. . Data description . Name | Platform | Year_of_Release | Genre | NA_sales (North American sales in USD million) | EU_sales (sales in Europe in USD million) | JP_sales (sales in Japan in USD million) | Other_sales (sales in other countries in USD million) | Critic_Score (maximum of 100) | User_Score (maximum of 10) | Rating (ESRB) | . Data for 2016 may be incomplete. . Solution . 1. Open the data file and study the general information . import pandas as pd import matplotlib as plt import numpy as np from scipy import stats as st import math . games = pd.read_csv(&#39;/datasets/games.csv&#39;) . YEAR = &#39;Year_of_Release&#39; . games.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16715 entries, 0 to 16714 Data columns (total 11 columns): Name 16713 non-null object Platform 16715 non-null object Year_of_Release 16446 non-null float64 Genre 16713 non-null object NA_sales 16715 non-null float64 EU_sales 16715 non-null float64 JP_sales 16715 non-null float64 Other_sales 16715 non-null float64 Critic_Score 8137 non-null float64 User_Score 10014 non-null object Rating 9949 non-null object dtypes: float64(6), object(5) memory usage: 1.4+ MB . games.head() . Name Platform Year_of_Release Genre NA_sales EU_sales JP_sales Other_sales Critic_Score User_Score Rating . 0 | Wii Sports | Wii | 2006.0 | Sports | 41.36 | 28.96 | 3.77 | 8.45 | 76.0 | 8 | E | . 1 | Super Mario Bros. | NES | 1985.0 | Platform | 29.08 | 3.58 | 6.81 | 0.77 | NaN | NaN | NaN | . 2 | Mario Kart Wii | Wii | 2008.0 | Racing | 15.68 | 12.76 | 3.79 | 3.29 | 82.0 | 8.3 | E | . 3 | Wii Sports Resort | Wii | 2009.0 | Sports | 15.61 | 10.93 | 3.28 | 2.95 | 80.0 | 8 | E | . 4 | Pokemon Red/Pokemon Blue | GB | 1996.0 | Role-Playing | 11.27 | 8.89 | 10.22 | 1.00 | NaN | NaN | NaN | . Number of Empty Cells . display(games[(games[&#39;Rating&#39;].isna()) &amp; (games[YEAR] &gt;= 2015)].count()) games[(games[&#39;Rating&#39;].isna()) &amp; (games[YEAR] &lt;= 1994)].count() . . Name 513 Platform 513 Year_of_Release 513 Genre 513 NA_sales 513 EU_sales 513 JP_sales 513 Other_sales 513 Critic_Score 28 User_Score 42 Rating 0 dtype: int64 . Name 481 Platform 483 Year_of_Release 483 Genre 481 NA_sales 483 EU_sales 483 JP_sales 483 Other_sales 483 Critic_Score 0 User_Score 0 Rating 0 dtype: int64 . games[&#39;User_Score&#39;].value_counts().sort_index() . 0 1 0.2 2 0.3 2 0.5 2 0.6 2 ... 9.4 11 9.5 6 9.6 2 9.7 1 tbd 2424 Name: User_Score, Length: 96, dtype: int64 . Conclusion . Only 483 games were released before the ESRB was created (1994). So the missing values appear to be random and not because of existing before the agency was created. Game reviews and user ratings also weren&#39;t a popular thing to do until later on in video games existence. | &#39;User_Score&#39; has both numeric and string values which we will need to address if we want to do mathematical calculations using those rows. | Overall the ratings and scores for each game are the least reliable data. | There is data for the same game multiple times, yet the values are not the same. Upon further inspection this appears to be from being released on multiple consoles and being re-released on new consoles later on. | . 2. Prepare the data . 2.1 Replace the column names (make them lowercase). . games.columns = games.columns.str.lower() YEAR = YEAR.lower() year_filter = games[YEAR] != games[YEAR].min() games.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16715 entries, 0 to 16714 Data columns (total 11 columns): name 16713 non-null object platform 16715 non-null object year_of_release 16446 non-null float64 genre 16713 non-null object na_sales 16715 non-null float64 eu_sales 16715 non-null float64 jp_sales 16715 non-null float64 other_sales 16715 non-null float64 critic_score 8137 non-null float64 user_score 10014 non-null object rating 9949 non-null object dtypes: float64(6), object(5) memory usage: 1.4+ MB . 2.2 Convert the data to the required types. . games[YEAR] = games[YEAR].fillna(1970) games[YEAR] = pd.to_datetime(games[YEAR], format=&#39;%Y&#39;) games.info() #games.head() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16715 entries, 0 to 16714 Data columns (total 11 columns): name 16713 non-null object platform 16715 non-null object year_of_release 16715 non-null datetime64[ns] genre 16713 non-null object na_sales 16715 non-null float64 eu_sales 16715 non-null float64 jp_sales 16715 non-null float64 other_sales 16715 non-null float64 critic_score 8137 non-null float64 user_score 10014 non-null object rating 9949 non-null object dtypes: datetime64[ns](1), float64(5), object(5) memory usage: 1.4+ MB . 2.3 Deal with missing values . games[&#39;user_score&#39;].where(games[&#39;user_score&#39;] != &#39;tbd&#39;, other=-2, inplace=True) #games[&#39;user_score&#39;].value_counts() games[&#39;user_score&#39;] = games[&#39;user_score&#39;].astype(&#39;float&#39;) . games = games.fillna(-1) games.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16715 entries, 0 to 16714 Data columns (total 11 columns): name 16715 non-null object platform 16715 non-null object year_of_release 16715 non-null datetime64[ns] genre 16715 non-null object na_sales 16715 non-null float64 eu_sales 16715 non-null float64 jp_sales 16715 non-null float64 other_sales 16715 non-null float64 critic_score 16715 non-null float64 user_score 16715 non-null float64 rating 16715 non-null object dtypes: datetime64[ns](1), float64(6), object(4) memory usage: 1.4+ MB . 2.4 Calculate total sales (the sum of sales in all regions) . games[&#39;total_sales&#39;] = games[&#39;na_sales&#39;] + games[&#39;eu_sales&#39;] + games[&#39;jp_sales&#39;] + games[&#39;other_sales&#39;] . games.head() . name platform year_of_release genre na_sales eu_sales jp_sales other_sales critic_score user_score rating total_sales . 0 | Wii Sports | Wii | 2006-01-01 | Sports | 41.36 | 28.96 | 3.77 | 8.45 | 76.0 | 8.0 | E | 82.54 | . 1 | Super Mario Bros. | NES | 1985-01-01 | Platform | 29.08 | 3.58 | 6.81 | 0.77 | -1.0 | -1.0 | -1 | 40.24 | . 2 | Mario Kart Wii | Wii | 2008-01-01 | Racing | 15.68 | 12.76 | 3.79 | 3.29 | 82.0 | 8.3 | E | 35.52 | . 3 | Wii Sports Resort | Wii | 2009-01-01 | Sports | 15.61 | 10.93 | 3.28 | 2.95 | 80.0 | 8.0 | E | 32.77 | . 4 | Pokemon Red/Pokemon Blue | GB | 1996-01-01 | Role-Playing | 11.27 | 8.89 | 10.22 | 1.00 | -1.0 | -1.0 | -1 | 31.38 | . Conclusion . column names changed to lowercase | year of release converted to datetime object | -1 used to fill empty values | &#39;tbd&#39; replaced for user_score with -2 so that we can use numerical operations on the column and identify them from the original NaN columns (but still be able to filter them out easily) | total sales calculated | . 3. Analyze the data . [x] Look at how many games were released in different years. Is the data for every period significant? | [x] Look at how sales varied from platform to platform. Choose the platforms with the greatest total sales and build a distribution based on data for each year. Find platforms that used to be popular but now have zero sales. How long does it generally take for new platforms to appear and old ones to fade? | [x] Determine what period you should take data for.¬†To do so, look at your answers to the previous questions. The data should allow you to build a prognosis for 2017. | [x] Work only with the data that you&#39;ve decided is relevant. Disregard the data for previous years. | [x] Which platforms are leading in sales? Which ones are growing or shrinking? Select several potentially profitable platforms. | [x] Build a box plot for the global sales of all games, broken down by platform. Are the differences in sales significant? What about average sales on various platforms? Describe your findings. | [x] Take a look at how user and professional reviews affect sales for one popular platform (you choose). Build a scatter plot and calculate the correlation between reviews and sales. Draw conclusions. | [x] Keeping your conclusions in mind, compare the sales of the same games on other platforms. | [x] Take a look at the general distribution of games by genre. What can we say about the most profitable genres? Can you generalize about genres with high and low sales? | . releases_per_year = ( games[year_filter] .pivot_table(index=[YEAR], values=&#39;name&#39;, aggfunc=&#39;count&#39;) ) releases_per_year.index = releases_per_year.index.year releases_per_year.columns = [&#39;number_of_releases&#39;] releases_per_year.plot(kind=&#39;bar&#39;, figsize=(8,8)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4cb03bbe90&gt; . sales_per_platform = ( games .pivot_table(index=[&#39;platform&#39;], values=[&#39;total_sales&#39;, &#39;na_sales&#39;, &#39;eu_sales&#39;, &#39;jp_sales&#39;], aggfunc=&#39;sum&#39;) .sort_values(&#39;total_sales&#39;, ascending=False) ) sales_per_platform.sort_values(&#39;total_sales&#39;, ascending=True)[&#39;total_sales&#39;].plot(kind=&#39;barh&#39;, figsize=(8,8)) sales_per_platform[&#39;total_sales&#39;].head() . platform PS2 1255.77 X360 971.42 PS3 939.65 Wii 907.51 DS 806.12 Name: total_sales, dtype: float64 . platforms = sales_per_platform.head().index.values print(platforms) for platform in platforms: platform_table = ( games[(year_filter) &amp; (games[&#39;platform&#39;] == platform)] .pivot_table(index=[YEAR], values=&#39;total_sales&#39;, aggfunc=&#39;sum&#39;) ) platform_table.index = platform_table.index.year platform_table.columns = [(platform + &#39; Sales&#39;)] platform_table[platform_table[YEAR] &gt; 1970].plot(kind=&#39;bar&#39;) . . [&#39;PS2&#39; &#39;X360&#39; &#39;PS3&#39; &#39;Wii&#39; &#39;DS&#39;] . outdated_platforms = np.append(platforms, [&#39;PSP&#39;]) games_new = games[games[YEAR] &gt;= pd.to_datetime(2007, format=&#39;%Y&#39;)] . new_sales_per_platform = ( games_new .pivot_table(index=[&#39;platform&#39;], values=[&#39;total_sales&#39;, &#39;na_sales&#39;, &#39;eu_sales&#39;, &#39;jp_sales&#39;], aggfunc=&#39;sum&#39;) .sort_values(&#39;total_sales&#39;, ascending=False) ) new_sales_per_platform.sort_values(&#39;total_sales&#39;, ascending=True)[&#39;total_sales&#39;].plot(kind=&#39;barh&#39;, figsize=(8,8), title=&#39;Sales by Platform&#39;) . . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4cad499b10&gt; . platform_filter = r&#39; b(?:{}) b&#39;.format(&#39;|&#39;.join(outdated_platforms)) new_platforms = new_sales_per_platform[~(new_sales_per_platform.index.str.contains(platform_filter))].head(6).index.values print(new_platforms) for platform in new_platforms: platform_table = ( games_new[(games_new[&#39;platform&#39;] == platform)] .pivot_table(index=[YEAR], values=&#39;total_sales&#39;, aggfunc=&#39;sum&#39;) ) platform_table.index = platform_table.index.year platform_table.columns = [(platform + &#39; Sales&#39;)] platform_table.plot(kind=&#39;bar&#39;) . [&#39;PS4&#39; &#39;3DS&#39; &#39;PC&#39; &#39;XOne&#39; &#39;WiiU&#39; &#39;PSV&#39;] . all_pc_sales = ( games[(year_filter) &amp; (games[&#39;platform&#39;] == &#39;PC&#39;)] .pivot_table(index=[YEAR], values=&#39;total_sales&#39;, aggfunc=&#39;sum&#39;) ) all_pc_sales.index = all_pc_sales.index.year all_pc_sales.columns = [&#39;pc_sales&#39;] all_pc_sales.plot(kind=&#39;bar&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4cad947190&gt; . games.boxplot(&#39;total_sales&#39;, by=&#39;platform&#39;, figsize=(15,13), showfliers=False) games_new.boxplot(&#39;total_sales&#39;, by=&#39;platform&#39;, figsize=(15,13), showfliers=False) . /opt/conda/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray return array(a, dtype, copy=False, order=order) /opt/conda/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray return array(a, dtype, copy=False, order=order) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4cad6ec990&gt; . ps2_games = games[games[&#39;platform&#39;] == &#39;PS2&#39;] ps2_games[ps2_games[&#39;user_score&#39;] &gt;= 0].plot.scatter(x=&#39;user_score&#39;, y=&#39;total_sales&#39;) ps2_games[ps2_games[&#39;critic_score&#39;] &gt;= 0].plot.scatter(x=&#39;critic_score&#39;, y=&#39;total_sales&#39;) ps2_games[[&#39;user_score&#39;,&#39;total_sales&#39;, &#39;critic_score&#39;]].corr() . user_score total_sales critic_score . user_score | 1.000000 | 0.258433 | 0.787248 | . total_sales | 0.258433 | 1.000000 | 0.299548 | . critic_score | 0.787248 | 0.299548 | 1.000000 | . For the PS2 user_score had very little correlation to game sales. But someone generally can&#39;t give their opinion on a game until after they&#39;ve bought and played it. So, even if a game isn&#39;t well liked, if theres enough hype/demand plenty of copies will be bought before the gaming community can give their full opinions. Critics opinions also had very little impact on game sales but were heavily correlated with user scores. | . multi_platform_games = ( games[&#39;name&#39;] .value_counts() .sort_values(ascending=False) .head() ) multi_platform_games . Need for Speed: Most Wanted 12 Ratatouille 9 LEGO Marvel Super Heroes 9 FIFA 14 9 Madden NFL 07 9 Name: name, dtype: int64 . for game in multi_platform_games.index: ( games[games[&#39;name&#39;] == game] .pivot_table(index=[&#39;platform&#39;], values=&#39;total_sales&#39;, aggfunc=&#39;sum&#39;) .plot(kind=&#39;bar&#39;, figsize=(6,6), title=game) ) . games_per_genre = ( games[games[&#39;genre&#39;] != -1] .pivot_table(index=[&#39;genre&#39;], values=&#39;name&#39;, aggfunc=&#39;count&#39;) .sort_values(&#39;name&#39;, ascending=False) ) games_per_genre.columns = [&#39;Number of Releases&#39;] sales_per_genre = ( games[games[&#39;genre&#39;] != -1] .pivot_table(index=[&#39;genre&#39;], values=[&#39;total_sales&#39;, &#39;na_sales&#39;, &#39;eu_sales&#39;,&#39;jp_sales&#39;], aggfunc=&#39;sum&#39;) .sort_values(&#39;total_sales&#39;, ascending=False) ) . sales_per_genre[&#39;total_sales&#39;].plot(kind=&#39;bar&#39;, figsize=(15,6), title=&#39;All Time Sales&#39;) games_per_genre.plot(kind=&#39;bar&#39;, figsize=(15,6), title=&#39;All Time Releases per Genre&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4cae057850&gt; . new_games_per_genre = ( games_new[games_new[&#39;genre&#39;] != -1] .pivot_table(index=[&#39;genre&#39;], values=&#39;name&#39;, aggfunc=&#39;count&#39;) .sort_values(&#39;name&#39;, ascending=False) ) new_games_per_genre.columns = [&#39;Number of Releases&#39;] new_sales_per_genre = ( games_new[games_new[&#39;genre&#39;] != -1] .pivot_table(index=[&#39;genre&#39;], values=[&#39;total_sales&#39;, &#39;na_sales&#39;, &#39;eu_sales&#39;,&#39;jp_sales&#39;], aggfunc=&#39;sum&#39;) .sort_values(&#39;total_sales&#39;, ascending=False) ) . new_sales_per_genre[&#39;total_sales&#39;].plot(kind=&#39;bar&#39;, figsize=(15,6), title=&#39;Video Game Sales by Genre from 2007-2016&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4cad693e50&gt; . new_games_per_genre.plot(kind=&#39;bar&#39;, figsize=(15,6), title=&#39;Releases per Genre 2007-2016&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4cad695d10&gt; . Conclusion . Pre-1994 games were not released in nearly the volume of modern day. 2005-2011 had a huge uptick in the volume of games released overall Examining the platforms with the highest amount of sales, we can see that platforms have about a 10yr lifespan. | To prepare for 2017 we should look at data from 2007 onwards | PS3 and X360 are still leading in sales along with the Wii, PS2, and DS but we have already determined them to be at end of life. I also weed out the PSP as it is superceded by the PSV and sales have dropped to nothing for the platform. | All game sales seemed to have taken a hit for 2016 but the PS4, XOne, 3DS, WiiU, and PSV still appear to have some life in them. PC games appear to be an outlier in that they produce steady sales that rise and fall over time and are not subject to the normal lifespan of a platform but make them a safe bet as well. | When comparing games sales globally by platform. Some games drastically outperform the rest making many times over what a less popular game would make in sales. And some platforms make more money overall, in their lifespan, than multiple platforms combined. | For the PS2 user_score had very little correlation to game sales. But someone generally can&#39;t give their opinion on a game until after they&#39;ve bought and played it. So, even if a game isnt well liked, if theres enough hype/demand plenty of copies will be bought before the gaming community can give their full opinions. Critics opinions also had very little impact on game sales but were heavily correlated with user scores. | Games can have heavily varying sales across platforms. If a game did well, from a brief glance, the most popular platforms had the largest spikes in sales. | Action games are still the most released titles, but shooter games have increased in market share for more recent years even though the number of releases have went down. Puzzle and Strategy games overall have been poor performers | . | . 4. Create a user profile for each region . For each region (NA, EU, JP), determine: . The top five platforms. Describe variations in their market shares from region to region. | The top five genres. Explain the difference. | Do ESRB ratings affect sales in individual regions? | . regions = [&#39;na_sales&#39;, &#39;eu_sales&#39;,&#39;jp_sales&#39;] . ( sales_per_genre[regions] .plot(kind=&#39;bar&#39;, figsize=(15,10), subplots=True,legend=True, title=&#39;Regional Sales by Genre - All Data&#39;) ) . array([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f4cad000b90&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f4cad553650&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f4cad8b4c50&gt;], dtype=object) . ( sales_per_platform[regions] .plot(kind=&#39;bar&#39;, figsize=(15,10), subplots=True,legend=True, title=&#39;Regional Sales by Platform- All Data&#39;) ) . array([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f4cad39d4d0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f4cad244510&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f4cad39d650&gt;], dtype=object) . ( new_sales_per_genre[regions] .plot(kind=&#39;bar&#39;, figsize=(15,10), subplots=True, legend=True, title=&#39;Sales by Genre per Region - Recent Data&#39;) ) . array([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f4cacca5b10&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f4cae057590&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f4cad925a10&gt;], dtype=object) . ( new_sales_per_genre[regions] .plot(kind=&#39;bar&#39;, figsize=(15,10), subplots=True, legend=True, title=&#39;Sales by Genre per Region - Recent Data&#39;) ) . array([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f4cad1ea950&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f4cacf45450&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f4cad703490&gt;], dtype=object) . rating_table = ( games_new[games_new[&#39;rating&#39;] != -1] .pivot_table(index=&#39;rating&#39;, values=[&#39;na_sales&#39;, &#39;eu_sales&#39;, &#39;jp_sales&#39;], aggfunc=&#39;sum&#39;) ) rating_table.plot(kind=&#39;bar&#39;, figsize=(15,10), subplots=True, legend=True, title=&#39;Regional Sales by ESRB Rating&#39;) . array([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f4cacb3f050&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f4cacefe550&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f4cacb17b90&gt;], dtype=object) . Conclusion . Market share of different platforms varies from region to region. In Japan they favor more portable platforms like the DS and 3DS and the X360 (a US Platform) has almost no presence. Where in western culture (NA and EU) the X360 is a major platform and portable consoles don&#39;t dominate the market. | NA favors Action, Shooters and Sport games which is pretty on par with our societal values. With Role-playing games just edging into the top 5 as well. In the EU they also favor Action, Shooter and Sport games but also like racing games a bit more than the other regions possibly due to Formula 1. In JP Role-Playing games are a huge part of the market with over a 30% market share. Follow it up with a decent interest in action games and then the rest of the genres start to blur together in market share. A lot of Role-Playing games made for JP are released in the US and we call them JRPG&#39;s. | ESRB ratings and sales seem to be pretty consistent among the different regions with the one major variation being that T(een) games do better in JP and M(ature) games sell better in NA and EU. But its roughly a 50/50 split of overall sales for games suitable for younger individuals and games more suited towards teens and adults | . Step 5. Test the following hypotheses: . Average user ratings of the Xbox One and PC platforms are the same. | Average user ratings for the Action and Sports genres are different. Set the¬†alpha¬†threshold value yourself. Explain: | How you formulated the null and alternative hypotheses | What significance level you chose to test the hypotheses, and why | . Test 1A - All Data . H0 (null hypothesis) = Average user ratings of the Xbox One and PC platforms are the same | H1 (alternative hypothesis) = Average user ratings of the Xbox One and PC platforms are different | . xone_games = games[(games[&#39;platform&#39;] == &#39;XOne&#39;) &amp; (games[&#39;user_score&#39;] &gt; -1)] pc_games = games[(games[&#39;platform&#39;] == &#39;PC&#39;) &amp; (games[&#39;user_score&#39;] &gt; -1)] var_samp_1 = np.var(xone_games[&#39;user_score&#39;]) var_samp_2 = np.var(pc_games[&#39;user_score&#39;]) print(f&quot;Variance sample 1: {var_samp_1:.2f} nVariance sample 2: {var_samp_2:.2f}&quot;) alpha = 0.05 # 5% difference between variances percent_diff = (abs(var_samp_1-var_samp_2)/((var_samp_1 + var_samp_2)/2)) print(&quot;Percentage difference: {:.2f}&quot;.format(percent_diff)) if (percent_diff &lt; alpha): print(&quot;There is no difference between the variances&quot;) else: print(&quot;There is a difference between the variances&quot;) . Variance sample 1: 1.90 Variance sample 2: 2.34 Percentage difference: 0.21 There is a difference between the variances . results = st.ttest_ind(xone_games[&#39;user_score&#39;], pc_games[&#39;user_score&#39;], equal_var = False) print(&#39;p-value: &#39;, results.pvalue) if (results.pvalue &lt; alpha): print(&quot;We reject the null hypothesis&quot;) else: print(&quot;We can&#39;t reject the null hypothesis&quot;) . p-value: 4.935072360183565e-06 We reject the null hypothesis . Test 1B - Recent Data . H0 (null hypothesis) = Average user ratings of the Xbox One and PC platforms are the same | H1 (alternative hypothesis) = Average user ratings of the Xbox One and PC platforms are different | . xone_games = games_new[(games_new[&#39;platform&#39;] == &#39;XOne&#39;) &amp; (games_new[&#39;user_score&#39;] &gt; -1)] pc_games = games_new[(games_new[&#39;platform&#39;] == &#39;PC&#39;) &amp; (games_new[&#39;user_score&#39;] &gt; -1)] var_samp_1 = np.var(xone_games[&#39;user_score&#39;]) var_samp_2 = np.var(pc_games[&#39;user_score&#39;]) print(f&quot;Variance sample 1: {var_samp_1:.2f} nVariance sample 2: {var_samp_2:.2f}&quot;) alpha = 0.05 # 5% difference between variances percent_diff = (abs(var_samp_1-var_samp_2)/((var_samp_1 + var_samp_2)/2)) print(&quot;Percentage difference: {:.2f}&quot;.format(percent_diff)) if (percent_diff &lt; alpha): print(&quot;There is no difference between the variances&quot;) else: print(&quot;There is a difference between the variances&quot;) . Variance sample 1: 1.90 Variance sample 2: 2.34 Percentage difference: 0.21 There is a difference between the variances . results = st.ttest_ind(xone_games[&#39;user_score&#39;], pc_games[&#39;user_score&#39;], equal_var = False) print(&#39;p-value: &#39;, results.pvalue) if (results.pvalue &lt; alpha): print(&quot;We reject the null hypothesis&quot;) else: print(&quot;We can&#39;t reject the null hypothesis&quot;) . p-value: 0.04401892858926169 We reject the null hypothesis . Test 2A - All Data . H0 (null hypothesis) = Average user ratings for the Action and Sports genres are the same | H1 (alternative hypothesis) = Average user ratings for the Action and Sports genres are different | . action_games = games[(games[&#39;genre&#39;] == &#39;Action&#39;) &amp; (games[&#39;user_score&#39;] &gt; -1)] sports_games = games[(games[&#39;genre&#39;] == &#39;Sports&#39;) &amp; (games[&#39;user_score&#39;] &gt; -1)] var_samp_1 = np.var(action_games[&#39;user_score&#39;]) var_samp_2 = np.var(sports_games[&#39;user_score&#39;]) print(f&quot;Variance sample 1: {var_samp_1:.2f} nVariance sample 2: {var_samp_2:.2f}&quot;) alpha = 0.05 # 5% difference between variances percent_diff = (abs(var_samp_1-var_samp_2)/((var_samp_1 + var_samp_2)/2)) print(&quot;Percentage difference: {:.2f}&quot;.format(percent_diff)) if (percent_diff &lt; alpha): print(&quot;There is no difference between the variances&quot;) else: print(&quot;There is a difference between the variances&quot;) . Variance sample 1: 2.03 Variance sample 2: 2.59 Percentage difference: 0.24 There is a difference between the variances . results = st.ttest_ind(action_games[&#39;user_score&#39;], sports_games[&#39;user_score&#39;], equal_var = False) print(&#39;p-value: &#39;, results.pvalue) if (results.pvalue &lt; alpha): print(&quot;We reject the null hypothesis&quot;) else: print(&quot;We can&#39;t reject the null hypothesis&quot;) . p-value: 0.11483818791498286 We can&#39;t reject the null hypothesis . Test 2B - Recent Data . H0 (null hypothesis) = Average user ratings for the Action and Sports genres are the same | H1 (alternative hypothesis) = Average user ratings for the Action and Sports genres are different | . action_games = games_new[(games_new[&#39;genre&#39;] == &#39;Action&#39;) &amp; (games_new[&#39;user_score&#39;] &gt; -1)] sports_games = games_new[(games_new[&#39;genre&#39;] == &#39;Sports&#39;) &amp; (games_new[&#39;user_score&#39;] &gt; -1)] var_samp_1 = np.var(action_games[&#39;user_score&#39;]) var_samp_2 = np.var(sports_games[&#39;user_score&#39;]) print(f&quot;Variance sample 1: {var_samp_1:.2f} nVariance sample 2: {var_samp_2:.2f}&quot;) alpha = 0.05 # 5% difference between variances percent_diff = (abs(var_samp_1-var_samp_2)/((var_samp_1 + var_samp_2)/2)) print(&quot;Percentage difference: {:.2f}&quot;.format(percent_diff)) if (percent_diff &lt; alpha): print(&quot;There is no difference between the variances&quot;) else: print(&quot;There is a difference between the variances&quot;) . Variance sample 1: 1.83 Variance sample 2: 2.81 Percentage difference: 0.42 There is a difference between the variances . results = st.ttest_ind(action_games[&#39;user_score&#39;], sports_games[&#39;user_score&#39;], equal_var = False) print(&#39;p-value: &#39;, results.pvalue) if (results.pvalue &lt; alpha): print(&quot;We reject the null hypothesis&quot;) else: print(&quot;We can&#39;t reject the null hypothesis&quot;) . p-value: 4.540665510055229e-11 We reject the null hypothesis . Conclusion . For an alpha threshold I used 5%. This is the most commonly used level in statistics. We were taught this and I did some searching as a double check. | I always set the null hypothesis as the two variables are equal. This is the default standard in statistics. | For the 1st test, as seen above, we rejected the null hypothesis that the user ratings between Xbox One and PC games were the same. Even with filtering out older PC games by using more recent data we still rejected the null hypothesis. | For the 2nd test we again set the null hypothesis that sports and action games had equal user ratings. This time we received some interesting results. When testing all the data we couldn&#39;t reject the null hypothesis, but when we ran the test again only using more recent data we rejected the null hypothesis. | . Final Conclusion . Sales numbers for the dataset were reliable but user ratings, critic scores and ESRB Ratings left much to be desired. | Games are not being mass produced as quickly anymore and there was quite a 5 yr boom in game production from 2008-2012. | The worlds taste in video games has changed over time and we can see a clear difference between the games eastern and western cultures enjoy. This is seen by how much different the market share of different genres is. | Successful game platforms have about a 10yr lifecycle, with the exception of the PC, before sales fall off completely. | Games appear to have massive hits and complete duds just like the movie industry causing a huge swing in potential sales for a game. | Japan appears to prefer portable handheld consoles more than NA and the EU | .",
            "url": "https://nicholas-j-snyder.github.io/portfolio/fastpages/jupyter/2020/05/31/Video-Game-Sales.html",
            "relUrl": "/fastpages/jupyter/2020/05/31/Video-Game-Sales.html",
            "date": " ‚Ä¢ May 31, 2020"
        }
        
    
  

  
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://nicholas-j-snyder.github.io/portfolio/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}